{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preparation Overview\n",
    "\n",
    "In this tutorial we will extract all the data we need to run both the uncalibrated SWIM-RS model and to calibrate and run a parameterized model. The model uses a daily soil water balance, and as such, it needs a daily estimate of meteorological drivers and some *a priori* information on the soils in our sample locations. It also needs an estimate of the state of the vegetation on the surface, for which we use Landsat-based NDVI. With this information, we will be able to run SWIM-RS to estimate the daily ET, soil water storage, recharge, runoff, and simulated irrigation.\n",
    "\n",
    "To calibrate the model so it behaves more realistically, we must use the parameter inversion software (PEST++, in this case) alongside target data which provides *somewhat* independent estimates of ET and snow on the ground. Once the model is calibrated for a sample plot, these data are no longer needed. Therefore, the calibrated model can be run for periods before or after SNODAS or ETf data is available.\n",
    "\n",
    "The remote sensing data (ETf and NDVI) are the most time-consuming step, as the data are being extracted from potentially thousands of separate Landsat-like images in Earth Engine. The snow, soils, and irrigation data, on the other hand, are relatively quick, as the images are fewer, with one CONUS-wide image per day in SNODAS, a few static images for the soils data, and one image annually for the irrigation products. However, thanks to Earth Engine, even the Landsat-based products are quickly extracted if the number of sample plots are small and clustered in space, as they are for this tutorial.\n",
    "\n",
    "The SWIM-RS approach requires the following input datasets to run and/or calibrate:\n",
    "1. **NDVI**: Normalized Difference Vegetation Index is a measure made using the red and near-infrared bands of a multispectral instrument. It is a good way to estimate the relative density and vigor of vegetation, which is highly correlated with transpiration. NDVI is used as a proxy for the transpirative component of the crop coefficient in SWIM-RS, Kcb. Here, we access NDVI information from Landsat satellite images in Earth Engine.\n",
    "2. **ETf**: The rate of ET expressed as a fraction of reference/potential ET. This is also known in agricultural water use modeling as the 'crop coefficient', or Kc. For this tutorial we use SSEBop, accessed from Google Earth Engine. We could use results from any number of remote sensing-based modeling approaches (METRIC, OpenET ensemble, etc.). *FOR USE IN CALIBRATION ONLY*\n",
    "3. **Soils**: We need an initial estimate of soil hydraulic properties that govern the way water behaves in our very simple model soil water reservoir.\n",
    "4. **Irrigation**: We use an irrigation mask (IrrMapper or LANID) to constrain the data extraction of the irrigated and unirrigated portions of any given sample plot.\n",
    "5. **Snow**: We use the SNODAS product to estimate the snow water equivalent (SWE) at a daily time step to calibrate the simple snow model in SWIM-RS. *FOR USE IN CALIBRATION ONLY*\n",
    "\n",
    "Note: For this tutorial, we use 'field' and 'plot' somewhat interchangeably. Indeed, the sample plots for this tutorial are fields. However, we could draw an arbitrary polygon over a location of interest and run the model there. Keep in mind, the data will represent the mean of the irrigated and unirrigated portions of the sample plot. Therefore, using sensible land-use features (e.g., individual agricultural fields) is a good approach because assuming homogenous land-use management in a single field is not a terrible assumption.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Paths\n",
    "\n",
    "1. **Run extraction** (requires Earth Engine access): Execute the cells below to download data from Earth Engine and GridMET\n",
    "2. **Use pre-built data**: Skip to notebook 03 (data available in `data/prebuilt/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Authorize Earth Engine\n",
    "\n",
    "If new to Earth Engine, checkout https://developers.google.com/earth-engine/guides/auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ee\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.data_extraction.ee.etf_export import clustered_sample_etf\n",
    "from swimrs.data_extraction.ee.ndvi_export import clustered_sample_ndvi\n",
    "from swimrs.data_extraction.ee.snodas_export import sample_snodas_swe\n",
    "from swimrs.data_extraction.ee.ee_props import get_irrigation, get_ssurgo, get_landcover\n",
    "from swimrs.data_extraction.ee.ee_utils import is_authorized\n",
    "\n",
    "sys.setrecursionlimit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_authorized():\n",
    "    ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load project configuration from the TOML file. This provides all paths, date ranges, and bucket settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: 1_Boulder\n",
      "Bucket: wudr\n",
      "Date range: 2004-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "Shapefile: /home/dgketchum/code/swim-rs/examples/1_Boulder/data/gis/mt_sid_boulder.shp\n",
      "ETf model: ssebop\n"
     ]
    }
   ],
   "source": [
    "# Load project configuration\n",
    "project_dir = os.path.abspath('.')\n",
    "config_file = os.path.join(project_dir, '1_Boulder.toml')\n",
    "\n",
    "cfg = ProjectConfig()\n",
    "cfg.read_config(config_file, project_root_override=project_dir)\n",
    "\n",
    "print(f\"Project: {cfg.project_name}\")\n",
    "print(f\"Bucket: {cfg.ee_bucket}\")\n",
    "print(f\"Date range: {cfg.start_dt} to {cfg.end_dt}\")\n",
    "print(f\"Shapefile: {cfg.fields_shapefile}\")\n",
    "print(f\"ETf model: {cfg.etf_target_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export destination - use Cloud Storage bucket (faster) or Google Drive\n",
    "# Change to True to use Google Drive instead of a bucket\n",
    "USE_DRIVE = False\n",
    "\n",
    "# Export settings derived from config\n",
    "export_dest = 'drive' if USE_DRIVE else 'bucket'\n",
    "export_bucket = None if USE_DRIVE else cfg.ee_bucket\n",
    "file_prefix = cfg.project_name  # Bucket path prefix\n",
    "\n",
    "# Drive folder from config (or default to project name)\n",
    "drive_folder = cfg.resolved_config.get('earth_engine', {}).get('drive_folder', cfg.project_name)\n",
    "\n",
    "# Date range from config\n",
    "start_year = cfg.start_dt.year\n",
    "end_year = cfg.end_dt.year\n",
    "\n",
    "# Optional: Limit extraction to specific fields for testing\n",
    "# select_fields = ['043_000130', '043_000128', '043_000161']\n",
    "\n",
    "# running all fields will take a bit longer\n",
    "select_fields = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Remote Sensing Extraction (ETf and NDVI)\n",
    "\n",
    "## Extract ETf Raster Data\n",
    "\n",
    "Now we're ready to do 'zonal stats' on our fields. We provide a local shapefile, and the code converts it to a FeatureCollection internally, then extracts ETf/NDVI summaries per field to CSV.\n",
    "\n",
    "We need to use an irrigated lands mask (IrrMapper or LANID) to find irrigated and unirrigated zones within the polygons of our shapefile. You see this implemented in the code below, where the `mask` argument is either `irr` for irrigated, or `inv_irr` for the inverse of the irrigated mask, which are unirrigated areas.\n",
    "\n",
    "For the raster data extraction, there are three options to get at the data:\n",
    "\n",
    "* **`clustered_sample_etf`**: This function finds all Landsat images intersecting the sample polygons (i.e., our fields). Since our fields are clustered together, this finds a reasonable number of images and iterates over them, extracting data from each. We use this on the tutorial since the sample from the Montana fields database is geographically constrained.\n",
    "\n",
    "* **`sparse_sample_etf`**: This function assumes the samples (fields) are spread out over many different Landsat images. It runs sample-by-sample, finding Landsat images overlapping each sample and extracting from them. This is used when we extract data for widely-spaced sites across the Conterminous US in examples 4 and 5, or globally, as in example 6.\n",
    "\n",
    "* **`export_etf_images`**: This function exports the Landsat images themselves, clipped to the bounds of a 'hopefully' clustered set of sample polygons. This is helpful for experimentation with buffering zones and so on, but not meant for large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ETf (inv_irr)...\n",
      "etf_inv_irr_2004\n",
      "etf_inv_irr_2005\n",
      "etf_inv_irr_2006\n",
      "etf_inv_irr_2007\n",
      "etf_inv_irr_2008\n",
      "etf_inv_irr_2009\n",
      "etf_inv_irr_2010\n",
      "etf_inv_irr_2011\n",
      "etf_inv_irr_2012\n",
      "etf_inv_irr_2013\n",
      "etf_inv_irr_2014\n",
      "etf_inv_irr_2015\n",
      "etf_inv_irr_2016\n",
      "etf_inv_irr_2017\n",
      "etf_inv_irr_2018\n",
      "etf_inv_irr_2019\n",
      "etf_inv_irr_2020\n",
      "etf_inv_irr_2021\n",
      "etf_inv_irr_2022\n",
      "Extracting ETf (irr)...\n",
      "etf_irr_2004\n",
      "etf_irr_2005\n",
      "etf_irr_2006\n",
      "etf_irr_2007\n",
      "etf_irr_2008\n",
      "etf_irr_2009\n",
      "etf_irr_2010\n",
      "etf_irr_2011\n",
      "etf_irr_2012\n",
      "etf_irr_2013\n",
      "etf_irr_2014\n",
      "etf_irr_2015\n",
      "etf_irr_2016\n",
      "etf_irr_2017\n",
      "etf_irr_2018\n",
      "etf_irr_2019\n",
      "etf_irr_2020\n",
      "etf_irr_2021\n",
      "etf_irr_2022\n"
     ]
    }
   ],
   "source": [
    "# Extract ETf for both irrigated and unirrigated masks\n",
    "# This divides every sample into a 'purely' irrigated section (irr) and an unirrigated one (inv_irr)\n",
    "# This allows us to build a model for irrigated areas that aren't contaminated by unirrigated areas.\n",
    "\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting ETf ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='drive', \n",
    "            state_col=cfg.state_col, drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='bucket', \n",
    "            file_prefix=file_prefix, state_col=cfg.state_col, drive_categorize=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Extract NDVI Raster Data\n\nThis is just like the ETf extraction, but for NDVI. This is a little more straightforward as we can get the data straight from the Landsat collection, and don't need special permissions or knowledge of where the data are stored.\n\n**Note: Spectral Harmonization** - All Landsat NDVI values are automatically harmonized to the\nLandsat 8 OLI reference standard using Spectral Bandpass Adjustment Factors (SBAF) from\nRoy et al. (2016). This ensures consistent NDVI values across Landsat 4/5/7/8/9.\n\nAs with the ETf code, the extraction has three options to get at the data:\n* **`clustered_sample_ndvi`**: For clustered fields (what we use here)\n* **`sparse_sample_ndvi`**: For fields spread across many Landsat images\n* **`export_ndvi_images`**: For exporting the Landsat images themselves"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting NDVI (inv_irr)...\n",
      "ndvi_2004_inv_irr\n",
      "ndvi_2005_inv_irr\n",
      "ndvi_2006_inv_irr\n",
      "ndvi_2007_inv_irr\n",
      "ndvi_2008_inv_irr\n",
      "ndvi_2009_inv_irr\n",
      "ndvi_2010_inv_irr\n",
      "ndvi_2011_inv_irr\n",
      "ndvi_2012_inv_irr\n",
      "ndvi_2013_inv_irr\n",
      "ndvi_2014_inv_irr\n",
      "ndvi_2015_inv_irr\n",
      "ndvi_2016_inv_irr\n",
      "ndvi_2017_inv_irr\n",
      "ndvi_2018_inv_irr\n",
      "ndvi_2019_inv_irr\n",
      "ndvi_2020_inv_irr\n",
      "ndvi_2021_inv_irr\n",
      "ndvi_2022_inv_irr\n",
      "Extracting NDVI (irr)...\n",
      "ndvi_2004_irr\n",
      "ndvi_2005_irr\n",
      "ndvi_2006_irr\n",
      "ndvi_2007_irr\n",
      "ndvi_2008_irr\n",
      "ndvi_2009_irr\n",
      "ndvi_2010_irr\n",
      "ndvi_2011_irr\n",
      "ndvi_2012_irr\n",
      "ndvi_2013_irr\n",
      "ndvi_2014_irr\n",
      "ndvi_2015_irr\n",
      "ndvi_2016_irr\n",
      "ndvi_2017_irr\n",
      "ndvi_2018_irr\n",
      "ndvi_2019_irr\n",
      "ndvi_2020_irr\n",
      "ndvi_2021_irr\n",
      "ndvi_2022_irr\n"
     ]
    }
   ],
   "source": [
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting NDVI ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='drive', \n",
    "            drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='bucket', \n",
    "            file_prefix=file_prefix, drive_categorize=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rmieb3fgd48",
   "source": "## Extract Sentinel-2 NDVI Data\n\nSentinel-2 provides higher spatial resolution (10m) and a different revisit cycle than Landsat, giving us more frequent observations for sensor merging. Note that Sentinel-2 data is only available from 2015 onwards.\n\n**Note: Spectral Harmonization** - Sentinel-2 NDVI values are automatically harmonized to the\nLandsat 8 OLI reference standard using SBAF coefficients from Claverie et al. (2018). This\nensures Sentinel-2 observations can be directly merged with Landsat without bias correction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w5upqg7sml",
   "source": "# Extract Sentinel-2 NDVI (available from 2015+)\nsentinel_start_year = max(2015, start_year)  # Sentinel-2 launched in 2015\n\nif sentinel_start_year <= end_year:\n    for mask in ['inv_irr', 'irr']:\n        print(f\"Extracting Sentinel-2 NDVI ({mask})...\")\n\n        if USE_DRIVE:\n            clustered_sample_ndvi(\n                cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask,\n                check_dir=None, start_yr=sentinel_start_year, end_yr=end_year,\n                feature_id=cfg.feature_id_col, select=select_fields,\n                satellite='sentinel', dest='drive',\n                drive_folder=drive_folder, drive_categorize=True\n            )\n        else:\n            clustered_sample_ndvi(\n                cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask,\n                check_dir=None, start_yr=sentinel_start_year, end_yr=end_year,\n                feature_id=cfg.feature_id_col, select=select_fields,\n                satellite='sentinel', dest='bucket',\n                file_prefix=file_prefix, drive_categorize=False\n            )\nelse:\n    print(\"Skipping Sentinel-2: date range ends before 2015\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Snow, Irrigation, and Soils Extraction\n",
    "\n",
    "For the raster data extraction, there are three functions we need to run:\n",
    "\n",
    "* **`sample_snodas_swe`**: This function iterates over the daily SNODAS images in Earth Engine, extracting mean SWE for each sample plot for each day, September through May. (https://nsidc.org/data/g02158/versions/1)\n",
    "\n",
    "* **`get_irrigation`**: This function uses IrrMapper to get statistics about the irrigation status of each plot for each year, including the fraction of the plot that was irrigated. (https://www.mdpi.com/2072-4292/12/14/2328)\n",
    "\n",
    "* **`get_ssurgo`**: This function uses data summarized and put in a public Earth Engine asset by Charles Morton at Desert Research Institute from SSURGO to summarize plot-scale soil texture and hydraulic properties used by SWIM-RS.\n",
    "\n",
    "Note: The module also has functions for extracting vegetation height (`get_landfire`) and crop type (`get_cdl`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## SWE Data (SNODAS)\n",
    "\n",
    "Notes:\n",
    "- Ensure that your Cloud Storage bucket has the correct permissions for Earth Engine to write to it.\n",
    "- This will produce a monthly dataset for Sep - May, regardless of SWE status at the sample plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SNODAS SWE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SNODAS: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [03:57<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNODAS exported 240, skipped 0 existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting SNODAS SWE...\")\n",
    "sample_snodas_swe(\n",
    "    cfg.fields_shapefile, bucket=export_bucket, debug=False, \n",
    "    check_dir=None, overwrite=False, feature_id=cfg.feature_id_col,\n",
    "    select=select_fields,\n",
    "    dest=export_dest, drive_folder=drive_folder, drive_categorize=True,\n",
    "    file_prefix=file_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Irrigation Data (IrrMapper/LANID)\n",
    "\n",
    "This will produce an annual dataset of the IrrMapper-estimated irrigated fraction for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting irrigation data...\n",
      "1_Boulder_irr\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting irrigation data...\")\n",
    "get_irrigation(\n",
    "    cfg.fields_shapefile, desc=f'{cfg.project_name}_irr', debug=False, \n",
    "    selector=cfg.feature_id_col, select=select_fields, lanid=True, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Land Cover Data\n",
    "\n",
    "Extract dominant landcover (MODIS LC_Type1 + FROM-GLC10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting land cover data...\n",
      "1_Boulder_landcover\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting land cover data...\")\n",
    "get_landcover(\n",
    "    cfg.fields_shapefile, desc=f'{cfg.project_name}_landcover', debug=False, \n",
    "    selector=cfg.feature_id_col, select=select_fields, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Soils Data (SSURGO)\n",
    "\n",
    "This will produce a single dataset of the SSURGO-estimated soil properties for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SSURGO soil data...\n",
      "1_Boulder_ssurgo\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting SSURGO soil data...\")\n",
    "get_ssurgo(\n",
    "    cfg.fields_shapefile, desc=f'{cfg.project_name}_ssurgo', debug=False, \n",
    "    selector=cfg.feature_id_col, select=select_fields, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Meteorology Data Extraction (GridMET)\n",
    "\n",
    "In this section, we will:\n",
    "1. Associate sample plots (fields) with their nearest GridMET cell\n",
    "2. Extract GridMET bias-correction information from DRI's rasters\n",
    "3. Download GridMET data from the THREDDS server, and NLDAS-2 hourly precipitation data\n",
    "\n",
    "Read about GridMET: https://www.climatologylab.org/gridmet.html  \n",
    "Read about NLDAS-2: https://ldas.gsfc.nasa.gov/nldas/nldas-2-model-data\n",
    "\n",
    "## GridMET Cell Assignment\n",
    "\n",
    "Our fields are in a pretty tight cluster. We're preparing to download meteorology from a 4-km resolution dataset (GridMET), so it's unnecessary to download a meteorology time series for each field, as many will just be copies. Rather, we'll identify the GridMET 'cells' with a shapefile, and find the closest cell to each field.\n",
    "\n",
    "In addition to the raw meteorology data, we will also be accessing rasters that show the observed bias between AgriMet weather stations and GridMET's reference ET. These biases are due to the impacts of irrigated agriculture on the near-surface atmosphere, which often tends to see relatively high humidity and low temperature compared to arid and semi-arid surroundings. This bias is documented in Blankeneau (2020; https://doi.org/10.1016/j.agwat.2020.106376). The bias correction surfaces were mapped over CONUS by Desert Research Institute and OpenET and are documented in Melton et al., 2021 (https://doi.org/10.1111/1752-1688.12956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swimrs.data_extraction.gridmet.gridmet import assign_gridmet_and_corrections, download_gridmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each field with nearest GridMET cell and extract bias correction factors\n",
    "assign_gridmet_and_corrections(\n",
    "    fields=cfg.fields_shapefile, \n",
    "    gridmet_points=cfg.gridmet_centroids, \n",
    "    gridmet_ras=cfg.correction_tifs, \n",
    "    fields_join=cfg.gridmet_mapping_shp, \n",
    "    factors_js=cfg.gridmet_factors, \n",
    "    feature_id=cfg.feature_id_col,\n",
    "    field_select=select_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The GeoDataFrame you are attempting to plot is empty.\")\n",
    "\n",
    "gdf = gpd.read_file(cfg.fields_shapefile)\n",
    "cdf = gpd.read_file(cfg.gridmet_centroids)\n",
    "gdf_gfid = gpd.read_file(cfg.gridmet_mapping_shp)\n",
    "\n",
    "# Reproject to EPSG:5071 (NAD83 / Conus Albers) for visualization\n",
    "target_crs = \"EPSG:5071\"\n",
    "gdf = gdf.to_crs(target_crs)\n",
    "cdf = cdf.to_crs(target_crs)\n",
    "gdf_gfid = gdf_gfid.to_crs(target_crs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Color fields by their assigned GridMET cell\n",
    "unique_gfids = set(cdf['GFID'].unique()).union(gdf_gfid['GFID'].unique())\n",
    "colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(unique_gfids))]\n",
    "color_map = dict(zip(unique_gfids, colors))\n",
    "\n",
    "for gfid, color in color_map.items():\n",
    "    cdf[cdf['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color)\n",
    "    gdf_gfid[gdf_gfid['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color)\n",
    "\n",
    "# Hybrid basemap: satellite imagery + labels overlay\n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs=target_crs)\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.PositronOnlyLabels, crs=target_crs, alpha=0.8)\n",
    "\n",
    "plt.title('Fields colored by GridMET Cell Assignment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "Each field's color should match that of the nearest GridMET centroid. The 'GFID' for each field has been saved in the output shapefile. This reduces the data we must download by a large factor.\n",
    "\n",
    "## Download GridMET Data\n",
    "\n",
    "Now we download the daily meteorological timeseries from GridMET's THREDDS server. This will probably take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.met_dir, exist_ok=True)\n",
    "\n",
    "download_gridmet(\n",
    "    cfg.gridmet_mapping_shp, cfg.gridmet_factors, cfg.met_dir, \n",
    "    start='1987-01-01', end='2023-12-31',\n",
    "    target_fields=select_fields, overwrite=False, feature_id=cfg.feature_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "Note that even though we have 78 fields, we only had to download four GridMet time series. Let's look at one of the GridMET time series and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Find a GridMET file\n",
    "met_files = [f for f in os.listdir(cfg.met_dir) if f.endswith('.parquet')]\n",
    "if met_files:\n",
    "    met_data = os.path.join(cfg.met_dir, met_files[0])\n",
    "    met_df = pd.read_parquet(met_data)\n",
    "    print(f\"Loaded {met_files[0]}\")\n",
    "    print(met_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Here, we see we have information on the date, the location, and daily meteorological information from GridMET, including `tmin_c`, `tmax_c`, and `prcp_mm`. We see the critical reference ET estimates in 'uncorrected' form (`eto_mm` and `etr_mm`), which we could use in natural vegetation, and also in 'corrected' form (`eto_corr` and `etr_corr`), which we will be using over our irrigated study area.\n",
    "\n",
    "---\n",
    "\n",
    "# Part D: Sync Data from Cloud Storage\n",
    "\n",
    "Once your Earth Engine export tasks have completed (monitor at https://code.earthengine.google.com/tasks), sync the data from your Cloud Storage bucket to your local filesystem.\n",
    "\n",
    "## Bucket and Local Directory Mirroring\n",
    "\n",
    "The Cloud Storage bucket structure mirrors the local project `data/` directory. When Earth Engine exports data, it writes to paths like:\n",
    "\n",
    "```\n",
    "gs://{bucket}/{project_name}/remote_sensing/landsat/extracts/ssebop_etf/...\n",
    "gs://{bucket}/{project_name}/snow/snodas/extracts/...\n",
    "gs://{bucket}/{project_name}/properties/...\n",
    "```\n",
    "\n",
    "The `sync_from_bucket()` method uses `gsutil rsync` to pull these files into the corresponding local paths:\n",
    "\n",
    "```\n",
    "data/remote_sensing/landsat/extracts/ssebop_etf/...\n",
    "data/snow/snodas/extracts/...\n",
    "data/properties/...\n",
    "```\n",
    "\n",
    "This mirroring design means:\n",
    "1. **Consistent paths**: Code that reads data works identically whether data came from the bucket or was generated locally\n",
    "2. **Incremental sync**: Only new/changed files are downloaded on subsequent syncs\n",
    "3. **Multi-project buckets**: The `{project_name}` prefix allows multiple projects to share one bucket without collision\n",
    "\n",
    "**Note**: If using Google Drive export (`USE_DRIVE = True`), you'll need to manually download files from Drive and place them in the corresponding `data/` subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview what will be synced (dry run)\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Preview of files to sync (dry run):\")\n",
    "    cfg.sync_from_bucket(dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing data from bucket...\n",
      "Running: gsutil -m rsync -r gs://wudr/1_Boulder/ /home/dgketchum/code/swim-rs/examples/1_Boulder/data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying gs://wudr/1_Boulder/properties/1_Boulder_irr.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/properties/1_Boulder_landcover.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/properties/1_Boulder_ssurgo.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_01.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_09.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_09.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_05.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_05.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_05.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_12.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_06.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_10.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2017_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_11.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_12.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_10.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_06.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_11.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2018_12.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_02.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_11.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_05.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_12.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_11.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2019_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_08.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2009_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_07.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_02.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_04.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_08.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_09.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_06.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2004_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2020_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_08.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2007_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_04.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2016_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_07.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2012_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2021_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2014_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2013_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2011_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_04.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_03.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_07.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2005_06.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2015_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2006_01.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2008_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2010_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_08.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_10.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2022_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_02.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_01.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_03.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_04.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_06.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_05.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_07.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_09.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_08.csv...\n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_12.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_11.csv...             \n",
      "Copying gs://wudr/1_Boulder/snow/snodas/extracts/swe_2023_10.csv...             \n",
      "\\ [243/243 files][  4.5 MiB/  4.5 MiB] 100% Done                                \n",
      "Operation completed over 243 objects/4.5 MiB.                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync complete!\n"
     ]
    }
   ],
   "source": [
    "# Actually sync data from bucket\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Syncing data from bucket...\")\n",
    "    cfg.sync_from_bucket(dry_run=False)\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grt7wcn486",
   "metadata": {},
   "source": "---\n\n## Summary\n\nData extraction is complete. After syncing from the bucket, you should have:\n- `data/remote_sensing/landsat/extracts/ssebop_etf/` - ETf CSVs by year and mask\n- `data/remote_sensing/landsat/extracts/ndvi/` - Landsat NDVI CSVs by year and mask  \n- `data/remote_sensing/sentinel/extracts/ndvi/` - Sentinel-2 NDVI CSVs by year and mask (2015+)\n- `data/snow/snodas/extracts/` - SNODAS SWE CSVs by month\n- `data/properties/` - Irrigation, landcover, and soils CSVs\n- `data/met_timeseries/gridmet/` - GridMET meteorology parquet files\n\n**Next**: Run notebook 03 to ingest this data into the SwimContainer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c9434-4682-4881-af49-4a3db9a32996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}