{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preparation Overview\n",
    "\n",
    "In this tutorial we will extract all the data we need to run both the uncalibrated SWIM-RS model and to calibrate and run a parameterized model. The model uses a daily soil water balance, and as such, it needs a daily estimate of meteorological drivers and some *a priori* information on the soils in our sample locations. It also needs an estimate of the state of the vegetation on the surface, for which we use Landsat-based NDVI. With this information, we will be able to run SWIM-RS to estimate the daily ET, soil water storage, recharge, runoff, and simulated irrigation.\n",
    "\n",
    "To calibrate the model so it behaves more realistically, we must use the parameter inversion software (PEST++, in this case) alongside target data which provides *somewhat* independent estimates of ET and snow on the ground. Once the model is calibrated for a sample plot, these data are no longer needed. Therefore, the calibrated model can be run for periods before or after SNODAS or ETf data is available.\n",
    "\n",
    "The remote sensing data (ETf and NDVI) are the most time-consuming step, as the data are being extracted from potentially thousands of separate Landsat-like images in Earth Engine. The snow, soils, and irrigation data, on the other hand, are relatively quick, as the images are fewer, with one CONUS-wide image per day in SNODAS, a few static images for the soils data, and one image annually for the irrigation products. However, thanks to Earth Engine, even the Landsat-based products are quickly extracted if the number of sample plots are small and clustered in space, as they are for this tutorial.\n",
    "\n",
    "The SWIM-RS approach requires the following input datasets to run and/or calibrate:\n",
    "1. **NDVI**: Normalized Difference Vegetation Index is a measure made using the red and near-infrared bands of a multispectral instrument. It is a good way to estimate the relative density and vigor of vegetation, which is highly correlated with transpiration. NDVI is used as a proxy for the transpirative component of the crop coefficient in SWIM-RS, Kcb. Here, we access NDVI information from Landsat satellite images in Earth Engine.\n",
    "2. **ETf**: The rate of ET expressed as a fraction of reference/potential ET. This is also known in agricultural water use modeling as the 'crop coefficient', or Kc. For this tutorial we use SSEBop, accessed from Google Earth Engine. We could use results from any number of remote sensing-based modeling approaches (METRIC, OpenET ensemble, etc.). *FOR USE IN CALIBRATION ONLY*\n",
    "3. **Soils**: We need an initial estimate of soil hydraulic properties that govern the way water behaves in our very simple model soil water reservoir.\n",
    "4. **Irrigation**: We use an irrigation mask (IrrMapper or LANID) to constrain the data extraction of the irrigated and unirrigated portions of any given sample plot.\n",
    "5. **Snow**: We use the SNODAS product to estimate the snow water equivalent (SWE) at a daily time step to calibrate the simple snow model in SWIM-RS. *FOR USE IN CALIBRATION ONLY*\n",
    "\n",
    "Note: For this tutorial, we use 'field' and 'plot' somewhat interchangeably. Indeed, the sample plots for this tutorial are fields. However, we could draw an arbitrary polygon over a location of interest and run the model there. Keep in mind, the data will represent the mean of the irrigated and unirrigated portions of the sample plot. Therefore, using sensible land-use features (e.g., individual agricultural fields) is a good approach because assuming homogenous land-use management in a single field is not a terrible assumption.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Paths\n",
    "\n",
    "1. **Run extraction** (requires Earth Engine access): Execute the cells below to download data from Earth Engine and GridMET\n",
    "2. **Use pre-built data**: Skip to notebook 03 (data available in `data/prebuilt/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Authorize Earth Engine\n",
    "\n",
    "If new to Earth Engine, checkout https://developers.google.com/earth-engine/guides/auth"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:20:29.766548018Z",
     "start_time": "2026-01-09T22:20:29.042790070Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import ee\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.data_extraction.ee.etf_export import clustered_sample_etf\n",
    "from swimrs.data_extraction.ee.ndvi_export import clustered_sample_ndvi\n",
    "from swimrs.data_extraction.ee.snodas_export import sample_snodas_swe\n",
    "from swimrs.data_extraction.snodas.snodas import create_timeseries_json\n",
    "from swimrs.data_extraction.ee.ee_props import get_irrigation, get_ssurgo, get_landcover\n",
    "from swimrs.data_extraction.ee.ee_utils import is_authorized\n",
    "\n",
    "sys.setrecursionlimit(5000)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:20:32.053138101Z",
     "start_time": "2026-01-09T22:20:30.494298626Z"
    }
   },
   "source": [
    "if not is_authorized():\n",
    "    ee.Authenticate()\n",
    "ee.Initialize()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load project configuration from the TOML file. This provides all paths, date ranges, and bucket settings."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:20:59.716417057Z",
     "start_time": "2026-01-09T22:20:59.657750287Z"
    }
   },
   "source": [
    "# Load project configuration\n",
    "project_dir = os.path.abspath('.')\n",
    "config_file = os.path.join(project_dir, '1_Boulder.toml')\n",
    "\n",
    "cfg = ProjectConfig()\n",
    "cfg.read_config(config_file, project_root_override=project_dir)\n",
    "\n",
    "print(f\"Project: {cfg.project_name}\")\n",
    "print(f\"Bucket: {cfg.ee_bucket}\")\n",
    "print(f\"Date range: {cfg.start_dt} to {cfg.end_dt}\")\n",
    "print(f\"Shapefile: {cfg.fields_shapefile}\")\n",
    "print(f\"ETf model: {cfg.etf_target_model}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: 1_Boulder\n",
      "Bucket: wudr\n",
      "Date range: 2004-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "Shapefile: /home/dgketchum/code/swim-rs/examples/1_Boulder/data/gis/mt_sid_boulder.shp\n",
      "ETf model: ssebop\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:21:03.555862283Z",
     "start_time": "2026-01-09T22:21:03.532965471Z"
    }
   },
   "source": [
    "# Export destination - use Cloud Storage bucket (faster) or Google Drive\n",
    "# Change to True to use Google Drive instead of a bucket\n",
    "USE_DRIVE = False\n",
    "\n",
    "# Export settings derived from config\n",
    "export_dest = 'drive' if USE_DRIVE else 'bucket'\n",
    "export_bucket = None if USE_DRIVE else cfg.ee_bucket\n",
    "file_prefix = cfg.project_name  # Bucket path prefix\n",
    "\n",
    "# Drive folder from config (or default to project name)\n",
    "drive_folder = cfg.resolved_config.get('earth_engine', {}).get('drive_folder', cfg.project_name)\n",
    "\n",
    "# Date range from config\n",
    "start_year = cfg.start_dt.year\n",
    "end_year = cfg.end_dt.year\n",
    "\n",
    "# Optional: Limit extraction to specific fields for testing\n",
    "select_fields = ['043_000130', '043_000128', '043_000161']\n",
    "\n",
    "# running all fields will take a bit longer\n",
    "# select_fields = None"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Remote Sensing Extraction (ETf and NDVI)\n",
    "\n",
    "## Extract ETf Raster Data\n",
    "\n",
    "Now we're ready to do 'zonal stats' on our fields. We provide a local shapefile, and the code converts it to a FeatureCollection internally, then extracts ETf/NDVI summaries per field to CSV.\n",
    "\n",
    "We need to use an irrigated lands mask (IrrMapper or LANID) to find irrigated and unirrigated zones within the polygons of our shapefile. You see this implemented in the code below, where the `mask` argument is either `irr` for irrigated, or `inv_irr` for the inverse of the irrigated mask, which are unirrigated areas.\n",
    "\n",
    "For the raster data extraction, there are three options to get at the data:\n",
    "\n",
    "* **`clustered_sample_etf`**: This function finds all Landsat images intersecting the sample polygons (i.e., our fields). Since our fields are clustered together, this finds a reasonable number of images and iterates over them, extracting data from each. We use this on the tutorial since the sample from the Montana fields database is geographically constrained.\n",
    "\n",
    "* **`sparse_sample_etf`**: This function assumes the samples (fields) are spread out over many different Landsat images. It runs sample-by-sample, finding Landsat images overlapping each sample and extracting from them. This is used when we extract data for widely-spaced sites across the Conterminous US in examples 4 and 5, or globally, as in example 6.\n",
    "\n",
    "* **`export_etf_images`**: This function exports the Landsat images themselves, clipped to the bounds of a 'hopefully' clustered set of sample polygons. This is helpful for experimentation with buffering zones and so on, but not meant for large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:21:07.225031250Z",
     "start_time": "2026-01-09T22:21:06.592357132Z"
    }
   },
   "source": [
    "# Extract ETf for both irrigated and unirrigated masks\n",
    "# This divides every sample into a 'purely' irrigated section (irr) and an unirrigated one (inv_irr)\n",
    "# This allows us to build a model for irrigated areas that aren't contaminated by unirrigated areas.\n",
    "\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting ETf ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='drive', \n",
    "            state_col=cfg.state_col, drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='bucket', \n",
    "            file_prefix=file_prefix, state_col=cfg.state_col, drive_categorize=False\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ETf (inv_irr)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "state_col must be provided for clustered ETf extraction",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m      9\u001B[39m     clustered_sample_etf(\n\u001B[32m     10\u001B[39m         cfg.fields_shapefile, bucket=\u001B[38;5;28;01mNone\u001B[39;00m, debug=\u001B[38;5;28;01mFalse\u001B[39;00m, mask_type=mask, \n\u001B[32m     11\u001B[39m         check_dir=\u001B[38;5;28;01mNone\u001B[39;00m, start_yr=start_year, end_yr=end_year, \n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m         state_col=cfg.state_col, drive_folder=drive_folder, drive_categorize=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     15\u001B[39m     )\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     \u001B[43mclustered_sample_etf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfields_shapefile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbucket\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mee_bucket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheck_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_yr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstart_year\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_yr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mend_year\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfeature_id_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mselect_fields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43metf_target_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musgs_nhm\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbucket\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfile_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfile_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_col\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstate_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrive_categorize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/data_extraction/ee/etf_export.py:404\u001B[39m, in \u001B[36mclustered_sample_etf\u001B[39m\u001B[34m(feature_coll, bucket, debug, mask_type, check_dir, start_yr, end_yr, feature_id, select, state_col, model, usgs_nhm, source, scale, dest, drive_folder, file_prefix, drive_categorize)\u001B[39m\n\u001B[32m    402\u001B[39m \u001B[38;5;66;03m# Determine region: all-west or all-east based on state_col\u001B[39;00m\n\u001B[32m    403\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m state_col \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m404\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mstate_col must be provided for clustered ETf extraction\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    405\u001B[39m states_distinct = ee.List(feature_coll.aggregate_array(state_col)).distinct().getInfo()\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m states_distinct:\n",
      "\u001B[31mValueError\u001B[39m: state_col must be provided for clustered ETf extraction"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Extract NDVI Raster Data\n",
    "\n",
    "This is just like the ETf extraction, but for NDVI. This is a little more straightforward as we can get the data straight from the Landsat collection, and don't need special permissions or knowledge of where the data are stored.\n",
    "\n",
    "As with the ETf code, the extraction has three options to get at the data:\n",
    "* **`clustered_sample_ndvi`**: For clustered fields (what we use here)\n",
    "* **`sparse_sample_ndvi`**: For fields spread across many Landsat images\n",
    "* **`export_ndvi_images`**: For exporting the Landsat images themselves"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:19:50.533923802Z",
     "start_time": "2026-01-09T22:19:48.465006173Z"
    }
   },
   "source": [
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting NDVI ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='drive', \n",
    "            drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='bucket', \n",
    "            file_prefix=file_prefix, drive_categorize=False\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting NDVI (inv_irr)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      5\u001B[39m     clustered_sample_ndvi(\n\u001B[32m      6\u001B[39m         cfg.fields_shapefile, bucket=\u001B[38;5;28;01mNone\u001B[39;00m, debug=\u001B[38;5;28;01mFalse\u001B[39;00m, mask_type=mask, \n\u001B[32m      7\u001B[39m         check_dir=\u001B[38;5;28;01mNone\u001B[39;00m, start_yr=start_year, end_yr=end_year, \n\u001B[32m   (...)\u001B[39m\u001B[32m     10\u001B[39m         drive_folder=drive_folder, drive_categorize=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     11\u001B[39m     )\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     \u001B[43mclustered_sample_ndvi\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfields_shapefile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbucket\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mee_bucket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheck_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_yr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstart_year\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_yr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mend_year\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfeature_id_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mselect_fields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43msatellite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlandsat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbucket\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfile_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfile_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrive_categorize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/data_extraction/ee/ndvi_export.py:380\u001B[39m, in \u001B[36mclustered_sample_ndvi\u001B[39m\u001B[34m(feature_coll, bucket, debug, mask_type, check_dir, start_yr, end_yr, feature_id, select, state_col, satellite, dest, drive_folder, file_prefix, drive_categorize)\u001B[39m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    378\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mMust choose a satellite from landsat or sentinel\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m380\u001B[39m ndvi_scenes = \u001B[43mcoll\u001B[49m\u001B[43m.\u001B[49m\u001B[43maggregate_histogram\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msystem:index\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetInfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m img_id \u001B[38;5;129;01min\u001B[39;00m ndvi_scenes:\n\u001B[32m    384\u001B[39m     splt = img_id.split(\u001B[33m'\u001B[39m\u001B[33m_\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/ee/computedobject.py:108\u001B[39m, in \u001B[36mComputedObject.getInfo\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgetInfo\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Any | \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    103\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Fetch and return information about this object.\u001B[39;00m\n\u001B[32m    104\u001B[39m \n\u001B[32m    105\u001B[39m \u001B[33;03m  Returns:\u001B[39;00m\n\u001B[32m    106\u001B[39m \u001B[33;03m    The object can evaluate to anything.\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcomputeValue\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/ee/data.py:1064\u001B[39m, in \u001B[36mcomputeValue\u001B[39m\u001B[34m(obj)\u001B[39m\n\u001B[32m   1061\u001B[39m body = {\u001B[33m'\u001B[39m\u001B[33mexpression\u001B[39m\u001B[33m'\u001B[39m: serializer.encode(obj, for_cloud_api=\u001B[38;5;28;01mTrue\u001B[39;00m)}\n\u001B[32m   1062\u001B[39m _maybe_populate_workload_tag(body)\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_execute_cloud_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_get_cloud_projects\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_get_projects_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprettyPrint\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   1068\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mresult\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/ee/data.py:349\u001B[39m, in \u001B[36m_execute_cloud_call\u001B[39m\u001B[34m(call, num_retries)\u001B[39m\n\u001B[32m    347\u001B[39m num_retries = _get_state().max_retries \u001B[38;5;28;01mif\u001B[39;00m num_retries \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m num_retries\n\u001B[32m    348\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m googleapiclient.errors.HttpError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    351\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m _translate_cloud_exception(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001B[39m, in \u001B[36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m positional_parameters_enforcement == POSITIONAL_WARNING:\n\u001B[32m    129\u001B[39m         logger.warning(message)\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/googleapiclient/http.py:923\u001B[39m, in \u001B[36mHttpRequest.execute\u001B[39m\u001B[34m(self, http, num_retries)\u001B[39m\n\u001B[32m    920\u001B[39m     \u001B[38;5;28mself\u001B[39m.headers[\u001B[33m\"\u001B[39m\u001B[33mcontent-length\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.body))\n\u001B[32m    922\u001B[39m \u001B[38;5;66;03m# Handle retries for server-side errors.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m923\u001B[39m resp, content = \u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrequest\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sleep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_rand\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43muri\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.response_callbacks:\n\u001B[32m    936\u001B[39m     callback(resp)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/googleapiclient/http.py:191\u001B[39m, in \u001B[36m_retry_request\u001B[39m\u001B[34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    190\u001B[39m     exception = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     resp, content = \u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[38;5;66;03m# Retry on SSL errors and socket timeout errors.\u001B[39;00m\n\u001B[32m    193\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _ssl_SSLError \u001B[38;5;28;01mas\u001B[39;00m ssl_error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/google_auth_httplib2.py:216\u001B[39m, in \u001B[36mAuthorizedHttp.request\u001B[39m\u001B[34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001B[39m\n\u001B[32m    213\u001B[39m     body_stream_position = body.tell()\n\u001B[32m    215\u001B[39m \u001B[38;5;66;03m# Make the request.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m response, content = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m    \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[43mredirections\u001B[49m\u001B[43m=\u001B[49m\u001B[43mredirections\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconnection_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconnection_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[38;5;66;03m# If the response indicated that the credentials needed to be\u001B[39;00m\n\u001B[32m    227\u001B[39m \u001B[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001B[39;00m\n\u001B[32m    228\u001B[39m \u001B[38;5;66;03m# request.\u001B[39;00m\n\u001B[32m    229\u001B[39m \u001B[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001B[39;00m\n\u001B[32m    230\u001B[39m \u001B[38;5;66;03m# the time the request is made, so we may need to try twice.\u001B[39;00m\n\u001B[32m    231\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    232\u001B[39m     response.status \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._refresh_status_codes\n\u001B[32m    233\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m _credential_refresh_attempt < \u001B[38;5;28mself\u001B[39m._max_refresh_attempts\n\u001B[32m    234\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/ee/_cloud_api_utils.py:79\u001B[39m, in \u001B[36m_Http.request\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m redirections  \u001B[38;5;66;03m# Ignored\u001B[39;00m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     75\u001B[39m   \u001B[38;5;66;03m# googleapiclient is expecting an httplib2 object, and doesn't include\u001B[39;00m\n\u001B[32m     76\u001B[39m   \u001B[38;5;66;03m# requests error in the list of transient errors. Therefore, transient\u001B[39;00m\n\u001B[32m     77\u001B[39m   \u001B[38;5;66;03m# requests errors should be converted to kinds that googleapiclient\u001B[39;00m\n\u001B[32m     78\u001B[39m   \u001B[38;5;66;03m# consider transient.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_session\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     80\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_timeout\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m requests.exceptions.ConnectionError \u001B[38;5;28;01mas\u001B[39;00m connection_error:\n\u001B[32m     83\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(connection_error) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mconnection_error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/requests/sessions.py:589\u001B[39m, in \u001B[36mSession.request\u001B[39m\u001B[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[39m\n\u001B[32m    584\u001B[39m send_kwargs = {\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m: timeout,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m: allow_redirects,\n\u001B[32m    587\u001B[39m }\n\u001B[32m    588\u001B[39m send_kwargs.update(settings)\n\u001B[32m--> \u001B[39m\u001B[32m589\u001B[39m resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/requests/sessions.py:703\u001B[39m, in \u001B[36mSession.send\u001B[39m\u001B[34m(self, request, **kwargs)\u001B[39m\n\u001B[32m    700\u001B[39m start = preferred_clock()\n\u001B[32m    702\u001B[39m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m r = \u001B[43madapter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[32m    706\u001B[39m elapsed = preferred_clock() - start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/requests/adapters.py:644\u001B[39m, in \u001B[36mHTTPAdapter.send\u001B[39m\u001B[34m(self, request, stream, timeout, verify, cert, proxies)\u001B[39m\n\u001B[32m    641\u001B[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001B[32m    643\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m644\u001B[39m     resp = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    645\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    646\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    647\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    648\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    651\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    654\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    655\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    656\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    659\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request=request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001B[39m, in \u001B[36mHTTPConnectionPool.urlopen\u001B[39m\u001B[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[39m\n\u001B[32m    784\u001B[39m response_conn = conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    786\u001B[39m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m787\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    788\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    789\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    790\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    791\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    792\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    799\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[32m    803\u001B[39m clean_exit = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001B[39m, in \u001B[36mHTTPConnectionPool._make_request\u001B[39m\u001B[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[39m\n\u001B[32m    532\u001B[39m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m534\u001B[39m     response = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    536\u001B[39m     \u001B[38;5;28mself\u001B[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/site-packages/urllib3/connection.py:571\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    568\u001B[39m _shutdown = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.sock, \u001B[33m\"\u001B[39m\u001B[33mshutdown\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    570\u001B[39m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m571\u001B[39m httplib_response = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    574\u001B[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/http/client.py:1395\u001B[39m, in \u001B[36mHTTPConnection.getresponse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1393\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1394\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1395\u001B[39m         \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1396\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[32m   1397\u001B[39m         \u001B[38;5;28mself\u001B[39m.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/http/client.py:325\u001B[39m, in \u001B[36mHTTPResponse.begin\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[32m    324\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m     version, status, reason = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m status != CONTINUE:\n\u001B[32m    327\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/http/client.py:286\u001B[39m, in \u001B[36mHTTPResponse._read_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m     line = \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m.fp.readline(_MAXLINE + \u001B[32m1\u001B[39m), \u001B[33m\"\u001B[39m\u001B[33miso-8859-1\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    287\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) > _MAXLINE:\n\u001B[32m    288\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[33m\"\u001B[39m\u001B[33mstatus line\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/socket.py:718\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    716\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    717\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m718\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    719\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    720\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/ssl.py:1314\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1310\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1311\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1312\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1313\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1314\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1315\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1316\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/swim/lib/python3.11/ssl.py:1166\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1164\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1165\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1166\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1167\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1168\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Snow, Irrigation, and Soils Extraction\n",
    "\n",
    "For the raster data extraction, there are three functions we need to run:\n",
    "\n",
    "* **`sample_snodas_swe`**: This function iterates over the daily SNODAS images in Earth Engine, extracting mean SWE for each sample plot for each day, September through May. (https://nsidc.org/data/g02158/versions/1)\n",
    "\n",
    "* **`get_irrigation`**: This function uses IrrMapper to get statistics about the irrigation status of each plot for each year, including the fraction of the plot that was irrigated. (https://www.mdpi.com/2072-4292/12/14/2328)\n",
    "\n",
    "* **`get_ssurgo`**: This function uses data summarized and put in a public Earth Engine asset by Charles Morton at Desert Research Institute from SSURGO to summarize plot-scale soil texture and hydraulic properties used by SWIM-RS.\n",
    "\n",
    "Note: The module also has functions for extracting vegetation height (`get_landfire`) and crop type (`get_cdl`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## SWE Data (SNODAS)\n",
    "\n",
    "Notes:\n",
    "- Ensure that your Cloud Storage bucket has the correct permissions for Earth Engine to write to it.\n",
    "- This will produce a monthly dataset for Sep - May, regardless of SWE status at the sample plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SNODAS SWE...\")\n",
    "sample_snodas_swe(\n",
    "    cfg.fields_shapefile, bucket=export_bucket, debug=False, \n",
    "    check_dir=None, overwrite=False, feature_id=cfg.feature_id_col, \n",
    "    dest=export_dest, drive_folder=drive_folder, drive_categorize=True,\n",
    "    file_prefix=file_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Irrigation Data (IrrMapper/LANID)\n",
    "\n",
    "This will produce an annual dataset of the IrrMapper-estimated irrigated fraction for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting irrigation data...\")\n",
    "get_irrigation(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_irr', debug=False, \n",
    "    selector=cfg.feature_id_col, lanid=True, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Land Cover Data\n",
    "\n",
    "Extract dominant landcover (MODIS LC_Type1 + FROM-GLC10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting land cover data...\")\n",
    "get_landcover(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_landcover', debug=False, \n",
    "    selector=cfg.feature_id_col, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Soils Data (SSURGO)\n",
    "\n",
    "This will produce a single dataset of the SSURGO-estimated soil properties for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SSURGO soil data...\")\n",
    "get_ssurgo(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_ssurgo', debug=False, \n",
    "    selector=cfg.feature_id_col, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Meteorology Data Extraction (GridMET)\n",
    "\n",
    "In this section, we will:\n",
    "1. Associate sample plots (fields) with their nearest GridMET cell\n",
    "2. Extract GridMET bias-correction information from DRI's rasters\n",
    "3. Download GridMET data from the THREDDS server, and NLDAS-2 hourly precipitation data\n",
    "\n",
    "Read about GridMET: https://www.climatologylab.org/gridmet.html  \n",
    "Read about NLDAS-2: https://ldas.gsfc.nasa.gov/nldas/nldas-2-model-data\n",
    "\n",
    "## GridMET Cell Assignment\n",
    "\n",
    "Our fields are in a pretty tight cluster. We're preparing to download meteorology from a 4-km resolution dataset (GridMET), so it's unnecessary to download a meteorology time series for each field, as many will just be copies. Rather, we'll identify the GridMET 'cells' with a shapefile, and find the closest cell to each field.\n",
    "\n",
    "In addition to the raw meteorology data, we will also be accessing rasters that show the observed bias between AgriMet weather stations and GridMET's reference ET. These biases are due to the impacts of irrigated agriculture on the near-surface atmosphere, which often tends to see relatively high humidity and low temperature compared to arid and semi-arid surroundings. This bias is documented in Blankeneau (2020; https://doi.org/10.1016/j.agwat.2020.106376). The bias correction surfaces were mapped over CONUS by Desert Research Institute and OpenET and are documented in Melton et al., 2021 (https://doi.org/10.1111/1752-1688.12956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swimrs.data_extraction.gridmet.gridmet import assign_gridmet_and_corrections, download_gridmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each field with nearest GridMET cell and extract bias correction factors\n",
    "assign_gridmet_and_corrections(\n",
    "    fields=cfg.fields_shapefile, \n",
    "    gridmet_points=cfg.gridmet_centroids, \n",
    "    gridmet_ras=cfg.gridmet_corr_dir, \n",
    "    fields_join=cfg.gridmet_mapping, \n",
    "    factors_js=cfg.gridmet_factors, \n",
    "    feature_id=cfg.feature_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "This should print 'Get gridmet for 4 target points', as there should be only four unique GridMET cells that are closest to each of the fields. Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import random\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The GeoDataFrame you are attempting to plot is empty.\")\n\ngdf = gpd.read_file(cfg.fields_shapefile)\ncdf = gpd.read_file(cfg.gridmet_centroids)\ngdf_gfid = gpd.read_file(cfg.gridmet_mapping)\n\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.epsg(5071)})\n\n# Color fields by their assigned GridMET cell\nunique_gfids = set(cdf['GFID'].unique()).union(gdf_gfid['GFID'].unique())\ncolors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(unique_gfids))]\ncolor_map = dict(zip(unique_gfids, colors))\n\nfor gfid, color in color_map.items():\n    cdf[cdf['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color, transform=ccrs.epsg(5071))\n    gdf_gfid[gdf_gfid['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color, transform=ccrs.epsg(5071))\n\n# Hybrid basemap: satellite imagery + labels overlay\nctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs=ccrs.epsg(5071))\nctx.add_basemap(ax, source=ctx.providers.Stadia.StamenTonerLabels, crs=ccrs.epsg(5071), alpha=0.8)\n\nplt.title('Fields colored by GridMET Cell Assignment')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "Each field's color should match that of the nearest GridMET centroid. The 'GFID' for each field has been saved in the output shapefile. This reduces the data we must download by a large factor.\n",
    "\n",
    "## Download GridMET Data\n",
    "\n",
    "Now we download the daily meteorological timeseries from GridMET's THREDDS server. This will probably take a few minutes.\n",
    "\n",
    "Note: Under the hood, this code will also be downloading hourly precipitation data from NLDAS-2. This is helpful on days when there is precipitation and we want to know its intensity for modeling purposes (to estimate runoff/recharge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.met_dir, exist_ok=True)\n",
    "\n",
    "download_gridmet(\n",
    "    cfg.gridmet_mapping, cfg.gridmet_factors, cfg.met_dir, \n",
    "    start='1987-01-01', end='2023-12-31',\n",
    "    target_fields=None, overwrite=False, feature_id=cfg.feature_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "Let's look at one of the GridMET time series and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Find a GridMET file\n",
    "met_files = [f for f in os.listdir(cfg.met_dir) if f.endswith('.csv')]\n",
    "if met_files:\n",
    "    met_data = os.path.join(cfg.met_dir, met_files[0])\n",
    "    met_df = pd.read_csv(met_data, index_col='date')\n",
    "    print(f\"Loaded {met_files[0]}\")\n",
    "    print(met_df.head())\n",
    "    print(f\"\\nColumns: {list(met_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Here, we see we have information on the date, the location, and daily meteorological information from GridMET, including `tmin_c`, `tmax_c`, and `prcp_mm`. We see the critical reference ET estimates in 'uncorrected' form (`eto_mm_uncorr` and `etr_mm_uncorr`), which we could use in natural vegetation, and also in 'corrected' form (`eto_mm` and `etr_mm`), which we will be using over our irrigated study area. We also see `prcp_hr_XX`, which is the hourly NLDAS-2 precipitation estimate.\n",
    "\n",
    "---\n",
    "\n",
    "# Part D: Sync Data from Cloud Storage\n",
    "\n",
    "Once your Earth Engine export tasks have completed (monitor at https://code.earthengine.google.com/tasks), sync the data from your Cloud Storage bucket to your local filesystem.\n",
    "\n",
    "**Note**: If using Google Drive export (`USE_DRIVE = True`), you'll need to manually download files from Drive instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview what will be synced (dry run)\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Preview of files to sync (dry run):\")\n",
    "    cfg.sync_from_bucket(dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually sync data from bucket\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Syncing data from bucket...\")\n",
    "    cfg.sync_from_bucket(dry_run=False)\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Convert SNODAS to Time Series JSON\n",
    "\n",
    "After syncing, convert the month-by-month SNODAS files to per-field daily time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month-by-month SNODAS files to per-field daily time series\n",
    "snow_extracts = os.path.join(cfg.data_dir, 'snow', 'snodas', 'extracts')\n",
    "snow_out = os.path.join(cfg.data_dir, 'snow', 'snodas', 'snodas.json')\n",
    "\n",
    "if os.path.exists(snow_extracts):\n",
    "    os.makedirs(os.path.dirname(snow_out), exist_ok=True)\n",
    "    create_timeseries_json(snow_extracts, snow_out, feature_id=cfg.feature_id_col)\n",
    "    print(f\"Created SNODAS time series: {snow_out}\")\n",
    "else:\n",
    "    print(f\"SNODAS extracts not found at {snow_extracts}\")\n",
    "    print(\"Run sync_from_bucket() after EE tasks complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Data extraction is complete. After syncing from the bucket, you should have:\n",
    "- `data/remote_sensing/landsat/extracts/ssebop_etf/` - ETf CSVs by year and mask\n",
    "- `data/remote_sensing/landsat/extracts/ndvi/` - NDVI CSVs by year and mask  \n",
    "- `data/snow/snodas/snodas.json` - Per-field SWE time series\n",
    "- `data/properties/` - Irrigation, landcover, and soils CSVs\n",
    "- `data/met_timeseries/gridmet/` - GridMET meteorology CSVs\n",
    "\n",
    "**Next**: Run notebook 03 to ingest this data into the SwimContainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
