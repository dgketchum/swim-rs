{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preparation Overview\n",
    "\n",
    "In this tutorial we will extract all the data we need to run both the uncalibrated SWIM-RS model and to calibrate and run a parameterized model. The model uses a daily soil water balance, and as such, it needs a daily estimate of meteorological drivers and some *a priori* information on the soils in our sample locations. It also needs an estimate of the state of the vegetation on the surface, for which we use Landsat-based NDVI. With this information, we will be able to run SWIM-RS to estimate the daily ET, soil water storage, recharge, runoff, and simulated irrigation.\n",
    "\n",
    "To calibrate the model so it behaves more realistically, we must use the parameter inversion software (PEST++, in this case) alongside target data which provides *somewhat* independent estimates of ET and snow on the ground. Once the model is calibrated for a sample plot, these data are no longer needed. Therefore, the calibrated model can be run for periods before or after SNODAS or ETf data is available.\n",
    "\n",
    "The remote sensing data (ETf and NDVI) are the most time-consuming step, as the data are being extracted from potentially thousands of separate Landsat-like images in Earth Engine. The snow, soils, and irrigation data, on the other hand, are relatively quick, as the images are fewer, with one CONUS-wide image per day in SNODAS, a few static images for the soils data, and one image annually for the irrigation products. However, thanks to Earth Engine, even the Landsat-based products are quickly extracted if the number of sample plots are small and clustered in space, as they are for this tutorial.\n",
    "\n",
    "The SWIM-RS approach requires the following input datasets to run and/or calibrate:\n",
    "1. **NDVI**: Normalized Difference Vegetation Index is a measure made using the red and near-infrared bands of a multispectral instrument. It is a good way to estimate the relative density and vigor of vegetation, which is highly correlated with transpiration. NDVI is used as a proxy for the transpirative component of the crop coefficient in SWIM-RS, Kcb. Here, we access NDVI information from Landsat satellite images in Earth Engine.\n",
    "2. **ETf**: The rate of ET expressed as a fraction of reference/potential ET. This is also known in agricultural water use modeling as the 'crop coefficient', or Kc. For this tutorial we use SSEBop, accessed from Google Earth Engine. We could use results from any number of remote sensing-based modeling approaches (METRIC, OpenET ensemble, etc.). *FOR USE IN CALIBRATION ONLY*\n",
    "3. **Soils**: We need an initial estimate of soil hydraulic properties that govern the way water behaves in our very simple model soil water reservoir.\n",
    "4. **Irrigation**: We use an irrigation mask (IrrMapper or LANID) to constrain the data extraction of the irrigated and unirrigated portions of any given sample plot.\n",
    "5. **Snow**: We use the SNODAS product to estimate the snow water equivalent (SWE) at a daily time step to calibrate the simple snow model in SWIM-RS. *FOR USE IN CALIBRATION ONLY*\n",
    "\n",
    "Note: For this tutorial, we use 'field' and 'plot' somewhat interchangeably. Indeed, the sample plots for this tutorial are fields. However, we could draw an arbitrary polygon over a location of interest and run the model there. Keep in mind, the data will represent the mean of the irrigated and unirrigated portions of the sample plot. Therefore, using sensible land-use features (e.g., individual agricultural fields) is a good approach because assuming homogenous land-use management in a single field is not a terrible assumption.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Paths\n",
    "\n",
    "1. **Run extraction** (requires Earth Engine access): Execute the cells below to download data from Earth Engine and GridMET\n",
    "2. **Use pre-built data**: Skip to notebook 03 (data available in `data/prebuilt/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Authorize Earth Engine\n",
    "\n",
    "If new to Earth Engine, checkout https://developers.google.com/earth-engine/guides/auth"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:53:45.724122535Z",
     "start_time": "2026-01-09T22:53:45.690956702Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import ee\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.data_extraction.ee.etf_export import clustered_sample_etf\n",
    "from swimrs.data_extraction.ee.ndvi_export import clustered_sample_ndvi\n",
    "from swimrs.data_extraction.ee.snodas_export import sample_snodas_swe\n",
    "from swimrs.data_extraction.snodas.snodas import create_timeseries_json\n",
    "from swimrs.data_extraction.ee.ee_props import get_irrigation, get_ssurgo, get_landcover\n",
    "from swimrs.data_extraction.ee.ee_utils import is_authorized\n",
    "\n",
    "sys.setrecursionlimit(5000)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:53:46.887501214Z",
     "start_time": "2026-01-09T22:53:45.726382088Z"
    }
   },
   "source": [
    "if not is_authorized():\n",
    "    ee.Authenticate()\n",
    "ee.Initialize()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load project configuration from the TOML file. This provides all paths, date ranges, and bucket settings."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:53:47.004009326Z",
     "start_time": "2026-01-09T22:53:46.945602541Z"
    }
   },
   "source": [
    "# Load project configuration\n",
    "project_dir = os.path.abspath('.')\n",
    "config_file = os.path.join(project_dir, '1_Boulder.toml')\n",
    "\n",
    "cfg = ProjectConfig()\n",
    "cfg.read_config(config_file, project_root_override=project_dir)\n",
    "\n",
    "print(f\"Project: {cfg.project_name}\")\n",
    "print(f\"Bucket: {cfg.ee_bucket}\")\n",
    "print(f\"Date range: {cfg.start_dt} to {cfg.end_dt}\")\n",
    "print(f\"Shapefile: {cfg.fields_shapefile}\")\n",
    "print(f\"ETf model: {cfg.etf_target_model}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: 1_Boulder\n",
      "Bucket: wudr\n",
      "Date range: 2004-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "Shapefile: /home/dgketchum/code/swim-rs/examples/1_Boulder/data/gis/mt_sid_boulder.shp\n",
      "ETf model: ssebop\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:53:47.067108142Z",
     "start_time": "2026-01-09T22:53:47.006007366Z"
    }
   },
   "source": [
    "# Export destination - use Cloud Storage bucket (faster) or Google Drive\n",
    "# Change to True to use Google Drive instead of a bucket\n",
    "USE_DRIVE = False\n",
    "\n",
    "# Export settings derived from config\n",
    "export_dest = 'drive' if USE_DRIVE else 'bucket'\n",
    "export_bucket = None if USE_DRIVE else cfg.ee_bucket\n",
    "file_prefix = cfg.project_name  # Bucket path prefix\n",
    "\n",
    "# Drive folder from config (or default to project name)\n",
    "drive_folder = cfg.resolved_config.get('earth_engine', {}).get('drive_folder', cfg.project_name)\n",
    "\n",
    "# Date range from config\n",
    "start_year = cfg.start_dt.year\n",
    "end_year = cfg.end_dt.year\n",
    "\n",
    "# Optional: Limit extraction to specific fields for testing\n",
    "select_fields = ['043_000130', '043_000128', '043_000161']\n",
    "\n",
    "# running all fields will take a bit longer\n",
    "# select_fields = None"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Remote Sensing Extraction (ETf and NDVI)\n",
    "\n",
    "## Extract ETf Raster Data\n",
    "\n",
    "Now we're ready to do 'zonal stats' on our fields. We provide a local shapefile, and the code converts it to a FeatureCollection internally, then extracts ETf/NDVI summaries per field to CSV.\n",
    "\n",
    "We need to use an irrigated lands mask (IrrMapper or LANID) to find irrigated and unirrigated zones within the polygons of our shapefile. You see this implemented in the code below, where the `mask` argument is either `irr` for irrigated, or `inv_irr` for the inverse of the irrigated mask, which are unirrigated areas.\n",
    "\n",
    "For the raster data extraction, there are three options to get at the data:\n",
    "\n",
    "* **`clustered_sample_etf`**: This function finds all Landsat images intersecting the sample polygons (i.e., our fields). Since our fields are clustered together, this finds a reasonable number of images and iterates over them, extracting data from each. We use this on the tutorial since the sample from the Montana fields database is geographically constrained.\n",
    "\n",
    "* **`sparse_sample_etf`**: This function assumes the samples (fields) are spread out over many different Landsat images. It runs sample-by-sample, finding Landsat images overlapping each sample and extracting from them. This is used when we extract data for widely-spaced sites across the Conterminous US in examples 4 and 5, or globally, as in example 6.\n",
    "\n",
    "* **`export_etf_images`**: This function exports the Landsat images themselves, clipped to the bounds of a 'hopefully' clustered set of sample polygons. This is helpful for experimentation with buffering zones and so on, but not meant for large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:55:19.074057895Z",
     "start_time": "2026-01-09T22:53:52.314632311Z"
    }
   },
   "source": [
    "# Extract ETf for both irrigated and unirrigated masks\n",
    "# This divides every sample into a 'purely' irrigated section (irr) and an unirrigated one (inv_irr)\n",
    "# This allows us to build a model for irrigated areas that aren't contaminated by unirrigated areas.\n",
    "\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting ETf ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='drive', \n",
    "            state_col=cfg.state_col, drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_etf(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            model=cfg.etf_target_model, usgs_nhm=True, dest='bucket', \n",
    "            file_prefix=file_prefix, state_col=cfg.state_col, drive_categorize=False\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ETf (inv_irr)...\n",
      "etf_inv_irr_2004\n",
      "etf_inv_irr_2005\n",
      "etf_inv_irr_2006\n",
      "etf_inv_irr_2007\n",
      "etf_inv_irr_2008\n",
      "etf_inv_irr_2009\n",
      "etf_inv_irr_2010\n",
      "etf_inv_irr_2011\n",
      "etf_inv_irr_2012\n",
      "etf_inv_irr_2013\n",
      "etf_inv_irr_2014\n",
      "etf_inv_irr_2015\n",
      "etf_inv_irr_2016\n",
      "etf_inv_irr_2017\n",
      "etf_inv_irr_2018\n",
      "etf_inv_irr_2019\n",
      "etf_inv_irr_2020\n",
      "etf_inv_irr_2021\n",
      "etf_inv_irr_2022\n",
      "Extracting ETf (irr)...\n",
      "etf_irr_2004\n",
      "etf_irr_2005\n",
      "etf_irr_2006\n",
      "etf_irr_2007\n",
      "etf_irr_2008\n",
      "etf_irr_2009\n",
      "etf_irr_2010\n",
      "etf_irr_2011\n",
      "etf_irr_2012\n",
      "etf_irr_2013\n",
      "etf_irr_2014\n",
      "etf_irr_2015\n",
      "etf_irr_2016\n",
      "etf_irr_2017\n",
      "etf_irr_2018\n",
      "etf_irr_2019\n",
      "etf_irr_2020\n",
      "etf_irr_2021\n",
      "etf_irr_2022\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Extract NDVI Raster Data\n",
    "\n",
    "This is just like the ETf extraction, but for NDVI. This is a little more straightforward as we can get the data straight from the Landsat collection, and don't need special permissions or knowledge of where the data are stored.\n",
    "\n",
    "As with the ETf code, the extraction has three options to get at the data:\n",
    "* **`clustered_sample_ndvi`**: For clustered fields (what we use here)\n",
    "* **`sparse_sample_ndvi`**: For fields spread across many Landsat images\n",
    "* **`export_ndvi_images`**: For exporting the Landsat images themselves"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T22:57:12.640780125Z",
     "start_time": "2026-01-09T22:55:19.078175061Z"
    }
   },
   "source": [
    "for mask in ['inv_irr', 'irr']:\n",
    "    print(f\"Extracting NDVI ({mask})...\")\n",
    "    \n",
    "    if USE_DRIVE:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=None, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='drive', \n",
    "            drive_folder=drive_folder, drive_categorize=True\n",
    "        )\n",
    "    else:\n",
    "        clustered_sample_ndvi(\n",
    "            cfg.fields_shapefile, bucket=cfg.ee_bucket, debug=False, mask_type=mask, \n",
    "            check_dir=None, start_yr=start_year, end_yr=end_year, \n",
    "            feature_id=cfg.feature_id_col, select=select_fields, \n",
    "            satellite='landsat', dest='bucket', \n",
    "            file_prefix=file_prefix, drive_categorize=False\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting NDVI (inv_irr)...\n",
      "ndvi_2004_inv_irr\n",
      "ndvi_2005_inv_irr\n",
      "ndvi_2006_inv_irr\n",
      "ndvi_2007_inv_irr\n",
      "ndvi_2008_inv_irr\n",
      "ndvi_2009_inv_irr\n",
      "ndvi_2010_inv_irr\n",
      "ndvi_2011_inv_irr\n",
      "ndvi_2012_inv_irr\n",
      "ndvi_2013_inv_irr\n",
      "ndvi_2014_inv_irr\n",
      "ndvi_2015_inv_irr\n",
      "ndvi_2016_inv_irr\n",
      "ndvi_2017_inv_irr\n",
      "ndvi_2018_inv_irr\n",
      "ndvi_2019_inv_irr\n",
      "ndvi_2020_inv_irr\n",
      "ndvi_2021_inv_irr\n",
      "ndvi_2022_inv_irr\n",
      "Extracting NDVI (irr)...\n",
      "ndvi_2004_irr\n",
      "ndvi_2005_irr\n",
      "ndvi_2006_irr\n",
      "ndvi_2007_irr\n",
      "ndvi_2008_irr\n",
      "ndvi_2009_irr\n",
      "ndvi_2010_irr\n",
      "ndvi_2011_irr\n",
      "ndvi_2012_irr\n",
      "ndvi_2013_irr\n",
      "ndvi_2014_irr\n",
      "ndvi_2015_irr\n",
      "ndvi_2016_irr\n",
      "ndvi_2017_irr\n",
      "ndvi_2018_irr\n",
      "ndvi_2019_irr\n",
      "ndvi_2020_irr\n",
      "ndvi_2021_irr\n",
      "ndvi_2022_irr\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Snow, Irrigation, and Soils Extraction\n",
    "\n",
    "For the raster data extraction, there are three functions we need to run:\n",
    "\n",
    "* **`sample_snodas_swe`**: This function iterates over the daily SNODAS images in Earth Engine, extracting mean SWE for each sample plot for each day, September through May. (https://nsidc.org/data/g02158/versions/1)\n",
    "\n",
    "* **`get_irrigation`**: This function uses IrrMapper to get statistics about the irrigation status of each plot for each year, including the fraction of the plot that was irrigated. (https://www.mdpi.com/2072-4292/12/14/2328)\n",
    "\n",
    "* **`get_ssurgo`**: This function uses data summarized and put in a public Earth Engine asset by Charles Morton at Desert Research Institute from SSURGO to summarize plot-scale soil texture and hydraulic properties used by SWIM-RS.\n",
    "\n",
    "Note: The module also has functions for extracting vegetation height (`get_landfire`) and crop type (`get_cdl`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## SWE Data (SNODAS)\n",
    "\n",
    "Notes:\n",
    "- Ensure that your Cloud Storage bucket has the correct permissions for Earth Engine to write to it.\n",
    "- This will produce a monthly dataset for Sep - May, regardless of SWE status at the sample plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SNODAS SWE...\")\n",
    "sample_snodas_swe(\n",
    "    cfg.fields_shapefile, bucket=export_bucket, debug=False, \n",
    "    check_dir=None, overwrite=False, feature_id=cfg.feature_id_col, \n",
    "    dest=export_dest, drive_folder=drive_folder, drive_categorize=True,\n",
    "    file_prefix=file_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Irrigation Data (IrrMapper/LANID)\n",
    "\n",
    "This will produce an annual dataset of the IrrMapper-estimated irrigated fraction for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting irrigation data...\")\n",
    "get_irrigation(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_irr', debug=False, \n",
    "    selector=cfg.feature_id_col, lanid=True, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Land Cover Data\n",
    "\n",
    "Extract dominant landcover (MODIS LC_Type1 + FROM-GLC10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting land cover data...\")\n",
    "get_landcover(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_landcover', debug=False, \n",
    "    selector=cfg.feature_id_col, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Soils Data (SSURGO)\n",
    "\n",
    "This will produce a single dataset of the SSURGO-estimated soil properties for each sample plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SSURGO soil data...\")\n",
    "get_ssurgo(\n",
    "    cfg.fields_shapefile, description=f'{cfg.project_name}_ssurgo', debug=False, \n",
    "    selector=cfg.feature_id_col, dest=export_dest, \n",
    "    bucket=export_bucket, file_prefix=file_prefix,\n",
    "    drive_folder=drive_folder, drive_categorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Meteorology Data Extraction (GridMET)\n",
    "\n",
    "In this section, we will:\n",
    "1. Associate sample plots (fields) with their nearest GridMET cell\n",
    "2. Extract GridMET bias-correction information from DRI's rasters\n",
    "3. Download GridMET data from the THREDDS server, and NLDAS-2 hourly precipitation data\n",
    "\n",
    "Read about GridMET: https://www.climatologylab.org/gridmet.html  \n",
    "Read about NLDAS-2: https://ldas.gsfc.nasa.gov/nldas/nldas-2-model-data\n",
    "\n",
    "## GridMET Cell Assignment\n",
    "\n",
    "Our fields are in a pretty tight cluster. We're preparing to download meteorology from a 4-km resolution dataset (GridMET), so it's unnecessary to download a meteorology time series for each field, as many will just be copies. Rather, we'll identify the GridMET 'cells' with a shapefile, and find the closest cell to each field.\n",
    "\n",
    "In addition to the raw meteorology data, we will also be accessing rasters that show the observed bias between AgriMet weather stations and GridMET's reference ET. These biases are due to the impacts of irrigated agriculture on the near-surface atmosphere, which often tends to see relatively high humidity and low temperature compared to arid and semi-arid surroundings. This bias is documented in Blankeneau (2020; https://doi.org/10.1016/j.agwat.2020.106376). The bias correction surfaces were mapped over CONUS by Desert Research Institute and OpenET and are documented in Melton et al., 2021 (https://doi.org/10.1111/1752-1688.12956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swimrs.data_extraction.gridmet.gridmet import assign_gridmet_and_corrections, download_gridmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each field with nearest GridMET cell and extract bias correction factors\n",
    "assign_gridmet_and_corrections(\n",
    "    fields=cfg.fields_shapefile, \n",
    "    gridmet_points=cfg.gridmet_centroids, \n",
    "    gridmet_ras=cfg.gridmet_corr_dir, \n",
    "    fields_join=cfg.gridmet_mapping, \n",
    "    factors_js=cfg.gridmet_factors, \n",
    "    feature_id=cfg.feature_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "This should print 'Get gridmet for 4 target points', as there should be only four unique GridMET cells that are closest to each of the fields. Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import random\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The GeoDataFrame you are attempting to plot is empty.\")\n\ngdf = gpd.read_file(cfg.fields_shapefile)\ncdf = gpd.read_file(cfg.gridmet_centroids)\ngdf_gfid = gpd.read_file(cfg.gridmet_mapping)\n\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.epsg(5071)})\n\n# Color fields by their assigned GridMET cell\nunique_gfids = set(cdf['GFID'].unique()).union(gdf_gfid['GFID'].unique())\ncolors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(unique_gfids))]\ncolor_map = dict(zip(unique_gfids, colors))\n\nfor gfid, color in color_map.items():\n    cdf[cdf['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color, transform=ccrs.epsg(5071))\n    gdf_gfid[gdf_gfid['GFID'] == gfid].plot(ax=ax, edgecolor='black', color=color, transform=ccrs.epsg(5071))\n\n# Hybrid basemap: satellite imagery + labels overlay\nctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs=ccrs.epsg(5071))\nctx.add_basemap(ax, source=ctx.providers.Stadia.StamenTonerLabels, crs=ccrs.epsg(5071), alpha=0.8)\n\nplt.title('Fields colored by GridMET Cell Assignment')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "Each field's color should match that of the nearest GridMET centroid. The 'GFID' for each field has been saved in the output shapefile. This reduces the data we must download by a large factor.\n",
    "\n",
    "## Download GridMET Data\n",
    "\n",
    "Now we download the daily meteorological timeseries from GridMET's THREDDS server. This will probably take a few minutes.\n",
    "\n",
    "Note: Under the hood, this code will also be downloading hourly precipitation data from NLDAS-2. This is helpful on days when there is precipitation and we want to know its intensity for modeling purposes (to estimate runoff/recharge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.met_dir, exist_ok=True)\n",
    "\n",
    "download_gridmet(\n",
    "    cfg.gridmet_mapping, cfg.gridmet_factors, cfg.met_dir, \n",
    "    start='1987-01-01', end='2023-12-31',\n",
    "    target_fields=None, overwrite=False, feature_id=cfg.feature_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "Let's look at one of the GridMET time series and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Find a GridMET file\n",
    "met_files = [f for f in os.listdir(cfg.met_dir) if f.endswith('.csv')]\n",
    "if met_files:\n",
    "    met_data = os.path.join(cfg.met_dir, met_files[0])\n",
    "    met_df = pd.read_csv(met_data, index_col='date')\n",
    "    print(f\"Loaded {met_files[0]}\")\n",
    "    print(met_df.head())\n",
    "    print(f\"\\nColumns: {list(met_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Here, we see we have information on the date, the location, and daily meteorological information from GridMET, including `tmin_c`, `tmax_c`, and `prcp_mm`. We see the critical reference ET estimates in 'uncorrected' form (`eto_mm_uncorr` and `etr_mm_uncorr`), which we could use in natural vegetation, and also in 'corrected' form (`eto_mm` and `etr_mm`), which we will be using over our irrigated study area. We also see `prcp_hr_XX`, which is the hourly NLDAS-2 precipitation estimate.\n",
    "\n",
    "---\n",
    "\n",
    "# Part D: Sync Data from Cloud Storage\n",
    "\n",
    "Once your Earth Engine export tasks have completed (monitor at https://code.earthengine.google.com/tasks), sync the data from your Cloud Storage bucket to your local filesystem.\n",
    "\n",
    "**Note**: If using Google Drive export (`USE_DRIVE = True`), you'll need to manually download files from Drive instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview what will be synced (dry run)\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Preview of files to sync (dry run):\")\n",
    "    cfg.sync_from_bucket(dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually sync data from bucket\n",
    "if not USE_DRIVE and cfg.ee_bucket:\n",
    "    print(\"Syncing data from bucket...\")\n",
    "    cfg.sync_from_bucket(dry_run=False)\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Convert SNODAS to Time Series JSON\n",
    "\n",
    "After syncing, convert the month-by-month SNODAS files to per-field daily time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month-by-month SNODAS files to per-field daily time series\n",
    "snow_extracts = os.path.join(cfg.data_dir, 'snow', 'snodas', 'extracts')\n",
    "snow_out = os.path.join(cfg.data_dir, 'snow', 'snodas', 'snodas.json')\n",
    "\n",
    "if os.path.exists(snow_extracts):\n",
    "    os.makedirs(os.path.dirname(snow_out), exist_ok=True)\n",
    "    create_timeseries_json(snow_extracts, snow_out, feature_id=cfg.feature_id_col)\n",
    "    print(f\"Created SNODAS time series: {snow_out}\")\n",
    "else:\n",
    "    print(f\"SNODAS extracts not found at {snow_extracts}\")\n",
    "    print(\"Run sync_from_bucket() after EE tasks complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Data extraction is complete. After syncing from the bucket, you should have:\n",
    "- `data/remote_sensing/landsat/extracts/ssebop_etf/` - ETf CSVs by year and mask\n",
    "- `data/remote_sensing/landsat/extracts/ndvi/` - NDVI CSVs by year and mask  \n",
    "- `data/snow/snodas/snodas.json` - Per-field SWE time series\n",
    "- `data/properties/` - Irrigation, landcover, and soils CSVs\n",
    "- `data/met_timeseries/gridmet/` - GridMET meteorology CSVs\n",
    "\n",
    "**Next**: Run notebook 03 to ingest this data into the SwimContainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
