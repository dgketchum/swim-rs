{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Ingest Data into the SwimContainer\n",
    "\n",
    "This notebook demonstrates how to ingest extracted data into the SwimContainer. The container provides a unified `ingest` API for all data sources:\n",
    "\n",
    "- `container.ingest.ndvi()` - NDVI from Earth Engine exports\n",
    "- `container.ingest.etf()` - ETf from SSEBop or other models\n",
    "- `container.ingest.gridmet()` - Meteorology\n",
    "- `container.ingest.snodas()` - Snow water equivalent\n",
    "- `container.ingest.properties()` - Soils, LULC, irrigation\n",
    "\n",
    "## Two Data Sources\n",
    "\n",
    "1. **Extracted data**: If you ran notebook 02, data is in `data/remote_sensing/landsat/extracts/`, `data/snow/snodas/extracts/`, etc.\n",
    "2. **Pre-built data**: If you don't have Earth Engine access, use data from `data/prebuilt/`\n",
    "\n",
    "Set `USE_PREBUILT = True` or `False` below to choose your data source."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:46:25.914617474Z",
     "start_time": "2026-01-12T20:46:25.891206692Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.container import open_container\n",
    "from swimrs.swim.config import ProjectConfig"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Choose whether to use pre-built data or extracted data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:46:25.978416891Z",
     "start_time": "2026-01-12T20:46:25.918326407Z"
    }
   },
   "source": [
    "# Set to True to use pre-built data, False to use data from notebook 02\n",
    "USE_PREBUILT = False\n",
    "\n",
    "project_dir = Path.cwd()\n",
    "data_dir = project_dir / 'data'\n",
    "container_path = data_dir / '1_Boulder.swim'\n",
    "\n",
    "# Load config to get feature_id\n",
    "config_path = project_dir / '1_Boulder.toml'\n",
    "config = ProjectConfig()\n",
    "config.read_config(str(config_path))\n",
    "feature_id = config.feature_id_col\n",
    "\n",
    "print(f\"Using feature_id column: {feature_id}\")\n",
    "\n",
    "if USE_PREBUILT:\n",
    "    print(\"Using pre-built data from data/prebuilt/\")\n",
    "    ndvi_root = data_dir / 'prebuilt' / 'remote_sensing' / 'landsat' / 'extracts' / 'ndvi'\n",
    "    etf_root = data_dir / 'prebuilt' / 'remote_sensing' / 'landsat' / 'extracts' / 'ssebop_etf'\n",
    "    met_dir = data_dir / 'prebuilt' / 'met_timeseries' / 'gridmet'\n",
    "    snodas_dir = data_dir / 'prebuilt' / 'snow' / 'snodas' / 'extracts'\n",
    "    properties_dir = data_dir / 'prebuilt' / 'properties'\n",
    "else:\n",
    "    print(\"Using extracted data from notebook 02\")\n",
    "    ndvi_root = data_dir / 'remote_sensing' / 'landsat' / 'extracts' / 'ndvi'\n",
    "    etf_root = data_dir / 'remote_sensing' / 'landsat' / 'extracts' / 'ssebop_etf'\n",
    "    met_dir = data_dir / 'met_timeseries' / 'gridmet'\n",
    "    snodas_dir = data_dir / 'snow' / 'snodas' / 'extracts'\n",
    "    properties_dir = data_dir / 'properties'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using feature_id column: FID_1\n",
      "Using extracted data from notebook 02\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open the Container\n",
    "\n",
    "Open the container we created in notebook 01 in read-write mode."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:46:29.312408413Z",
     "start_time": "2026-01-12T20:46:25.981199248Z"
    }
   },
   "source": [
    "container = open_container(str(container_path), mode='r+')\n",
    "\n",
    "print(f\"Opened container: {container.project_name}\")\n",
    "print(f\"Fields: {container.n_fields}\")\n",
    "print(f\"Date range: {container.start_date} to {container.end_date}\")"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not acquire lock for container: /home/dgketchum/code/swim-rs/examples/1_Boulder/data/1_Boulder.swim\nAnother process may have it open, or a previous process crashed.\nIf you're sure no other process is using this container, delete the lock file:\n  /home/dgketchum/code/swim-rs/examples/1_Boulder/data/1_Boulder.swim.lock",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m container = \u001B[43mopen_container\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcontainer_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr+\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOpened container: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontainer.project_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFields: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontainer.n_fields\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/container.py:765\u001B[39m, in \u001B[36mopen_container\u001B[39m\u001B[34m(uri, mode, **storage_kwargs)\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mopen_container\u001B[39m(\n\u001B[32m    735\u001B[39m     uri: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[32m    736\u001B[39m     mode: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    737\u001B[39m     **storage_kwargs: Any,\n\u001B[32m    738\u001B[39m ) -> SwimContainer:\n\u001B[32m    739\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    740\u001B[39m \u001B[33;03m    Open an existing SWIM container.\u001B[39;00m\n\u001B[32m    741\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    763\u001B[39m \u001B[33;03m        )\u001B[39;00m\n\u001B[32m    764\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m765\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSwimContainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mstorage_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/container.py:210\u001B[39m, in \u001B[36mSwimContainer.open\u001B[39m\u001B[34m(cls, uri, mode, **kwargs)\u001B[39m\n\u001B[32m    175\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    176\u001B[39m \u001B[33;03mOpen an existing container from a URI.\u001B[39;00m\n\u001B[32m    177\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    207\u001B[39m \u001B[33;03m    )\u001B[39;00m\n\u001B[32m    208\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    209\u001B[39m provider = StorageProviderFactory.from_uri(uri, mode=mode, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprovider\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/container.py:148\u001B[39m, in \u001B[36mSwimContainer.__init__\u001B[39m\u001B[34m(self, path_or_provider, mode)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mr+\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._provider.exists():\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mContainer not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m._provider.uri\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/container.py:163\u001B[39m, in \u001B[36mSwimContainer._open_storage\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    155\u001B[39m     warnings.filterwarnings(\n\u001B[32m    156\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    157\u001B[39m         message=\u001B[33m\"\u001B[39m\u001B[33mDuplicate name:\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    158\u001B[39m         category=\u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    159\u001B[39m         module=\u001B[33m\"\u001B[39m\u001B[33mzipfile\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    160\u001B[39m     )\n\u001B[32m    162\u001B[39m \u001B[38;5;66;03m# Open storage provider and get root group\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m163\u001B[39m \u001B[38;5;28mself\u001B[39m._root = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_provider\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[38;5;66;03m# Load metadata\u001B[39;00m\n\u001B[32m    166\u001B[39m \u001B[38;5;28mself\u001B[39m._load_metadata()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/storage/local.py:103\u001B[39m, in \u001B[36mZipStoreProvider.open\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28mself\u001B[39m._lock.acquire()\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Timeout:\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    104\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCould not acquire lock for container: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m._path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    105\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAnother process may have it open, or a previous process crashed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    106\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIf you\u001B[39m\u001B[33m'\u001B[39m\u001B[33mre sure no other process is using this container, delete the lock file:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlock_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    108\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# Suppress ZipStore duplicate name warnings during writes.\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m# Zarr's ZipStore creates duplicate zip entries when writing to the\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[38;5;66;03m# same chunk multiple times, which is expected during ingestion.\u001B[39;00m\n\u001B[32m    113\u001B[39m warnings.filterwarnings(\n\u001B[32m    114\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    115\u001B[39m     message=\u001B[33m\"\u001B[39m\u001B[33mDuplicate name:\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    116\u001B[39m     category=\u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    117\u001B[39m     module=\u001B[33m\"\u001B[39m\u001B[33mzipfile\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    118\u001B[39m )\n",
      "\u001B[31mRuntimeError\u001B[39m: Could not acquire lock for container: /home/dgketchum/code/swim-rs/examples/1_Boulder/data/1_Boulder.swim\nAnother process may have it open, or a previous process crashed.\nIf you're sure no other process is using this container, delete the lock file:\n  /home/dgketchum/code/swim-rs/examples/1_Boulder/data/1_Boulder.swim.lock"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Current Status\n",
    "\n",
    "Before ingestion, let's see what data the container currently holds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(container.query.status())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ingest NDVI Data\n",
    "\n",
    "Ingest NDVI for both irrigated (`irr`) and non-irrigated (`inv_irr`) masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"no_data_found\", \"component\": \"ingestor\", \"timestamp\": \"2026-01-12T20:39:31.652617Z\", \"source\": \"/home/dgketchum/code/swim-rs/examples/1_Boulder/data/remote_sensing/landsat/extracts/ndvi/irr\"}\n",
      "{\"event\": \"no_data_found\", \"component\": \"ingestor\", \"timestamp\": \"2026-01-12T20:39:31.656829Z\", \"source\": \"/home/dgketchum/code/swim-rs/examples/1_Boulder/data/remote_sensing/landsat/extracts/ndvi/inv_irr\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting NDVI (irr)...\n",
      "Ingesting NDVI (inv_irr)...\n"
     ]
    }
   ],
   "source": [
    "for mask in ['irr', 'inv_irr']:\n",
    "    ndvi_dir = ndvi_root / mask\n",
    "    if ndvi_dir.exists():\n",
    "        print(f\"Ingesting NDVI ({mask})...\")\n",
    "        container.ingest.ndvi(\n",
    "            source_dir=str(ndvi_dir),\n",
    "            uid_column=feature_id,\n",
    "            instrument='landsat',\n",
    "            mask=mask,\n",
    "            overwrite=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: NDVI directory not found: {ndvi_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ingest ETf Data\n",
    "\n",
    "Ingest SSEBop ETf for both masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting ETf (irr)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"operation_failed\", \"component\": \"ingestor\", \"timestamp\": \"2026-01-12T20:39:35.979711Z\", \"operation\": \"ingest_etf\", \"target\": \"remote_sensing/etf/landsat/ssebop/irr\", \"source\": \"/home/dgketchum/code/swim-rs/examples/1_Boulder/data/remote_sensing/landsat/extracts/ssebop_etf/irr\", \"model\": \"ssebop\", \"instrument\": \"landsat\", \"mask\": \"irr\", \"duration_seconds\": 0.56, \"error\": \"ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\", \"error_type\": \"TypeError\"}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m etf_dir.exists():\n\u001B[32m      4\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIngesting ETf (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmask\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     \u001B[43mcontainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mingest\u001B[49m\u001B[43m.\u001B[49m\u001B[43metf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m        \u001B[49m\u001B[43msource_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43metf_dir\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m        \u001B[49m\u001B[43muid_column\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43minstrument\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlandsat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mssebop\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m        \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     14\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mWarning: ETf directory not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00metf_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/components/ingestor.py:203\u001B[39m, in \u001B[36mIngestor.etf\u001B[39m\u001B[34m(self, source_dir, uid_column, model, mask, instrument, fields, overwrite)\u001B[39m\n\u001B[32m    200\u001B[39m all_data = all_data.where((all_data >= \u001B[32m0\u001B[39m) & (all_data <= \u001B[32m2.0\u001B[39m))\n\u001B[32m    202\u001B[39m \u001B[38;5;66;03m# Align to container grid and write\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m records = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_write_timeseries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    205\u001B[39m ctx[\u001B[33m\"\u001B[39m\u001B[33mrecords_processed\u001B[39m\u001B[33m\"\u001B[39m] = records\n\u001B[32m    206\u001B[39m ctx[\u001B[33m\"\u001B[39m\u001B[33mfields_processed\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mlen\u001B[39m(all_data.columns)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/container/components/ingestor.py:927\u001B[39m, in \u001B[36mIngestor._write_timeseries\u001B[39m\u001B[34m(self, path, data, fields)\u001B[39m\n\u001B[32m    924\u001B[39m     field_idx = \u001B[38;5;28mself\u001B[39m._state._uid_to_index[field_uid]\n\u001B[32m    925\u001B[39m     arr[:, field_idx] = aligned[field_uid].values\n\u001B[32m--> \u001B[39m\u001B[32m927\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(np.count_nonzero(~\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43misnan\u001B[49m\u001B[43m(\u001B[49m\u001B[43maligned\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m))\n",
      "\u001B[31mTypeError\u001B[39m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "for mask in ['irr', 'inv_irr']:\n",
    "    etf_dir = etf_root / mask\n",
    "    if etf_dir.exists():\n",
    "        print(f\"Ingesting ETf ({mask})...\")\n",
    "        container.ingest.etf(\n",
    "            source_dir=str(etf_dir),\n",
    "            uid_column=feature_id,\n",
    "            instrument='landsat',\n",
    "            model='ssebop',\n",
    "            mask=mask,\n",
    "            overwrite=True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: ETf directory not found: {etf_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ingest Meteorology Data\n",
    "\n",
    "Ingest GridMET meteorology including bias-corrected reference ET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if met_dir.exists():\n",
    "    print(\"Ingesting GridMET meteorology...\")\n",
    "    container.ingest.gridmet(\n",
    "        source_dir=str(met_dir),\n",
    "        variables=['eto', 'etr', 'prcp', 'tmin', 'tmax', 'srad', 'u2', 'ea'],\n",
    "        include_corrected=True,\n",
    "        overwrite=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Warning: GridMET directory not found: {met_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ingest Snow Data (SNODAS)\n",
    "\n",
    "Ingest SNODAS snow water equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snodas_dir.exists():\n",
    "    print(\"Ingesting SNODAS SWE...\")\n",
    "    container.ingest.snodas(\n",
    "        source_dir=str(snodas_dir),\n",
    "        uid_column=feature_id,\n",
    "        overwrite=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Warning: SNODAS directory not found: {snodas_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ingest Properties\n",
    "\n",
    "Ingest static properties: soils, land cover, and irrigation fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soils_csv = properties_dir / '1_Boulder_ssurgo.csv'\n",
    "lulc_csv = properties_dir / '1_Boulder_landcover.csv'\n",
    "irr_csv = properties_dir / '1_Boulder_irr.csv'\n",
    "\n",
    "# Check which files exist\n",
    "props_exist = {\n",
    "    'soils': soils_csv.exists(),\n",
    "    'lulc': lulc_csv.exists(),\n",
    "    'irrigation': irr_csv.exists()\n",
    "}\n",
    "\n",
    "if any(props_exist.values()):\n",
    "    print(\"Ingesting properties...\")\n",
    "    container.ingest.properties(\n",
    "        soils_csv=str(soils_csv) if props_exist['soils'] else None,\n",
    "        lulc_csv=str(lulc_csv) if props_exist['lulc'] else None,\n",
    "        irr_csv=str(irr_csv) if props_exist['irrigation'] else None,\n",
    "        uid_column=feature_id,\n",
    "        overwrite=True\n",
    "    )\n",
    "    print(f\"  Soils: {'OK' if props_exist['soils'] else 'not found'}\")\n",
    "    print(f\"  Land cover: {'OK' if props_exist['lulc'] else 'not found'}\")\n",
    "    print(f\"  Irrigation: {'OK' if props_exist['irrigation'] else 'not found'}\")\n",
    "else:\n",
    "    print(f\"Warning: No property files found in {properties_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Container Status After Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.query.status(detailed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Explore Ingested Data with xarray\n",
    "\n",
    "One of the powerful features of the SwimContainer is seamless xarray integration. Let's visualize some of the ingested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NDVI as an xarray DataArray\n",
    "try:\n",
    "    ndvi = container.to_xarray('remote_sensing/ndvi/landsat/irr')\n",
    "    print(f\"NDVI shape: {ndvi.shape}\")\n",
    "    print(f\"Dimensions: {ndvi.dims}\")\n",
    "    print(f\"Sites: {list(ndvi.site.values[:5])}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load NDVI: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NDVI time series for a single field\n",
    "try:\n",
    "    sample_site = container.field_uids[0]\n",
    "    ndvi_site = ndvi.sel(site=sample_site)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ndvi_site.plot(ax=ax, marker='.', linestyle='none', markersize=2)\n",
    "    ax.set_title(f'NDVI Time Series - Field {sample_site}')\n",
    "    ax.set_ylabel('NDVI')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot NDVI: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ETf vs ETo for a single year\n",
    "try:\n",
    "    etf = container.to_xarray('remote_sensing/etf/landsat/ssebop/irr')\n",
    "    eto = container.to_xarray('meteorology/gridmet/eto')\n",
    "    \n",
    "    # Select one site and one year\n",
    "    sample_site = container.field_uids[0]\n",
    "    etf_2020 = etf.sel(site=sample_site, time='2020')\n",
    "    eto_2020 = eto.sel(site=sample_site, time='2020')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 6), sharex=True)\n",
    "    \n",
    "    axes[0].plot(eto_2020.time, eto_2020, 'b-', alpha=0.7, label='ETo')\n",
    "    axes[0].set_ylabel('ETo (mm/day)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(etf_2020.time, etf_2020, 'go', markersize=3, label='ETf')\n",
    "    axes[1].set_ylabel('ETf (fraction)')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    fig.suptitle(f'Reference ET and ETf - Field {sample_site} (2020)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot ETf/ETo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. View Provenance\n",
    "\n",
    "The container automatically tracks all operations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Provenance Log:\")\n",
    "for event in container.provenance.events[-10:]:\n",
    "    print(f\"  {event.timestamp[:19]} - {event.operation}: {event.target or 'container'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.save()\n",
    "container.close()\n",
    "\n",
    "print(f\"Container saved to: {container_path}\")\n",
    "print(\"\\nNext: Run notebook 04 to compute dynamics and export model inputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
