{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Run\n",
    "\n",
    "Now we only need to run the (uncalibrated) model to see what we're working with. This consists of a couple simple steps:\n",
    "\n",
    "1. Configure the model using the project TOML file.\n",
    "2. Open the SwimContainer and build a SwimInput object.\n",
    "3. Run the model using the fast JIT-compiled loop.\n",
    "4. Inspect the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import toml\n",
    "import pandas as pd\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# 1. The ProjectConfig object.\n",
    "\n",
    "We've avoided using this so far, because configuration files can encourage us to passively press 'GO' and not think about the code we need to run. Now that we've thought about it, we use the configuration file so we don't have to think about it again. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(root, 'examples', '1_Boulder', '1_Boulder.toml')\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = toml.load(f)\n",
    "\n",
    "formatted_config = json.dumps(config, indent=4)\n",
    "print(formatted_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "This is a nice dict structure of key: value pairs that we'd easily be able to access during the internal model set up and the model run. However, to make things even easier, we provide this code to a config file parser, that takes this data and creates a configuration class, which makes all this data clean and easy for the model to access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swimrs.swim.config import ProjectConfig\n",
    "\n",
    "# Our project workspace will replace the \"{project_root}\" in the paths in the config file,\n",
    "# several directories will be placed there. Let's use the top level directory of this tutorial\n",
    "project_ws = os.path.join(root, 'examples', '1_Boulder')\n",
    "print(f'Setting project root to {project_ws}')\n",
    "\n",
    "config = ProjectConfig()\n",
    "config.read_config(config_file, project_root_override=project_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "Now, we have a succinct Python object with all the data we need for basic model setup, for example, the project workspace and the location of our container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.project_ws, config.container_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "# 2. The SwimInput object.\n",
    "\n",
    "The modern SWIM-RS workflow uses the `process` package for simulation. We:\n",
    "\n",
    "1. Open the **SwimContainer** (created in notebooks 01-04)\n",
    "2. Build a **SwimInput** object from the container using `build_swim_input()`\n",
    "3. Run the simulation using `run_daily_loop_fast()` (JIT-compiled for speed)\n",
    "\n",
    "The SwimInput object packages all data needed for simulation into an efficient HDF5 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swimrs.container import SwimContainer\n",
    "from swimrs.process.input import build_swim_input\n",
    "from swimrs.process.loop_fast import run_daily_loop_fast\n",
    "\n",
    "# Open the container we created in the previous notebooks\n",
    "container_path = os.path.join(project_ws, 'data', '1_Boulder.swim')\n",
    "container = SwimContainer.open(container_path, mode='r')\n",
    "\n",
    "print(f\"Container: {container.project_name}\")\n",
    "print(f\"Fields: {container.n_fields}\")\n",
    "print(f\"Date range: {container.start_date} to {container.end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "Now we build the SwimInput from the container. This extracts all the data needed for simulation and writes it to a temporary HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary HDF5 file for the SwimInput\n",
    "temp_h5 = tempfile.NamedTemporaryFile(suffix='.h5', delete=False)\n",
    "temp_h5_path = temp_h5.name\n",
    "temp_h5.close()\n",
    "\n",
    "# Build SwimInput from container\n",
    "swim_input = build_swim_input(\n",
    "    container,\n",
    "    output_h5=temp_h5_path,\n",
    "    runoff_process=getattr(config, 'runoff_process', 'cn'),\n",
    "    etf_model=getattr(config, 'etf_target_model', 'ssebop'),\n",
    "    met_source='gridmet',\n",
    ")\n",
    "\n",
    "print(f\"SwimInput created with {swim_input.n_fields} fields and {swim_input.n_days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "The SwimInput object provides access to field properties and time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field IDs are stored in swim_input.fids\n",
    "print(\"Field IDs (first 10):\")\n",
    "print(swim_input.fids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Field properties are in swim_input.properties\nfeature_161 = '043_000161'\nidx = swim_input.fids.index(feature_161)\n\nprint(f\"Properties for field {feature_161} (index {idx}):\")\nprint(f\"  AWC: {swim_input.properties.awc[idx]:.2f} mm\")\nprint(f\"  Ksat: {swim_input.properties.ksat[idx]:.2f} mm/day\")\nprint(f\"  REW: {swim_input.properties.rew[idx]:.1f} mm\")\nprint(f\"  TEW: {swim_input.properties.tew[idx]:.1f} mm\")\nprint(f\"  Zr_max: {swim_input.properties.zr_max[idx]:.2f} m\")\nprint(f\"  Irrigated: {swim_input.properties.irr_status[idx]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data is accessed via get_time_series()\n",
    "# Returns array of shape (n_days, n_fields)\n",
    "tmin = swim_input.get_time_series('tmin')\n",
    "print(f\"Tmin shape: {tmin.shape}\")\n",
    "print(f\"Tmin for field {feature_161} on day 0: {tmin[0, idx]:.2f} C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "# 3. The daily model run.\n",
    "\n",
    "Now we can run the model through time using `run_daily_loop_fast()`. This function uses Numba JIT compilation for ~300x speedup compared to the legacy Python loop.\n",
    "\n",
    "The output is a `DailyOutput` object containing numpy arrays of shape (n_days, n_fields) for each output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "start_time = time.time()\n",
    "output, final_state = run_daily_loop_fast(swim_input)\n",
    "end_time = time.time()\n",
    "print(f'\\nExecution time: {end_time - start_time:.2f} seconds\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "The `run_daily_loop_fast()` function is much faster than the legacy `field_day_loop()` because:\n",
    "\n",
    "1. It uses Numba JIT compilation for the physics kernels\n",
    "2. It operates on numpy arrays directly instead of Python dicts\n",
    "3. It avoids DataFrame overhead during simulation\n",
    "\n",
    "The output is a `DailyOutput` dataclass with arrays for each variable. Let's examine the output structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output shape: ({output.n_days}, {output.n_fields})\")\n",
    "print(f\"\\nAvailable output variables:\")\n",
    "for attr in ['eta', 'etf', 'kcb', 'ke', 'ks', 'kr', 'runoff', 'rain', 'melt', 'swe', 'depl_root', 'dperc', 'irr_sim', 'gw_sim']:\n",
    "    arr = getattr(output, attr)\n",
    "    print(f\"  {attr}: shape={arr.shape}, mean={arr.mean():.3f}, max={arr.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "We can see here the **priceless fruit of our labor**: daily model estimates of the state of our field and its soil, with accounting for all inputs and outputs from precipitation, irrigation, runoff and deep percolation (recharge). Also, we have a snow (SWE) accounting that makes this even more realistic. This model, while uncalibrated, uses model defaults that are the result of calibration in other locations. Further, the model is tied to the true field conditions through time via NDVI, making even an uncalibrated model realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "# 4. Convert output to DataFrame for analysis\n",
    "\n",
    "For visualization and analysis, we convert the DailyOutput arrays to pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "def output_to_dataframe(swim_input, output, field_idx):\n    \"\"\"Convert DailyOutput arrays to a DataFrame for a single field.\"\"\"\n    dates = pd.date_range(swim_input.start_date, periods=swim_input.n_days, freq='D')\n    \n    # Get input time series\n    etr = swim_input.get_time_series('etr')\n    prcp = swim_input.get_time_series('prcp')\n    tmin = swim_input.get_time_series('tmin')\n    tmax = swim_input.get_time_series('tmax')\n    ndvi = swim_input.get_time_series('ndvi')\n    \n    i = field_idx\n    awc = swim_input.properties.awc[i]\n    \n    df = pd.DataFrame({\n        # Model outputs\n        'et_act': output.eta[:, i],\n        'etref': etr[:, i],\n        'kc_act': output.etf[:, i],\n        'kc_bas': output.kcb[:, i],\n        'ks': output.ks[:, i],\n        'ke': output.ke[:, i],\n        'melt': output.melt[:, i],\n        'rain': output.rain[:, i],\n        'depl_root': output.depl_root[:, i],\n        'dperc': output.dperc[:, i],\n        'runoff': output.runoff[:, i],\n        'swe': output.swe[:, i],\n        'irrigation': output.irr_sim[:, i],\n        'gw_sim': output.gw_sim[:, i],\n        # Derived\n        'soil_water': awc - output.depl_root[:, i],\n        'aw': np.full(swim_input.n_days, awc),\n        # Input time series\n        'ppt': prcp[:, i],\n        'tmin': tmin[:, i],\n        'tmax': tmax[:, i],\n        'tavg': (tmin[:, i] + tmax[:, i]) / 2.0,\n        'ndvi': ndvi[:, i],\n    }, index=dates)\n    \n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = os.path.join(root, 'examples', '1_Boulder', 'data', 'model_output')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert output for field 161\n",
    "idx_161 = swim_input.fids.index(feature_161)\n",
    "df_161 = output_to_dataframe(swim_input, output, idx_161)\n",
    "\n",
    "csv_161 = os.path.join(output_dir, f'combined_output_{feature_161}.csv')\n",
    "df_161.to_csv(csv_161)\n",
    "\n",
    "print(f\"Output shape: {df_161.shape}\")\n",
    "df_161.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_161.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "Okay, let's plot some results. First, we define a function that will flexibly plot time series of our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(df, parameters, start='2007-05-01', end='2007-10-31', png_file=None):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.read_csv(df, index_col=0, parse_dates=True)\n",
    "\n",
    "    df = df.loc[start:end]\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    bar_vars = ['rain', 'melt', 'snow_fall', 'dperc', 'irrigation']\n",
    "    bar_colors = ['lightpink', 'lightblue', 'blue', 'lightsalmon', 'red']\n",
    "\n",
    "    for i, param in enumerate(parameters):\n",
    "        if param in bar_vars:\n",
    "            vals = df[param]\n",
    "            if param == 'dperc':\n",
    "                vals = vals * -1\n",
    "                print(f\"max dperc: {vals.max():.1f}\")\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=df.index, y=vals, name=param,\n",
    "                       marker=dict(color=bar_colors[bar_vars.index(param)])),\n",
    "                secondary_y=False,\n",
    "            )\n",
    "        else:\n",
    "            if param in ['et_act', 'etref'] and 'et_act' in parameters and 'etref' in parameters:\n",
    "                secondary_y = False\n",
    "            else:\n",
    "                secondary_y = True if i > 0 else False\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df.index, y=df[param], name=param),\n",
    "                secondary_y=secondary_y,\n",
    "            )\n",
    "\n",
    "    for param in parameters:\n",
    "        if param in ['etf_irr', 'etf_inv_irr', 'ndvi_irr', 'ndvi_inv_irr']:\n",
    "            ct_param = param + '_ct'\n",
    "            if ct_param in df.columns:\n",
    "                scatter_df = df[df[ct_param] == 1]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=scatter_df.index, y=scatter_df[param],\n",
    "                               mode='markers', marker_symbol='x',\n",
    "                               marker_size=5, name=f'{param} Retrieval'),\n",
    "                    secondary_y=True,\n",
    "                )\n",
    "\n",
    "    kwargs = dict(title_text=\"SWIM Model Time Series\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"mm\",\n",
    "        height=800,\n",
    "        template='plotly_dark',\n",
    "        xaxis=dict(showgrid=False),\n",
    "        yaxis=dict(showgrid=False),\n",
    "        yaxis2=dict(showgrid=False))\n",
    "    \n",
    "    if 'dperc' in parameters:\n",
    "        kwargs.update(dict(yaxis=dict(showgrid=False, range=[-20, None]), yaxis2=dict(showgrid=False, range=[-20, None])))\n",
    "        \n",
    "    fig.update_layout(**kwargs)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    if png_file:\n",
    "        fig.write_image(png_file)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_161, index_col=0, parse_dates=True)\n",
    "print(df.columns.tolist()[:15])  # Show first 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries(csv_161, ['soil_water', 'irrigation', 'rain', 'melt'], start='2017-01-01', end='2017-10-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "We can see the seasonal control on `soil_water`, from `melt` in the winter and spring, `rain` in May and June (the rainiest months in the area), and finally, as the soil water depletes during the hottest part of the growing season, `irrigation` kicks in. Use the range slider to zoom in on different time periods, and choose 'Pan' in the upper right to slide through time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "We can also see the deep percolation (recharge) caused by rain and snowmelt in late May, 2021. We made it 'negative' just so it would stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries(df_161, ['rain', 'melt', 'dperc'], start='2021-05-01', end='2021-07-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": "Also check out how NDVI relates to the crop coefficient (Kc_act):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "plot_timeseries(df_161, ['kc_act', 'ndvi'], start='2017-01-01', end='2021-01-01')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "Finally, perhaps the most valuable information is the actual ET signal, which, when superimposed over the reference ET signal gives an indication of soil water and plant stress, or the absence of vegetation to carry on ET when the surface soil layer has dried. Let's look at the example from before, with irrigated field 128 and unirrigated field 130. Zoom in to view the irrigation application simulated by the model in the final week of July in field 128, which resulted in a period of ET at near the reference rate for the following two weeks. Meanwhile, the neighboring field, after the last good period of rain in early July, sees ET drop and stay low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_128 = '043_000128'\n",
    "csv_128 = os.path.join(output_dir, f'combined_output_{feature_128}.csv')\n",
    "\n",
    "idx_128 = swim_input.fids.index(feature_128)\n",
    "df_128 = output_to_dataframe(swim_input, output, idx_128)\n",
    "df_128.to_csv(csv_128)\n",
    "\n",
    "irr_2004 = df_128.loc['2004-01-01': '2004-12-31']\n",
    "print(f'total irrigation: {irr_2004.irrigation.sum():.1f} mm')\n",
    "print(f'total et: {irr_2004.et_act.sum():.1f} mm')\n",
    "print(f'total precip: {irr_2004.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_timeseries(df_128, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], start='2004-01-01', end='2004-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_130 = '043_000130'\n",
    "csv_130 = os.path.join(output_dir, f'combined_output_{feature_130}.csv')\n",
    "\n",
    "idx_130 = swim_input.fids.index(feature_130)\n",
    "df_130 = output_to_dataframe(swim_input, output, idx_130)\n",
    "df_130.to_csv(csv_130)\n",
    "\n",
    "unirr_2004 = df_130.loc['2004-01-01': '2004-12-31']\n",
    "print(f'total irrigation: {unirr_2004.irrigation.sum():.1f} mm')\n",
    "print(f'total et: {unirr_2004.et_act.sum():.1f} mm')\n",
    "print(f'total precip: {unirr_2004.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_timeseries(df_130, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], start='2004-01-01', end='2004-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "\n",
    "Close the SwimInput and container, and clean up the temporary HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_input.close()\n",
    "container.close()\n",
    "\n",
    "# Clean up temp file\n",
    "if os.path.exists(temp_h5_path):\n",
    "    os.remove(temp_h5_path)\n",
    "    \n",
    "print(\"Resources cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. Created a SwimContainer from your shapefile\n",
    "2. Extracted (or used pre-built) data from Earth Engine and GridMET\n",
    "3. Ingested data into the container\n",
    "4. Computed dynamics and exported model inputs\n",
    "5. Run the SWIM-RS model using the modern `process` package and visualized the results\n",
    "\n",
    "The model outputs include:\n",
    "- **et_act (eta)**: Actual evapotranspiration (mm/day)\n",
    "- **soil_water**: Soil water storage (mm)\n",
    "- **irrigation (irr_sim)**: Simulated irrigation (mm/day)\n",
    "- **dperc**: Deep percolation / recharge (mm/day)\n",
    "- **swe**: Snow water equivalent (mm)\n",
    "\n",
    "Next steps might include:\n",
    "- Calibrating the model using PEST++ and observed ETf/SWE\n",
    "- Running the model for different time periods\n",
    "- Comparing irrigated vs non-irrigated fields\n",
    "- Aggregating results for water budget analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}