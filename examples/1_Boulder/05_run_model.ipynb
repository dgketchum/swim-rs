{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run\n",
    "\n",
    "Now we only need to run the (uncalibrated) model to see what we're working with. This consists of a couple simple steps:\n",
    "\n",
    "1. Configure the model using the project TOML file.\n",
    "2. Initialize the plot data.\n",
    "3. Run the model.\n",
    "4. Inspect the outputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:03.162989624Z",
     "start_time": "2026-01-13T21:34:02.754119790Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import toml\n",
    "import pandas as pd\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The ProjectConfig object.\n",
    "\n",
    "We've avoided using this so far, because configuration files can encourage us to passively press 'GO' and not think about the code we need to run. Now that we've thought about it, we use the configuration file so we don't have to think about it again. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:05.550110677Z",
     "start_time": "2026-01-13T21:34:03.164945019Z"
    }
   },
   "source": [
    "config_file = os.path.join(root, 'examples', '1_Boulder', '1_Boulder.toml')\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = toml.load(f)\n",
    "\n",
    "formatted_config = json.dumps(config, indent=4)\n",
    "print(formatted_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"project\": \"1_Boulder\",\n",
      "    \"root\": \".\",\n",
      "    \"paths\": {\n",
      "        \"project_workspace\": \"{root}\",\n",
      "        \"data\": \"{project_workspace}/data\",\n",
      "        \"remote_sensing\": \"{data}/remote_sensing\",\n",
      "        \"landsat\": \"{remote_sensing}/landsat\",\n",
      "        \"landsat_ee_data\": \"{landsat}/extracts\",\n",
      "        \"landsat_tables\": \"{landsat}/tables\",\n",
      "        \"sentinel\": \"{remote_sensing}/sentinel\",\n",
      "        \"sentinel_ee_data\": \"{sentinel}/extracts\",\n",
      "        \"met\": \"{data}/met_timeseries/gridmet\",\n",
      "        \"gis\": \"{data}/gis\",\n",
      "        \"fields_shapefile\": \"{gis}/mt_sid_boulder.shp\",\n",
      "        \"gridmet_centroids\": \"{gis}/gridmet_centroids.shp\",\n",
      "        \"gridmet_mapping\": \"{gis}/mt_sid_boulder_gfid.shp\",\n",
      "        \"correction_tifs\": \"{data}/bias_correction_tif\",\n",
      "        \"gridmet_factors\": \"{gis}/mt_sid_boulder_gfid.json\",\n",
      "        \"properties\": \"{data}/properties\",\n",
      "        \"irr\": \"{properties}/{project}_irr.csv\",\n",
      "        \"ssurgo\": \"{properties}/{project}_ssurgo.csv\",\n",
      "        \"lulc\": \"{properties}/{project}_landcover.csv\",\n",
      "        \"properties_json\": \"{properties}/{project}_properties.json\",\n",
      "        \"snodas_in\": \"{data}/snow/snodas/extracts\",\n",
      "        \"remote_sensing_tables\": \"{data}/rs_tables\",\n",
      "        \"joined_timeseries\": \"{data}/plot_timeseries\",\n",
      "        \"dynamics_data\": \"{data}/{project}_dynamics.json\",\n",
      "        \"prepped_input\": \"{data}/prepped_input.json\",\n",
      "        \"container\": \"{data}/{project}.swim\"\n",
      "    },\n",
      "    \"earth_engine\": {\n",
      "        \"bucket\": \"wudr\",\n",
      "        \"drive_folder\": \"swim\"\n",
      "    },\n",
      "    \"ids\": {\n",
      "        \"feature_id\": \"FID_1\",\n",
      "        \"gridmet_join_id\": \"GFID\",\n",
      "        \"gridmet_id\": \"GFID\",\n",
      "        \"state_col\": \"STATE\"\n",
      "    },\n",
      "    \"misc\": {\n",
      "        \"irrigation_threshold\": 0.3,\n",
      "        \"elev_units\": \"m\",\n",
      "        \"refet_type\": \"eto\",\n",
      "        \"swb_mode\": \"cn\"\n",
      "    },\n",
      "    \"date_range\": {\n",
      "        \"start_date\": \"2004-01-01\",\n",
      "        \"end_date\": \"2022-12-31\"\n",
      "    },\n",
      "    \"crop_coefficient\": {\n",
      "        \"kc_proxy\": \"etf\",\n",
      "        \"cover_proxy\": \"ndvi\"\n",
      "    },\n",
      "    \"calibration\": {\n",
      "        \"etf_target_model\": \"ssebop\"\n",
      "    },\n",
      "    \"forecast\": {}\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice dict structure of key: value pairs that we'd easily be able to access during the internal model set up and the model run. However, to make things even easier, we provide this code to a config file parser, that takes this data and creates a configuration class, which makes all this data clean and easy for the model to access:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:05.699490917Z",
     "start_time": "2026-01-13T21:34:05.623618151Z"
    }
   },
   "source": [
    "from swimrs.swim.config import ProjectConfig\n",
    "\n",
    "# Our project workspace will replace the \"{project_root}\" in the paths in the config file,\n",
    "# several directories will be placed there. Let's use the top level directory of this tutorial\n",
    "project_ws = os.path.join(root, 'examples', '1_Boulder')\n",
    "print(f'Setting project root to {project_ws}')\n",
    "\n",
    "config = ProjectConfig()\n",
    "config.read_config(config_file, project_root_override=project_ws)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting project root to /home/dgketchum/code/swim-rs/examples/1_Boulder\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a succinct Python object with all the data we need for basic model setup, for example, the project workspace and the location of our model input file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:05.772810049Z",
     "start_time": "2026-01-13T21:34:05.701076311Z"
    }
   },
   "source": [
    "config.project_ws, config.input_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dgketchum/code/swim-rs/examples/1_Boulder',\n",
       " '/home/dgketchum/code/swim-rs/examples/1_Boulder/data/prepped_input.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The SamplePlots object.\n",
    "\n",
    "Similarly, we will instantiate another important Python object that contains all the input data we built, and to which we will assign our output data. We call it `fields` here, but it could be any sample plots we've prepared:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:15.288715793Z",
     "start_time": "2026-01-13T21:34:05.774840237Z"
    }
   },
   "source": [
    "from swimrs.swim.sampleplots import SamplePlots\n",
    "\n",
    "fields = SamplePlots()\n",
    "fields.initialize_plot_data(config)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SamplePlots` object has all the data from our input file, it is rich in information. Its only attribute is `input` which is a dict:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:19.093650133Z",
     "start_time": "2026-01-13T21:34:15.319817019Z"
    }
   },
   "source": [
    "fields.input.keys()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['props', 'irr_data', 'gwsub_data', 'ke_max', 'kc_max', 'order', 'time_series', 'missing'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`props` has an entry for each sample plot (field) with irrigation fraction data, soils info, and plot area. Let's pick on field '043_000161'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:23.391282295Z",
     "start_time": "2026-01-13T21:34:19.118966522Z"
    }
   },
   "source": [
    "feature_161 = '043_000161'\n",
    "fields.input['props'][feature_161].keys()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['awc', 'clay', 'sand', 'ksat', 'modis_lc', 'irr', 'area_m2', 'lulc_code', 'root_depth', 'zr_mult'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`irr_data` has an entry for each field, which has a nested dict with an entry for each year, with more detailed irrigation information."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:23.835884933Z",
     "start_time": "2026-01-13T21:34:23.425376570Z"
    }
   },
   "source": [
    "fields.input['irr_data'][feature_161]['2022'].keys()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['irr_doys', 'irrigated', 'f_irr'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry the time series of meteorology and remote sensing-based data, we have a dict of date/time series pairs, `time_series`. There is a list of values under each date for each parameter, the order of which is held in `order`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:29.493484711Z",
     "start_time": "2026-01-13T21:34:23.837730256Z"
    }
   },
   "source": [
    "fields.input['order'][:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['043_000153',\n",
       " '043_000154',\n",
       " '043_000155',\n",
       " '043_000156',\n",
       " '043_000157',\n",
       " '043_000158',\n",
       " '043_000159',\n",
       " '043_000160',\n",
       " '043_000161',\n",
       " '043_000162']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:32.281493798Z",
     "start_time": "2026-01-13T21:34:29.530969152Z"
    }
   },
   "source": [
    "# each day has a timeseries, one entry for each field, for each variable\n",
    "# just display the first few\n",
    "fields.input['time_series']['2022-07-31']['tmin'][:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137,\n",
       " 10.949999809265137]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really need to worry about it, but if we needed a specific field's minimum temperature on January 3, 2019, we could find it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:32.386837658Z",
     "start_time": "2026-01-13T21:34:32.308304340Z"
    }
   },
   "source": [
    "idx = fields.input['order'].index(feature_161)\n",
    "fields.input['time_series']['2019-01-03']['tmin'][idx]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.650000095367432"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The daily model run.\n",
    "\n",
    "Now we can run the model through time, using the data we've prepared. We use the function `field_day_loop` found in `model.obs_field_cycle`. The abbreviation `etd` stands for 'ET-Demands', the excellent project that SWIM started from as a fork. It is now almost unrecognizable from ET-Demands, but does maintain a similar structure in its approach to stepping daily through time and executing a soil water balance in order.\n",
    "\n",
    "Check it out: https://github.com/WSWUP/et-demands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we run the model and assign the output to our `SamplePlots` object (`fields`). Note the model output is a dict of Pandas DataFrame objects, one per field, each of which is a time series that runs daily over the date range specified in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:34:38.407143237Z",
     "start_time": "2026-01-13T21:34:32.388650314Z"
    }
   },
   "source": [
    "from swimrs.model.obs_field_cycle import field_day_loop\n",
    "\n",
    "# Let's time this run\n",
    "start_time = time.time()\n",
    "fields.output = field_day_loop(config, fields, debug_flag=True)\n",
    "end_time = time.time()\n",
    "print('\\nExecution time: {:.2f} seconds\\n'.format(end_time - start_time))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING PARAMETER DEFAULTS\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ndvi_irr'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Let's time this run\u001B[39;00m\n\u001B[32m      4\u001B[39m start_time = time.time()\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m fields.output = \u001B[43mfield_day_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug_flag\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m end_time = time.time()\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mExecution time: \u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[33m seconds\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m.format(end_time - start_time))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/model/obs_field_cycle.py:150\u001B[39m, in \u001B[36mfield_day_loop\u001B[39m\u001B[34m(config, plots, debug_flag, params, state_in, capture_state, single_fid_idx)\u001B[39m\n\u001B[32m    147\u001B[39m     day_data.update_annual_irrigation(plots)\n\u001B[32m    148\u001B[39m     day_data.update_annual_groundwater_subsidy(plots)\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m \u001B[43mday_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate_daily_irrigation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplots\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    152\u001B[39m day_data.update_daily_inputs(vals, size)\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m capture_state \u001B[38;5;129;01mand\u001B[39;00m single_fid_idx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/swim-rs/src/swimrs/model/day_data.py:108\u001B[39m, in \u001B[36mDayData.update_daily_irrigation\u001B[39m\u001B[34m(self, plots, vals, config)\u001B[39m\n\u001B[32m    106\u001B[39m irrigated = \u001B[38;5;28mself\u001B[39m.irr_status[\u001B[32m0\u001B[39m, i]\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m irrigated:\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28mself\u001B[39m.ndvi[\u001B[32m0\u001B[39m, i] = \u001B[43mvals\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mndvi_irr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[i]\n\u001B[32m    109\u001B[39m     \u001B[38;5;28mself\u001B[39m.refet[\u001B[32m0\u001B[39m, i] = vals[\u001B[33m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m_corr\u001B[39m\u001B[33m'\u001B[39m.format(config.refet_type)][i]\n\u001B[32m    110\u001B[39m     \u001B[38;5;28mself\u001B[39m.irr_day[\u001B[32m0\u001B[39m, i] = \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m.doy \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.irr_doys[i])\n",
      "\u001B[31mKeyError\u001B[39m: 'ndvi_irr'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is slow!\n",
    "\n",
    "Note the use of `debug_flag=True`. This causes the model to return the dict of sample plot dataframes. This accounts for a 3x or so slowdown. The model does not need to deal in DataFrames; for that reason, we build an input file that is easily read into a Python `dict` structure with lists of data that are themselves easy to read into `numpy.ndArray` objects, which are fast in arithmetic operations.\n",
    "\n",
    "If set to `debug_flag=False`, the `field_day_loop` function assumes that calibration is underway, and returns `numpy.ndArray` objects for modeled SWE and ETf only, which is much more efficient for calibration, which needs to run the model multiple times.\n",
    "\n",
    "Set the above `debug` flag to `False` and see for yourself.\n",
    "\n",
    "We will look at ways to make the model run faster later.\n",
    "\n",
    "Let's take a closer look at the outputs from the field object '043_000161' again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "out_df = fields.output[feature_161].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "out_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here the **priceless fruit of our labor**: daily model estimates of the state of our field and its soil, with accounting for all inputs and outputs from precipitation, irrigation, runoff and deep percolation (recharge). Also, we have a snow (SWE) accounting that makes this even more realistic. This model, while uncalibrated, uses model defaults that are the result of calibration in other locations. Further, the model is tied to the true field conditions through time via NDVI, making even an uncalibrated model realistic."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "out_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the time series data together by concatenating the inputs to the `df` dataframe, which we can do with the SamplePlots `input_to_dataframe` method, and save it so we don't have to run the model again:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_dir = os.path.join(root, 'examples', '1_Boulder', 'data', 'model_output')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_161 = os.path.join(output_dir, f'combined_output_{feature_161}.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "in_df = fields.input_to_dataframe(feature_161)\n",
    "df = pd.concat([out_df, in_df], axis=1, ignore_index=False)\n",
    "df.to_csv(csv_161)\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's plot some results. First, we define a function that will flexibly plot time series of our variables:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "pyo.init_notebook_mode()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_timeseries(df, parameters, start='2007-05-01', end='2007-10-31', png_file=None):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.read_csv(df, index_col=0, parse_dates=True)\n",
    "\n",
    "    df = df.loc[start:end]\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    bar_vars = ['rain', 'melt', 'snow_fall', 'dperc', 'irrigation']\n",
    "    bar_colors = ['lightpink', 'lightblue', 'blue', 'lightsalmon', 'red']\n",
    "\n",
    "    for i, param in enumerate(parameters):\n",
    "        if param in bar_vars:\n",
    "            vals = df[param]\n",
    "            if param == 'dperc':\n",
    "                vals *= -1\n",
    "                print(max(vals))\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=df.index, y=vals, name=param,\n",
    "                       marker=dict(color=bar_colors[bar_vars.index(param)])),\n",
    "                secondary_y=False,\n",
    "            )\n",
    "        else:\n",
    "            if param in ['et_act', 'etref'] and 'et_act' in parameters and 'etref' in parameters:\n",
    "                secondary_y = False\n",
    "            else:\n",
    "                secondary_y = True if i > 0 else False\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df.index, y=df[param], name=param),\n",
    "                secondary_y=secondary_y,\n",
    "            )\n",
    "\n",
    "    for param in parameters:\n",
    "        if param in ['etf_irr', 'etf_inv_irr', 'ndvi_irr', 'ndvi_inv_irr']:\n",
    "            ct_param = param + '_ct'\n",
    "            if ct_param in df.columns:\n",
    "                scatter_df = df[df[ct_param] == 1]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=scatter_df.index, y=scatter_df[param],\n",
    "                               mode='markers', marker_symbol='x',\n",
    "                               marker_size=5, name=f'{param} Retrieval'),\n",
    "                    secondary_y=True,\n",
    "                )\n",
    "\n",
    "    kwargs = dict(title_text=\"SWIM Model Time Series\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"mm\",\n",
    "        height=800,\n",
    "        template='plotly_dark',\n",
    "        xaxis=dict(showgrid=False),\n",
    "        yaxis=dict(showgrid=False),\n",
    "        yaxis2=dict(showgrid=False))\n",
    "    \n",
    "    if 'dperc' in parameters:\n",
    "        kwargs.update(dict(yaxis=dict(showgrid=False, range=[-20, None]), yaxis2=dict(showgrid=False, range=[-20, None])))\n",
    "        \n",
    "    fig.update_layout(**kwargs)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    if png_file:\n",
    "        fig.write_image(png_file)\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv(csv_161, index_col=0, parse_dates=True)\n",
    "print(df.columns.tolist()[:20])  # Show first 20 columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_timeseries(csv_161, ['soil_water', 'irrigation', 'rain', 'melt'], start='2017-01-01', end='2017-10-01')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the seasonal control on `soil_water`, from `melt` in the winter and spring, `rain` in May and June (the rainiest months in the area), and finally, as the soil water depletes during the hottest part of the growing season, `irrigation` kicks in. Use the range slider to zoom in on different time periods, and choose 'Pan' in the upper right to slide through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the deep percolation (recharge) caused by rain and snowmelt in late May, 2021. We made it 'negative' just so it would stand out."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_timeseries(csv_161, ['snow_fall', 'rain', 'melt', 'dperc'], start='2021-05-01', end='2021-07-01')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check out our input remote sensing data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_timeseries(csv_161, ['etf_irr', 'ndvi_irr'], start='2017-01-01', end='2021-01-01')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perhaps the most valuable information is the actual ET signal, which, when superimposed over the reference ET signal gives an indication of soil water and plant stress, or the absence of vegetation to carry on ET when the surface soil layer has dried. Let's look at the example from before, with irrigated field 128 and unirrigated field 130. Zoom in to view the irrigation application simulated by the model in the final week of July in field 128, which resulted in a period of ET at near the reference rate for the following two weeks. Meanwhile, the neighboring field, after the last good period of rain in early July, sees ET drop and stay low:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feature_128 = '043_000128'\n",
    "csv_128 = os.path.join(output_dir, f'combined_output_{feature_128}.csv')\n",
    "\n",
    "irr_in = fields.input_to_dataframe(feature_128)\n",
    "out_df = fields.output[feature_128].copy()\n",
    "irr = pd.concat([out_df, irr_in], axis=1, ignore_index=False)\n",
    "irr.to_csv(csv_128)\n",
    "\n",
    "irr_2004 = irr.loc['2004-01-01': '2004-12-31']\n",
    "print(f'total irrigation: {irr_2004.irrigation.sum():.1f} mm')\n",
    "print(f'total et: {irr_2004.et_act.sum():.1f} mm')\n",
    "print(f'total precip: {irr_2004.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_timeseries(irr, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], start='2004-01-01', end='2004-12-31')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feature_130 = '043_000130'\n",
    "csv_130 = os.path.join(output_dir, f'combined_output_{feature_130}.csv')\n",
    "\n",
    "unirr_in = fields.input_to_dataframe(feature_130)\n",
    "out_df = fields.output[feature_130].copy()\n",
    "unirr = pd.concat([out_df, unirr_in], axis=1, ignore_index=False)\n",
    "unirr.to_csv(csv_130)\n",
    "\n",
    "unirr_2004 = unirr.loc['2004-01-01': '2004-12-31']\n",
    "print(f'total irrigation: {unirr_2004.irrigation.sum():.1f} mm')\n",
    "print(f'total et: {unirr_2004.et_act.sum():.1f} mm')\n",
    "print(f'total precip: {unirr_2004.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_timeseries(unirr, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], start='2004-01-01', end='2004-12-31')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. Created a SwimContainer from your shapefile\n",
    "2. Extracted (or used pre-built) data from Earth Engine and GridMET\n",
    "3. Ingested data into the container\n",
    "4. Computed dynamics and exported model inputs\n",
    "5. Run the SWIM-RS model and visualized the results\n",
    "\n",
    "The model outputs include:\n",
    "- **et_act**: Actual evapotranspiration (mm/day)\n",
    "- **soil_water**: Soil water storage (mm)\n",
    "- **irrigation**: Simulated irrigation (mm/day)\n",
    "- **dperc**: Deep percolation / recharge (mm/day)\n",
    "- **swe**: Snow water equivalent (mm)\n",
    "\n",
    "Next steps might include:\n",
    "- Calibrating the model using PEST++ and observed ETf/SWE\n",
    "- Running the model for different time periods\n",
    "- Comparing irrigated vs non-irrigated fields\n",
    "- Aggregating results for water budget analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
