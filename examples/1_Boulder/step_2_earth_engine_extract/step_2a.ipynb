{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Overview\n",
    "\n",
    "In this tutorial we will extract all the data we need to run both the uncalibrated SWIM-RS model and to calibrate and run a parameterized model. The model uses a daily soil water balance, and as such, it needs a daily estimate of meteorological drivers and some *a priori* information on the soils in our sample locations. It also needs an estimate of the state of the vegetation on the surface, for which we use Landsat-based NDVI. With this information, we will be able to run SWIM-RS to estimate the daily ET, soil water storage, recharge, runoff, and simulated irrigation.\n",
    "\n",
    "To calibrate the model so it behaves more realistically, we must use the parameter inversion software (PEST++, in this case) alongside target data which provides *somewhat* independent estimates of ET and snow on the ground. Once the model is calibrated for a sample plot, these data are no longer needed. Therefore, the calibrated model can be run for periods before or after SNODAS or ETf data is available.\n",
    "\n",
    "The remote sensing data (ETf and NDVI) are the most time-consuming step, as the data are being extracted from potentially thousands of separate Landsat-like images in Earth Engine. The snow, soils, and irrigation data, on the other hand, are relatively quick, as the images are fewer, with one CONUS-wide image per day in SNODAS, a few static images for the soils data, and one image annually for the irrigation products. However, thanks to Earth Engine, even the Landsat-based products are quickly extracted if the number of sample plots are small and clustered in space, as they are for this tutorial.\n",
    "\n",
    "The SWIM-RS approach requires the following input datasets to run and/or calibrate:\n",
    "1. NDVI: Normalized Difference Vegetation Index is a measure made using the red and near-infrared bands of a multispectral instrument. It is a good way to estimate the relative density and vigor of vegetation, which is highly correlated with transpiration. NDVI is used as a proxy for the transpirative component of the crop coefficient in SWIM-RS, Kcb. Here, we access NDVI information from Landsat satellite images in Earth Engine.\n",
    "2. ETf: The rate of ET expressed as a fraction of reference/potential ET. This is also known in agricultural water use modeling as the 'crop coefficient', or Kc. For this tutorial we use SSEBop, accessed from Google Earth Engine. We could use results from any number of remote sensing-based modeling approaches (METRIC, OpenET ensemble, etc.). *FOR USE IN CALIBRATION ONLY*\n",
    "3. Soils: We need an initial estimate of soil hydraulic properties that govern the way water behaves in our very simple model soil water reservoir.\n",
    "4. Irrigation: We use an irrigation mask (IrrMapper or LANID) to constrain the data extraction of the irrigated and unirrigated portions of any given sample plot.\n",
    "5. Snow: We use the SNODAS product to estimate the snow water equivalent (SWE) at a daily time step to calibrate the simple snow model in SWIM-RS. *FOR USE IN CALIBRATION ONLY*\n",
    "\n",
    "Note: For this tutorial, we use 'field' and 'plot' somewhat interchangeably. Indeed, the sample plots for this tutorial are fields. However, we could draw an arbitraty polygon over a location of interest and run the model there. Keep in mind, the data will represent the mean of the irrigated and unirrigated portions of the sample plot. Therefore, using sensible land-use features (e.g., individual agricultrual fields) is a good approach because assuming homogenous land-use managment in a single field is not a terrible assumption. \n",
    "\n",
    "# Local Shapefile and Remote Sensing Extraction\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Point to our local shapefile (no asset upload required).\n",
    "2. Use the `clustered_sample_etf` function to perform SSEBop ETf extraction.\n",
    "3. Use the `clustered_sample_ndvi` function to perform SSEBop NDVI extraction.\n",
    "\n",
    "\n",
    "Ensure that the Earth Engine Python API is authenticated and configured, and that the Earth Engine CLI is available in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Authorize Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:17:06.467348Z",
     "start_time": "2025-11-04T02:17:05.947565Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import ee\n",
    "\n",
    "root = os.path.abspath('../../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.data_extraction.ee.etf_export import clustered_sample_etf\n",
    "from swimrs.data_extraction.ee.ndvi_export import clustered_sample_ndvi\n",
    "\n",
    "from swimrs.data_extraction.ee.ee_utils import is_authorized\n",
    "from swimrs.utils.google_bucket import list_and_copy_gcs_bucket\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../..'))\n",
    "sys.setrecursionlimit(5000)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If new to Earth Engine, checkout https://developers.google.com/earth-engine/guides/auth"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:17:07.720759Z",
     "start_time": "2025-11-04T02:17:06.487146Z"
    }
   },
   "source": [
    "if not is_authorized():\n",
    "    ee.Authenticate()\n",
    "ee.Initialize()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Point to Local Shapefile\n",
    "\n",
    "Set a path to your local shapefile. The extraction functions will convert it to an Earth Engine FeatureCollection under the hood — no upload needed.\n",
    "\n",
    "After the upload is complete, you can proceed with the extraction steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Extract ETf Raster Data\n",
    "\n",
    "Now we're ready to do 'zonal stats' on our fields. We provide a local shapefile, and the code converts it to a FeatureCollection internally, then extracts ETf/NDVI summaries per field to CSV. \n",
    "\n",
    "We need to use an irrigated lands mask (IrrMapper or LANID) to find irrigated and unirrigated zones within the polygons of our shapefile. You see this implemented in the code below, where the `mask` argument is either `irr` for irrigated, or `inv_irr` for the inverse of the irrigated mask, which are unirrigated areas.\n",
    "\n",
    "For the raster data extraction, there are three options to get at the data:\n",
    "\n",
    "*   **`clustered_sample_etf`**:\n",
    "    *   This function finds all Landsat images intersecting the sample polygons (i.e., our fields).\n",
    "    *   Since our fields are clustered together, this finds a reasonable number of images and iterates over them, extracting data from each.\n",
    "    *   We use this on the tutorial since the sample from the Montana fields database is geographically constrained.\n",
    "*   **`sparse_sample_etf`**:\n",
    "    *   This function assumes the samples (fields) are spread out over many different Landsat images.\n",
    "    *   It runs sample-by-sample, finding Landsat images overlapping each sample and extracting from them.\n",
    "    *   This is used when we extract data for John Volk's flux data set, which are widely spaced across the Conterminous US.\n",
    "*   **`export_etf_images`**:\n",
    "    *   This function exports the Landsat images themselves, clipped to the bounds of a 'hopefully' clustered set of sample polygons.\n",
    "    *   This is helpful for experimentation with buffering zones and so on, but not meant for large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:17:07.797604Z",
     "start_time": "2025-11-04T02:17:07.786422Z"
    }
   },
   "source": [
    "# Change this to your own\n",
    "user = 'dgketchum'\n",
    "ee_account = f'ee-{user}'\n",
    "\n",
    "# The shapefile\n",
    "shapefile_path = os.path.join(root, 'examples', '1_Boulder', 'data', 'gis', 'mt_sid_boulder.shp')\n",
    "\n",
    "# Export destination switch\n",
    "# drive = True  # default: export to Google Drive\n",
    "# To use a Cloud Storage bucket instead (faster), uncomment the next two lines and set your bucket:\n",
    "drive = False\n",
    "bucket = 'wudr'\n",
    "\n",
    "# If you're using a bucket, specifcy the GSUTIL command location\n",
    "# If you don't have gsutil, there is a workaround described below\n",
    "command = os.path.join(os.path.expanduser('~'), 'google-cloud-sdk', 'bin', 'gsutil')\n",
    "\n",
    "# Derived export settings used below\n",
    "export_dest = 'drive' if drive else 'bucket'\n",
    "export_bucket = None if drive else bucket\n",
    "bucket_subdir = 'swim/examples/1_Boulder'\n",
    "# Using local shapefile; no EE asset needed.\n",
    "\n",
    "# We must specify which column in the shapefile represents the field's unique ID, in this case it is 'FID_1'\n",
    "FEATURE_ID = 'FID_1'\n",
    "# Limit extraction to a few fields for this tutorial\n",
    "select_fields = ['043_000130', '043_000128', '043_000161']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Ensure that your Cloud Storage bucket has the correct permissions for Earth Engine to write to it.\n",
    "- You can modify parameters in the `clustered_sample_etf` function for different masking and debugging options.\n",
    "- The data is downloaded by year."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:17:07.861513Z",
     "start_time": "2025-11-04T02:17:07.856056Z"
    }
   },
   "source": "etf_dst = os.path.join(root, 'examples', '1_Boulder', 'data', 'landsat', 'extracts', 'etf')\n",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:18:15.509337Z",
     "start_time": "2025-11-04T02:17:07.921050Z"
    }
   },
   "source": [
    "# Here, we run the ETf extraction against the local shapefile (no asset upload needed).\n",
    "\n",
    "# every sample is divided into a 'purely' irrigated section (i.e., 'irr') and an unirrigated one (i.e., 'inv_irr')\n",
    "# this allows us to build a model for irrigated areas that aren't contaminated by unirrigated areas.\n",
    "# for this tutorial, we're going to use both\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "\n",
    "    # the 'check_dir' will check the planned directory for the existence of the data\n",
    "    # if a run fails for some reason, move what is complete from the bucket to the directory, then rerun\n",
    "    # this will skip what's already there\n",
    "    chk = os.path.join(etf_dst, '{}'.format(mask))\n",
    "\n",
    "    # write the directory if it's not already there\n",
    "    if not os.path.exists(chk):\n",
    "        os.makedirs(chk, exist_ok=True)\n",
    "\n",
    "    # Export ETf tables (Drive by default). Show both options explicitly for clarity.\n",
    "    if drive:\n",
    "        # Drive export (recommended for most users)\n",
    "        clustered_sample_etf(shapefile_path, bucket=None, debug=False, mask_type=mask, check_dir=None, start_yr=2004,\n",
    "                          end_yr=2023, feature_id=FEATURE_ID, select=select_fields, model='ssebop', usgs_nhm=True, dest='drive',\n",
    "                          state_col='STATE', drive_folder='swim', drive_categorize=True)\n",
    "    else:\n",
    "        # Cloud Storage export (faster, requires a bucket)\n",
    "        clustered_sample_etf(shapefile_path, bucket=bucket, debug=False, mask_type=mask, check_dir=None, start_yr=2004,\n",
    "                          end_yr=2023, feature_id=FEATURE_ID, select=select_fields, model='ssebop', usgs_nhm=True, dest='bucket', file_prefix=bucket_subdir,\n",
    "                          state_col='STATE', drive_categorize=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etf_inv_irr_2004\n",
      "etf_inv_irr_2005\n",
      "etf_inv_irr_2006\n",
      "etf_inv_irr_2007\n",
      "etf_inv_irr_2008\n",
      "etf_inv_irr_2009\n",
      "etf_inv_irr_2010\n",
      "etf_inv_irr_2011\n",
      "etf_inv_irr_2012\n",
      "etf_inv_irr_2013\n",
      "etf_inv_irr_2014\n",
      "etf_inv_irr_2015\n",
      "etf_inv_irr_2016\n",
      "etf_inv_irr_2017\n",
      "etf_inv_irr_2018\n",
      "etf_inv_irr_2019\n",
      "etf_inv_irr_2020\n",
      "etf_inv_irr_2021\n",
      "etf_inv_irr_2022\n",
      "etf_inv_irr_2023\n",
      "etf_irr_2004\n",
      "etf_irr_2005\n",
      "etf_irr_2006\n",
      "etf_irr_2007\n",
      "etf_irr_2008\n",
      "etf_irr_2009\n",
      "etf_irr_2010\n",
      "etf_irr_2011\n",
      "etf_irr_2012\n",
      "etf_irr_2013\n",
      "etf_irr_2014\n",
      "etf_irr_2015\n",
      "etf_irr_2016\n",
      "etf_irr_2017\n",
      "etf_irr_2018\n",
      "etf_irr_2019\n",
      "etf_irr_2020\n",
      "etf_irr_2021\n",
      "etf_irr_2022\n",
      "etf_irr_2023\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Retrieve ETf Data from Google Drive (or Cloud Storage)\n",
    "\n",
    "Monitor export progress in the Earth Engine Code Editor (https://code.earthengine.google.com/) or the task monitor (https://code.earthengine.google.com/tasks). When tasks finish, retrieve the CSVs to your local machine as follows:\n",
    "\n",
    "Steps (Google Drive):\n",
    "1) Open Google Drive and locate the export folders. With `drive_categorize=True`, files go under separate folders:\n",
    "   - `swim_etf` for ETf exports\n",
    "   - `swim_ndvi` for NDVI exports\n",
    "   If categorization is disabled, look under the single `swim` folder.\n",
    "2) Inside `swim_etf`, you should see CSVs named like `etf_irr_2004.csv`, `etf_inv_irr_2004.csv`, etc. Inside `swim_ndvi`, you should see `ndvi_irr_2004.csv`, `ndvi_inv_irr_2004.csv`, etc.\n",
    "3) Create local folders in this project for the extracts (if they don’t exist):\n",
    "   - `examples/1_Boulder/data/landsat/extracts/etf/irr`\n",
    "   - `examples/1_Boulder/data/landsat/extracts/etf/inv_irr`\n",
    "   - `examples/1_Boulder/data/landsat/extracts/ndvi/irr`\n",
    "   - `examples/1_Boulder/data/landsat/extracts/ndvi/inv_irr`\n",
    "4) Download CSVs from Drive and place them into the matching local folders:\n",
    "   - `etf_irr_*.csv` → `.../extracts/etf/irr/`\n",
    "   - `etf_inv_irr_*.csv` → `.../extracts/etf/inv_irr/`\n",
    "   - `ndvi_irr_*.csv` → `.../extracts/ndvi/irr/`\n",
    "   - `ndvi_inv_irr_*.csv` → `.../extracts/ndvi/inv_irr/`\n",
    "5) Verify that years in your study period (e.g., 2004–2023) are present for both masks before proceeding.\n",
    "\n",
    "Tip: exporting to a Google Cloud Storage bucket is significantly faster, but most users will not have a bucket. If you do have a bucket, you can switch to `dest='bucket'` in the export calls and then move data with `gsutil` or the helper in `swimrs.utils.google_bucket`. For example:\n",
    "- List and copy with gsutil (Cloud SDK): `gsutil -m cp gs://<bucket>/etf*inv_irr* examples/1_Boulder/data/landsat/extracts/etf/inv_irr/`\n",
    "- Or use: `swimrs.utils.google_bucket.list_and_copy_gcs_bucket(cmd_path='gsutil', bucket_path='<bucket>', local_dir='.../extracts/etf/irr', glob='etf_irr')`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:18:15.994587Z",
     "start_time": "2025-11-04T02:18:15.582714Z"
    }
   },
   "source": [
    "for mask in ['inv_irr', 'irr']:\n",
    "    dst = os.path.join(etf_dst, '{}'.format(mask))\n",
    "    glob_ = f'etf_{mask}'\n",
    "\n",
    "    # list the data\n",
    "    list_and_copy_gcs_bucket(command, bucket, dst, glob=glob_, dry_run=True)\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dgketchum/google-cloud-sdk/bin/gsutil'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m glob_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124metf_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmask\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# list the data\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mlist_and_copy_gcs_bucket\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbucket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglob\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglob_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdry_run\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/swim-rs/src/swimrs/utils/google_bucket.py:10\u001B[0m, in \u001B[0;36mlist_and_copy_gcs_bucket\u001B[0;34m(cmd_path, bucket_path, local_dir, glob, dry_run, overwrite)\u001B[0m\n\u001B[1;32m      8\u001B[0m prepend \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs://\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      9\u001B[0m list_cmd \u001B[38;5;241m=\u001B[39m [cmd_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mls\u001B[39m\u001B[38;5;124m'\u001B[39m, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(prepend, bucket_path)]\n\u001B[0;32m---> 10\u001B[0m list_process \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlist_cmd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstderr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m stdout, stderr \u001B[38;5;241m=\u001B[39m list_process\u001B[38;5;241m.\u001B[39mcommunicate()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stderr:\n",
      "File \u001B[0;32m~/miniconda/envs/swim/lib/python3.9/subprocess.py:951\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001B[0m\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[1;32m    948\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[1;32m    949\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m--> 951\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[0;32m~/miniconda/envs/swim/lib/python3.9/subprocess.py:1837\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001B[0m\n\u001B[1;32m   1835\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errno_num \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1836\u001B[0m         err_msg \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstrerror(errno_num)\n\u001B[0;32m-> 1837\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001B[1;32m   1838\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(err_msg)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/dgketchum/google-cloud-sdk/bin/gsutil'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for mask in ['inv_irr', 'irr']:\n",
    "    dst = os.path.join(root, 'examples', '1_Boulder', 'data', 'landsat', 'extracts', 'etf', f'{mask}')\n",
    "    glob_ = f'etf_{mask}'\n",
    "\n",
    "    # copy the data\n",
    "    list_and_copy_gcs_bucket(command, bucket, dst, glob=glob_, dry_run=False, overwrite=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Extract NDVI Raster Data\n",
    "\n",
    "This is just like the ETf extraction, but for NDVI. This is a little more straightforward as we can get the data straight from the Landsat collection, and don't need special permissions or knowledge of where the data are stored.\n",
    "\n",
    "As with the ETf code, the extraction has three options to get at the data, depending on the clustering of fields and user needs, and the functions are split up in the same way:\n",
    "\n",
    "*   **`clustered_sample_ndvi`**:\n",
    "    *   This function finds all Landsat images intersecting the sample polygons (i.e., our fields).\n",
    "    *   Since our fields are clustered together, this finds a reasonable number of images and iterates over them, extracting data from each.\n",
    "    *   We use this on the tutorial since the sample from the Montana fields database is geographically constrained.\n",
    "*   **`sparse_sample_ndvi`**:\n",
    "    *   This function assumes the samples (fields) are spread out over many different Landsat images.\n",
    "    *   It runs sample-by-sample, finding Landsat images overlapping each sample and extracting from them.\n",
    "    *   This is used when we extract data for John Volk's flux data set, which are widely spaced across the Conterminous US.\n",
    "*   **`export_ndvi_images`**:\n",
    "    *   This function exports the Landsat images themselves, clipped to the bounds of a 'hopefully' clustered set of sample polygons.\n",
    "    *   This is helpful for experimentation with buffering zones and so on, but not meant for large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ndvi_dst = os.path.join(root, 'examples', '1_Boulder', 'data', 'landsat', 'extracts', 'ndvi')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Just like before, but with 'ndvi' instead of 'etf':\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "\n",
    "    # the 'check_dir' will check the planned directory for the existence of the data\n",
    "    # if a run fails for some reason, move what is complete from the bucket to the directory, then rerun\n",
    "    # this will skip what's already there\n",
    "    chk = os.path.join(ndvi_dst, '{}'.format(mask))\n",
    "\n",
    "    # write the directory if it's not already there\n",
    "    if not os.path.exists(chk):\n",
    "        os.makedirs(chk, exist_ok=True)\n",
    "\n",
    "    # Export NDVI tables (Drive by default). Show both options explicitly.\n",
    "    if drive:\n",
    "        clustered_sample_ndvi(shapefile_path, bucket=None, debug=False, mask_type=mask, check_dir=None, start_yr=2004,\n",
    "                           end_yr=2023, feature_id=FEATURE_ID, select=select_fields, satellite='landsat', dest='drive', drive_folder='swim',\n",
    "                           drive_categorize=True)\n",
    "    else:\n",
    "        clustered_sample_ndvi(shapefile_path, bucket=bucket, debug=False, mask_type=mask, check_dir=None, start_yr=2004,\n",
    "                           end_yr=2023, feature_id=FEATURE_ID, select=select_fields, satellite='landsat', dest='bucket', file_prefix=bucket_subdir,\n",
    "                           drive_categorize=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Just like before, but with 'ndvi' instead of 'etf'\n",
    "for mask in ['inv_irr', 'irr']:\n",
    "    dst = os.path.join(ndvi_dst, f'{mask}')\n",
    "    glob_ = f'ndvi_{mask}'\n",
    "\n",
    "    # copy the data from a Cloud Storage bucket, or download from Drive manually\n",
    "    if not drive and bucket:\n",
    "        list_and_copy_gcs_bucket(command, bucket, dst, glob=glob_, dry_run=False, overwrite=False)\n",
    "    else:\n",
    "        print('Drive exports: download NDVI CSVs from Drive into', dst)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
