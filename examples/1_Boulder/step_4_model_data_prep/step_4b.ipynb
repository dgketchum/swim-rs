{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70a7a0b-5c27-4914-a3ea-c6b380097323",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Model Data Preparation - Second Part (Prep Model File)\n",
    "\n",
    "This second half of Step 4 is to complete the meteorlogy data, join it to the remote sensing data, and prepare the final model input file.\n",
    "\n",
    "Now that the remote sensing data is prepped, we can proceed to 3 and 4:\n",
    "\n",
    "1. ~~Write each field's soil and irrigation infomation into a study-wide properties file.~~\n",
    "2. ~~Process the Landsat data and run a simple analysis of per-field NDVI dynamics that will provide some information on likely subseasonal irrigation application dates, harvest, and fallowing.~~\n",
    "3. Join our Earth Engine extracts and meteorology data into a per-field time series.\n",
    "4. Finally, write a single model input file that has all of the data needed.\n",
    "\n",
    "Step 3 depends on a successful run of Step 2, and Step 4 depends on Step 3, so ensure the code runs to completion on each before moving on.\n",
    "\n",
    "Note: For this tutorial we're specifying the data directories to which the data is being written as needed. This gets messy but is worth it to learn about the workflow in our first project. In the subsequent tutorial, we will standardize the directory structure for the calibration project and use a configuration file to specify all directories, model metadata, date range for the study period, etc. This will simplify our lives by hiding a lot of what we do in that tutorial under the hood of our SWIM car, so we can focus on calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6cc59b447795ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# append the project path to the environment\n",
    "root = os.path.abspath('../../..')\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7411b-6b59-41f3-ba69-ded4888ed72c",
   "metadata": {},
   "source": [
    "## 3. Join the Earth Engine and meteorology time series.\n",
    "\n",
    "We now specify the inputs we're going to use for our time series, which will have irrigated and unirrigated ETf and NDVI, and all the meteorology data we pulled from GridMET and NLDAS-2. We will need the shapefile we built that has the associated GridMET 'GFID' attribute added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60c754b-e589-4b7e-bda6-3eb27f4d2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-specific imports\n",
    "from swimrs.prep.field_timeseries import join_daily_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370415383c3d50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can start to see why we want all these paths in a configuration file, it feels like a waste\n",
    "# of time writing some of these for the third', 'fourth time\n",
    "fields_gridmet = os.path.join(root, 'examples', '1_Boulder', 'data', 'gis', 'mt_sid_boulder_gfid.shp')\n",
    "met = os.path.join(root, 'examples', '1_Boulder', 'data', 'met_timeseries')\n",
    "landsat = os.path.join(root, 'examples', '1_Boulder', 'data', 'landsat', 'remote_sensing.csv')\n",
    "snow = os.path.join(root, 'examples', '1_Boulder', 'data', 'snodas', 'snodas.json')\n",
    "\n",
    "joined_timeseries = os.path.join(root, 'examples', '1_Boulder', 'data', 'plot_timeseries')\n",
    "if not os.path.isdir(joined_timeseries):\n",
    "    os.mkdir(joined_timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2c73698c41c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [02:23<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 fields were successfully processed\n",
      "0 fields were dropped due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = ['etf_inv_irr',\n",
    "          'ndvi_inv_irr',\n",
    "          'etf_irr',\n",
    "          'ndvi_irr']\n",
    "params += ['{}_ct'.format(p) for p in params]\n",
    "\n",
    "join_daily_timeseries(fields=fields_gridmet,\n",
    "                      met_dir=met,\n",
    "                      rs_dir=landsat,\n",
    "                      snow=snow,\n",
    "                      dst_dir=joined_timeseries,\n",
    "                      overwrite=True,\n",
    "                      start_date='2004-01-01',\n",
    "                      end_date='2022-12-31',\n",
    "                      feature_id='FID_1',\n",
    "                      **{'params': params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a4dd2-741e-47d1-a60f-605c4a357c75",
   "metadata": {},
   "source": [
    "The ETf Earth Engine collection over the study area is patchy in 2023, so we reduced the coverage from 2004 - 2022. The way the model is setup right now will drop a field entirely if it has a missing year, so it's better to reduce the time coverage, rather than drop a bunch of fields from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdaf664-cd24-4840-8842-d6b7a2a2b002",
   "metadata": {},
   "source": [
    "## 4. Write the model input file.\n",
    "\n",
    "We now have everything we need and can run the final data preparation function `prep_fields_json`, which will bring together all the data we built and put it in a file format that will run much faster than if we fed the model all those .csv files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0405d88f-a739-4da6-97c2-d7582d6d0d70",
   "metadata": {},
   "outputs": [],
   "source": "from swimrs.prep.prep_plots import prep_fields_json"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b534645-0448-4cc1-8bf3-3dbbc4fb316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the properites and cuttings files we prepared before\n",
    "properties_json = os.path.join(root, 'examples', '1_Boulder', 'data', 'tutorial_properties.json')\n",
    "cuttings_json = os.path.join(root, 'examples', '1_Boulder',  'data', 'landsat', 'tutorial_cuttings.json')\n",
    "\n",
    "# the model input file\n",
    "prepped_input = os.path.join(root, 'examples', '1_Boulder', 'data', 'prepped_input.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5ecb43-9283-4821-a39f-b81aefb586c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [00:02<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote /home/dgketchum/PycharmProjects/swim-rs/tutorials/1_Boulder/data/prepped_input.json\n"
     ]
    }
   ],
   "source": [
    "processed_targets, excluded_targets = prep_fields_json(properties_json, joined_timeseries, prepped_input,\n",
    "                                                       dynamics=cuttings_json, target_plots=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55402502-f009-4a26-8224-270c957123fe",
   "metadata": {},
   "source": [
    "Pretty easy! On to running the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
