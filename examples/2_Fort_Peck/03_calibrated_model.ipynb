{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Calibration Tutorial - Fort Peck, MT - Unirrigated Flux Plot\n",
    "\n",
    "## Step 3: Running the Calibrated Model\n",
    "\n",
    "Now we evaluate whether calibration improved model performance by running in **forecast mode** with calibrated parameters.\n",
    "\n",
    "This notebook:\n",
    "1. Visualizes how parameters evolved during calibration\n",
    "2. Runs the model with calibrated parameters\n",
    "3. Compares calibrated vs uncalibrated performance against flux observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.swim.sampleplots import SamplePlots\n",
    "from swimrs.model.obs_field_cycle import field_day_loop\n",
    "\n",
    "from swimrs.viz.param_evolution import plot_parameter_histograms\n",
    "from swimrs.viz.swim_timeseries import plot_swim_timeseries\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_ws = os.path.abspath('.')\n",
    "data = os.path.join(project_ws, 'data')\n",
    "pest_dir = os.path.join(project_ws, 'pest')\n",
    "\n",
    "config_file = os.path.join(project_ws, '2_Fort_Peck.toml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-param-header",
   "metadata": {},
   "source": [
    "## 1. Visualize Parameter Evolution\n",
    "\n",
    "Let's see how the parameters changed across optimization iterations. The histograms show the distribution of parameter values across ensemble realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-param-hist",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = os.path.join(project_ws, 'params.csv')\n",
    "\n",
    "# Get all parameter files from optimization steps\n",
    "steps = []\n",
    "for i in range(10):  # Check up to 10 iterations\n",
    "    step_file = os.path.join(pest_dir, f'2_Fort_Peck.{i}.par.csv')\n",
    "    if os.path.exists(step_file):\n",
    "        steps.append(step_file)\n",
    "\n",
    "if steps:\n",
    "    print(f\"Found {len(steps)} optimization steps\")\n",
    "    \n",
    "    fig_dir = os.path.join(project_ws, 'figures', 'parameter_hist')\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot histograms (set fig_out_dir=fig_dir to save PNGs)\n",
    "    plot_parameter_histograms(initial_params, steps, fig_out_dir=None)\n",
    "else:\n",
    "    print(\"No parameter files found. Run notebook 02_calibration first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-header",
   "metadata": {},
   "source": [
    "## 2. Run the Calibrated Model\n",
    "\n",
    "To run with calibrated parameters:\n",
    "1. Set `forecast=True` when reading config\n",
    "2. Ensure `[forecast]` section in config points to the final `.par.csv` file\n",
    "\n",
    "The config file should have:\n",
    "```toml\n",
    "[forecast]\n",
    "forecast_parameters = \"{pest_run_dir}/pest/2_Fort_Peck.3.par.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fields(ini_path, project_ws, selected_feature, output_csv, forecast=False):\n",
    "    \"\"\"Run SWIM model and save combined input/output to CSV.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = ProjectConfig()\n",
    "    config.read_config(ini_path, project_ws, forecast=forecast)\n",
    "\n",
    "    fields = SamplePlots()\n",
    "    fields.initialize_plot_data(config)\n",
    "    fields.output = field_day_loop(config, fields, debug_flag=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'\\nExecution time: {end_time - start_time:.2f} seconds\\n')\n",
    "\n",
    "    out_df = fields.output[selected_feature].copy()\n",
    "    in_df = fields.input_to_dataframe(selected_feature)\n",
    "    df = pd.concat([out_df, in_df], axis=1, ignore_index=False)\n",
    "    df.to_csv(output_csv)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = 'US-FPe'\n",
    "out_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_calibrated.csv')\n",
    "\n",
    "# Run with forecast=True to use calibrated parameters\n",
    "df = run_fields(config_file, project_ws, selected_feature=selected_feature, \n",
    "                output_csv=out_csv, forecast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 3. Visualize Calibrated Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-year",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf = df.loc['2004-01-01': '2004-12-31']\n",
    "print(f'Total irrigation: {ydf.irrigation.sum():.1f} mm')\n",
    "print(f'Total ET: {ydf.et_act.sum():.1f} mm')\n",
    "print(f'Total precipitation: {ydf.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_swim_timeseries(ydf, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], \n",
    "                     start='2004-01-01', end='2004-12-31', png_dir='et_calibrated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-header",
   "metadata": {},
   "source": [
    "## 4. Compare with Flux Tower Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_etf_estimates(combined_output_path, flux_data_path, irr=False):\n",
    "    \"\"\"Compare model Kc_act and SSEBop ETf against flux tower observations.\"\"\"\n",
    "    flux_data = pd.read_csv(flux_data_path)\n",
    "    flux_data = flux_data.set_index(pd.to_datetime(flux_data['date']))['EToF']\n",
    "\n",
    "    combined_output = pd.read_csv(combined_output_path, index_col=0)\n",
    "    combined_output.index = pd.to_datetime(combined_output.index)\n",
    "\n",
    "    if irr:\n",
    "        etf, ct = 'etf_irr', 'etf_irr_ct'\n",
    "    else:\n",
    "        etf, ct = 'etf_inv_irr', 'etf_inv_irr_ct'\n",
    "\n",
    "    df = pd.DataFrame({'kc_act': combined_output['kc_act'],\n",
    "                       'etf': combined_output[etf],\n",
    "                       'ct': combined_output[ct],\n",
    "                       'EToF': flux_data})\n",
    "\n",
    "    # Filter for days that have a SSEBop ETf retrieval and a flux observation\n",
    "    df = df.dropna()\n",
    "    df = df.loc[df['ct'] == 1]\n",
    "\n",
    "    # Calculate RMSE and R-squared\n",
    "    rmse_kc_act = np.sqrt(mean_squared_error(df['EToF'], df['kc_act']))\n",
    "    r2_kc_act = r2_score(df['EToF'], df['kc_act'])\n",
    "\n",
    "    rmse_ssebop = np.sqrt(mean_squared_error(df['EToF'], df['etf']))\n",
    "    r2_ssebop = r2_score(df['EToF'], df['etf'])\n",
    "\n",
    "    print(f\"SWIM Kc_act vs. Flux EToF: RMSE = {rmse_kc_act:.2f}, R-squared = {r2_kc_act:.2f}\")\n",
    "    print(f\"SSEBop ETf vs. Flux EToF: RMSE = {rmse_ssebop:.2f}, R-squared = {r2_ssebop:.2f}\")\n",
    "    \n",
    "    return df, rmse_kc_act, r2_kc_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_data = os.path.join(data, 'US-FPe_daily_data.csv')\n",
    "comparison_df, rmse_cal, r2_cal = compare_etf_estimates(out_csv, flux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-uncal-header",
   "metadata": {},
   "source": [
    "### Compare with Uncalibrated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-both",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_uncalibrated.csv')\n",
    "\n",
    "if os.path.exists(uncal_csv):\n",
    "    print(\"Uncalibrated model:\")\n",
    "    _, rmse_uncal, r2_uncal = compare_etf_estimates(uncal_csv, flux_data)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"IMPROVEMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"RMSE reduction: {rmse_uncal:.2f} -> {rmse_cal:.2f} ({(rmse_uncal-rmse_cal)/rmse_uncal*100:.1f}% improvement)\")\n",
    "    print(f\"R-squared:      {r2_uncal:.2f} -> {r2_cal:.2f}\")\n",
    "else:\n",
    "    print(\"Uncalibrated output not found. Run notebook 01 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatter plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(comparison_df['EToF'], comparison_df['kc_act'], alpha=0.5, s=10)\n",
    "ax1.plot([0, 1.5], [0, 1.5], 'r--', label='1:1 line')\n",
    "ax1.set_xlabel('Flux EToF')\n",
    "ax1.set_ylabel('SWIM Kc_act')\n",
    "ax1.set_title(f'SWIM vs Flux (Calibrated)\\nRMSE={rmse_cal:.2f}, R2={r2_cal:.2f}')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 1.5)\n",
    "ax1.set_ylim(0, 1.5)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(comparison_df['EToF'], comparison_df['etf'], alpha=0.5, s=10)\n",
    "ax2.plot([0, 1.5], [0, 1.5], 'r--', label='1:1 line')\n",
    "ax2.set_xlabel('Flux EToF')\n",
    "ax2.set_ylabel('SSEBop ETf')\n",
    "ax2.set_title('SSEBop vs Flux')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, 1.5)\n",
    "ax2.set_ylim(0, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_scatter_calibrated.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "Calibration significantly improved model performance:\n",
    "\n",
    "- **RMSE reduced by >40%** from the uncalibrated model\n",
    "- **SWIM now outperforms SSEBop** in daily ET estimation\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "**The key insight is that we can mine the deep remote sensing-based ET record, but rather than driving the model with remote sensing ET directly, we drive the calibration with it.**\n",
    "\n",
    "The model has access to:\n",
    "1. Daily meteorological data (not just satellite overpass days)\n",
    "2. Physically-based soil water balance constraints\n",
    "3. Flexibility to tune parameters using the remote sensing record\n",
    "\n",
    "This combination gives SWIM a more grounded perspective on daily fluxes than remote sensing alone, resulting in better ET estimates.\n",
    "\n",
    "### Practical implications\n",
    "\n",
    "- **For data-sparse regions:** Calibrate once using available remote sensing, then apply calibrated parameters to generate daily ET\n",
    "- **For irrigated lands:** Same approach works with mask-switching between irrigated/non-irrigated periods\n",
    "- **For regional applications:** See Tutorial 4 (Flux Network) for scaling this approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
