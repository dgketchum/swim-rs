{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Calibration Tutorial - Fort Peck, MT - Unirrigated Flux Plot\n",
    "\n",
    "## Step 3: Running the Calibrated Model\n",
    "\n",
    "Now we evaluate whether calibration improved model performance by running in **forecast mode** with calibrated parameters.\n",
    "\n",
    "This notebook:\n",
    "1. Visualizes how parameters evolved during calibration\n",
    "2. Runs the model with calibrated parameters\n",
    "3. Compares calibrated vs uncalibrated performance against flux observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:05.021093235Z",
     "start_time": "2026-01-27T19:16:04.962655271Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "root = os.path.abspath(\"../..\")\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.container import SwimContainer\n",
    "from swimrs.process.input import build_swim_input\n",
    "from swimrs.process.loop_fast import run_daily_loop_fast\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.viz.param_evolution import plot_parameter_histograms\n",
    "from swimrs.viz.swim_timeseries import plot_swim_timeseries\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:05.073543058Z",
     "start_time": "2026-01-27T19:16:05.022721515Z"
    }
   },
   "outputs": [],
   "source": [
    "project_ws = os.path.abspath(\".\")\n",
    "data = os.path.join(project_ws, \"data\")\n",
    "pestrun = os.path.join(data, \"pestrun\")\n",
    "pest_dir = os.path.join(pestrun, \"pest\")\n",
    "\n",
    "config_file = os.path.join(project_ws, \"2_Fort_Peck.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-param-header",
   "metadata": {},
   "source": [
    "## 1. Visualize Parameter Evolution\n",
    "\n",
    "Let's see how the parameters changed across optimization iterations. The histograms show the distribution of parameter values across ensemble realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-param-hist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:05.131536303Z",
     "start_time": "2026-01-27T19:16:05.075462176Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_params = os.path.join(pestrun, \"params.csv\")\n",
    "\n",
    "# Get all parameter files from optimization steps\n",
    "steps = []\n",
    "for i in range(10):  # Check up to 10 iterations\n",
    "    step_file = os.path.join(pest_dir, f\"2_Fort_Peck.{i}.par.csv\")\n",
    "    if os.path.exists(step_file):\n",
    "        steps.append(step_file)\n",
    "\n",
    "if steps:\n",
    "    print(f\"Found {len(steps)} optimization steps\")\n",
    "\n",
    "    fig_dir = os.path.join(project_ws, \"figures\", \"parameter_hist\")\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "    # Plot histograms (set fig_out_dir=fig_dir to save PNGs)\n",
    "    plot_parameter_histograms(initial_params, steps, fig_out_dir=None)\n",
    "else:\n",
    "    print(\"No parameter files found. Run notebook 02_calibration first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuoeepayuti",
   "metadata": {},
   "source": [
    "### Parameter Comparison Table\n",
    "\n",
    "Compare initial parameter values with calibrated ensemble statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597cqo2cx5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:05.201704315Z",
     "start_time": "2026-01-27T19:16:05.133755019Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_parameter_comparison(initial_params_file, calibrated_params_file):\n",
    "    \"\"\"Display comparison of initial vs calibrated parameter values.\"\"\"\n",
    "\n",
    "    # Parameter descriptions\n",
    "    param_descriptions = {\n",
    "        \"aw\": \"Available water capacity (mm)\",\n",
    "        \"ks_alpha\": \"Evap stress coefficient\",\n",
    "        \"kr_alpha\": \"Transpiration stress coefficient\",\n",
    "        \"ndvi_k\": \"NDVI-Kcb slope\",\n",
    "        \"ndvi_0\": \"NDVI at Kcb=0\",\n",
    "        \"mad\": \"Mgmt allowed depletion\",\n",
    "        \"swe_alpha\": \"SWE melt coefficient\",\n",
    "        \"swe_beta\": \"SWE melt exponent\",\n",
    "    }\n",
    "\n",
    "    # Known parameter names (to properly parse from column headers)\n",
    "    known_params = [\n",
    "        \"aw\",\n",
    "        \"ks_alpha\",\n",
    "        \"kr_alpha\",\n",
    "        \"ndvi_k\",\n",
    "        \"ndvi_0\",\n",
    "        \"mad\",\n",
    "        \"swe_alpha\",\n",
    "        \"swe_beta\",\n",
    "    ]\n",
    "\n",
    "    # Read initial parameters\n",
    "    initial_df = pd.read_csv(initial_params_file, index_col=0)\n",
    "\n",
    "    # Read calibrated parameters (ensemble)\n",
    "    cal_df = pd.read_csv(calibrated_params_file)\n",
    "\n",
    "    # Extract parameter columns (skip 'real_name' column)\n",
    "    param_cols = [c for c in cal_df.columns if c.startswith(\"pname:\")]\n",
    "\n",
    "    rows = []\n",
    "    for col in param_cols:\n",
    "        # Parse parameter name from column header like 'pname:p_ndvi_k_us-fpe_:0_ptype:cn_usecol:1_pstyle:m'\n",
    "        parts = col.split(\":\")\n",
    "        pname_part = parts[1]  # 'p_ndvi_k_us-fpe_'\n",
    "\n",
    "        # Find which known parameter this column represents\n",
    "        param_key = None\n",
    "        for known in known_params:\n",
    "            if pname_part.startswith(f\"p_{known}_\"):\n",
    "                param_key = known\n",
    "                break\n",
    "\n",
    "        if param_key is None:\n",
    "            continue\n",
    "\n",
    "        # Get calibrated ensemble statistics\n",
    "        cal_values = cal_df[col].values\n",
    "        cal_mean = cal_values.mean()\n",
    "        cal_std = cal_values.std()\n",
    "\n",
    "        # Find matching initial parameter\n",
    "        initial_val = None\n",
    "        for idx in initial_df.index:\n",
    "            if param_key in idx.lower():\n",
    "                initial_val = initial_df.loc[idx, \"value\"]\n",
    "                break\n",
    "\n",
    "        if initial_val is not None:\n",
    "            change = cal_mean - initial_val\n",
    "            pct_change = (change / initial_val) * 100 if initial_val != 0 else 0\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Parameter\": param_key,\n",
    "                    \"Initial\": f\"{initial_val:.3f}\",\n",
    "                    \"Cal Mean\": f\"{cal_mean:.3f}\",\n",
    "                    \"Cal Std\": f\"{cal_std:.3f}\",\n",
    "                    \"Change\": f\"{change:+.3f}\",\n",
    "                    \"% Change\": f\"{pct_change:+.1f}%\",\n",
    "                    \"Description\": param_descriptions.get(param_key, \"\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "\n",
    "    print(\"=\" * 95)\n",
    "    print(\"PARAMETER COMPARISON: Initial vs Calibrated\")\n",
    "    print(\"=\" * 95)\n",
    "    print(result_df.to_string(index=False))\n",
    "    print(\"=\" * 95)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Find the final calibrated parameter file\n",
    "if steps:\n",
    "    final_params = steps[-1]\n",
    "    print(f\"Using calibrated parameters from: {os.path.basename(final_params)}\\n\")\n",
    "    param_comparison = show_parameter_comparison(initial_params, final_params)\n",
    "else:\n",
    "    # Check archive directory for calibrated parameters\n",
    "    archive_dir = os.path.join(pest_dir, \"archive\")\n",
    "    if os.path.exists(archive_dir):\n",
    "        archive_files = sorted([f for f in os.listdir(archive_dir) if f.endswith(\".par.csv\")])\n",
    "        if archive_files:\n",
    "            final_params = os.path.join(archive_dir, archive_files[-1])\n",
    "            print(f\"Using archived calibrated parameters from: {archive_files[-1]}\\n\")\n",
    "            param_comparison = show_parameter_comparison(initial_params, final_params)\n",
    "        else:\n",
    "            print(\"No calibrated parameter files found in archive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-header",
   "metadata": {},
   "source": [
    "## 2. Run the Calibrated Model\n",
    "\n",
    "To run with calibrated parameters:\n",
    "1. Set `forecast=True` when reading config\n",
    "2. Ensure `[forecast]` section in config points to the final `.par.csv` file\n",
    "\n",
    "The config file should have:\n",
    "```toml\n",
    "[forecast]\n",
    "forecast_parameters = \"{pest_run_dir}/pest/2_Fort_Peck.3.par.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:05.276852533Z",
     "start_time": "2026-01-27T19:16:05.204161168Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_fields(config, container, selected_feature, output_csv, forecast=False):\n",
    "    \"\"\"Run SWIM model using the process package and save output to CSV.\n",
    "\n",
    "    Uses the modern process package workflow:\n",
    "    1. Build SwimInput from container (with calibrated params if forecast=True)\n",
    "    2. Run simulation with run_daily_loop_fast()\n",
    "    3. Convert DailyOutput to DataFrame with time series\n",
    "    4. Add ETf observations from container for comparison\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create temporary HDF5 for SwimInput\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False) as tmp:\n",
    "        temp_h5_path = tmp.name\n",
    "\n",
    "    # Handle calibrated parameters for forecast mode\n",
    "    calibrated_params_path = None\n",
    "    if forecast:\n",
    "        # Read forecast parameters and convert to JSON\n",
    "        config.read_forecast_parameters()\n",
    "        if hasattr(config, \"forecast_parameters\") and config.forecast_parameters is not None:\n",
    "            calibrated_params_path = _convert_forecast_params_to_json(\n",
    "                config.forecast_parameters, os.path.dirname(output_csv)\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        # Build SwimInput from container\n",
    "        swim_input = build_swim_input(\n",
    "            container,\n",
    "            output_h5=temp_h5_path,\n",
    "            calibrated_params_path=calibrated_params_path,\n",
    "            runoff_process=getattr(config, \"runoff_process\", \"cn\"),\n",
    "            etf_model=getattr(config, \"etf_target_model\", \"ptjpl\"),\n",
    "            met_source=getattr(config, \"met_source\", \"gridmet\"),\n",
    "            fields=[selected_feature],\n",
    "        )\n",
    "\n",
    "        # Run simulation (uses fast JIT-compiled loop)\n",
    "        output, final_state = run_daily_loop_fast(swim_input)\n",
    "\n",
    "        # Get time series data\n",
    "        n_days = swim_input.n_days\n",
    "        dates = pd.date_range(swim_input.start_date, periods=n_days, freq=\"D\")\n",
    "\n",
    "        # Get input time series for DataFrame\n",
    "        etr = swim_input.get_time_series(\"etr\")\n",
    "        prcp = swim_input.get_time_series(\"prcp\")\n",
    "        tmin = swim_input.get_time_series(\"tmin\")\n",
    "        tmax = swim_input.get_time_series(\"tmax\")\n",
    "        ndvi = swim_input.get_time_series(\"ndvi\")\n",
    "\n",
    "        # Build DataFrame (field index 0 since we're doing single field)\n",
    "        i = 0\n",
    "        df_data = {\n",
    "            # Model outputs\n",
    "            \"et_act\": output.eta[:, i],\n",
    "            \"etref\": etr[:, i],\n",
    "            \"kc_act\": output.etf[:, i],\n",
    "            \"kc_bas\": output.kcb[:, i],\n",
    "            \"ks\": output.ks[:, i],\n",
    "            \"ke\": output.ke[:, i],\n",
    "            \"melt\": output.melt[:, i],\n",
    "            \"rain\": output.rain[:, i],\n",
    "            \"depl_root\": output.depl_root[:, i],\n",
    "            \"dperc\": output.dperc[:, i],\n",
    "            \"runoff\": output.runoff[:, i],\n",
    "            \"swe\": output.swe[:, i],\n",
    "            \"irrigation\": output.irr_sim[:, i],\n",
    "            \"gw_sim\": output.gw_sim[:, i],\n",
    "            # Input time series\n",
    "            \"ppt\": prcp[:, i],\n",
    "            \"tmin\": tmin[:, i],\n",
    "            \"tmax\": tmax[:, i],\n",
    "            \"tavg\": (tmin[:, i] + tmax[:, i]) / 2.0,\n",
    "            \"ndvi\": ndvi[:, i],\n",
    "        }\n",
    "\n",
    "        # Calculate derived columns\n",
    "        df_data[\"soil_water\"] = swim_input.properties.awc[i] - output.depl_root[:, i]\n",
    "\n",
    "        df = pd.DataFrame(df_data, index=dates)\n",
    "\n",
    "        swim_input.close()\n",
    "\n",
    "        # Load ETf observations from container for comparison\n",
    "        # These are the remote sensing observations (PT-JPL in this example)\n",
    "        etf_model = getattr(config, \"etf_target_model\", \"ptjpl\")\n",
    "        for mask in [\"inv_irr\", \"irr\"]:\n",
    "            etf_path = f\"remote_sensing/etf/landsat/{etf_model}/{mask}\"\n",
    "            try:\n",
    "                etf_df = container.query.dataframe(etf_path, fields=[selected_feature])\n",
    "                # Align with simulation dates\n",
    "                etf_series = etf_df[selected_feature].reindex(dates)\n",
    "                df[f\"etf_{mask}\"] = etf_series.values\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load ETf for {mask}: {e}\")\n",
    "                df[f\"etf_{mask}\"] = np.nan\n",
    "\n",
    "    finally:\n",
    "        # Clean up temp files\n",
    "        if os.path.exists(temp_h5_path):\n",
    "            os.remove(temp_h5_path)\n",
    "        if calibrated_params_path and os.path.exists(calibrated_params_path):\n",
    "            os.remove(calibrated_params_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExecution time: {end_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "    df.to_csv(output_csv)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _convert_forecast_params_to_json(forecast_params, out_dir: str) -> str:\n",
    "    \"\"\"Convert forecast_parameters Series to JSON format for build_swim_input.\n",
    "\n",
    "    The forecast_parameters Series has index like 'kc_max_FID1', 'ndvi_k_FID1', etc.\n",
    "    We convert to: {FID1: {kc_max: val, ndvi_k: val, ...}, ...}\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import tempfile\n",
    "\n",
    "    params_by_fid = {}\n",
    "    for param_name in forecast_params.index:\n",
    "        # Parse param name: expect format like 'kc_max_FID1' or 'ndvi_k_FID1'\n",
    "        parts = param_name.rsplit(\"_\", 1)\n",
    "        if len(parts) == 2:\n",
    "            base_param, fid = parts\n",
    "            if fid not in params_by_fid:\n",
    "                params_by_fid[fid] = {}\n",
    "            params_by_fid[fid][base_param] = float(forecast_params[param_name])\n",
    "\n",
    "    # Write to temp JSON file\n",
    "    fd, json_path = tempfile.mkstemp(suffix=\".json\", prefix=\"calib_params_\", dir=out_dir)\n",
    "    os.close(fd)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(params_by_fid, f)\n",
    "\n",
    "    return json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-model",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:06.593216325Z",
     "start_time": "2026-01-27T19:16:05.278164057Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_feature = \"US-FPe\"\n",
    "out_csv = os.path.join(project_ws, f\"combined_output_{selected_feature}_calibrated.csv\")\n",
    "\n",
    "# Open container for model run\n",
    "container_path = os.path.join(data, \"2_Fort_Peck.swim\")\n",
    "container = SwimContainer.open(container_path, mode=\"r\")\n",
    "\n",
    "# Load config with forecast=True to get calibrated parameter path\n",
    "config = ProjectConfig()\n",
    "config.read_config(config_file, project_ws, forecast=True)\n",
    "\n",
    "try:\n",
    "    # Run with forecast=True to use calibrated parameters\n",
    "    df = run_fields(\n",
    "        config, container, selected_feature=selected_feature, output_csv=out_csv, forecast=True\n",
    "    )\n",
    "finally:\n",
    "    container.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 3. Visualize Calibrated Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-year",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:09.215708238Z",
     "start_time": "2026-01-27T19:16:06.654944585Z"
    }
   },
   "outputs": [],
   "source": [
    "ydf = df.loc[\"2004-01-01\":\"2004-12-31\"]\n",
    "print(f\"Total irrigation: {ydf.irrigation.sum():.1f} mm\")\n",
    "print(f\"Total ET: {ydf.et_act.sum():.1f} mm\")\n",
    "print(f\"Total precipitation: {ydf.ppt.sum():.1f} mm\")\n",
    "\n",
    "plot_swim_timeseries(\n",
    "    ydf,\n",
    "    [\"et_act\", \"etref\", \"rain\", \"melt\", \"irrigation\"],\n",
    "    start=\"2004-01-01\",\n",
    "    end=\"2004-12-31\",\n",
    "    png_dir=\"et_calibrated.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-header",
   "metadata": {},
   "source": [
    "## 4. Compare with Flux Tower Observations\n",
    "\n",
    "We show two comparisons:\n",
    "- **Full time series**: SWIM (daily) vs PT-JPL (interpolated between Landsat dates) on all flux tower days\n",
    "- **Capture dates only**: Both methods compared only on Landsat overpass dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:09.349798879Z",
     "start_time": "2026-01-27T19:16:09.281836697Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_et_estimates(combined_output_path, flux_data_path, irr=False, label=\"\"):\n",
    "    \"\"\"Compare model ET and PT-JPL ET against flux tower observations.\n",
    "\n",
    "    Returns two comparison DataFrames:\n",
    "    1. Capture dates only: Both methods on Landsat overpass dates only\n",
    "    2. Full time series: SWIM daily, PT-JPL interpolated, on all flux tower days\n",
    "    \"\"\"\n",
    "    flux_data = pd.read_csv(flux_data_path, index_col=\"date\", parse_dates=True)\n",
    "    flux_et = flux_data[\"ET_corr\"]  # Energy-balance-corrected ET from flux tower (mm/day)\n",
    "\n",
    "    output = pd.read_csv(combined_output_path, index_col=0)\n",
    "    output.index = pd.to_datetime(output.index)\n",
    "\n",
    "    etf_col = \"etf_irr\" if irr else \"etf_inv_irr\"\n",
    "    ptjpl_et_sparse = output[etf_col] * output[\"etref\"]\n",
    "    ptjpl_et_interp = ptjpl_et_sparse.interpolate(method=\"linear\")\n",
    "    n_ptjpl_obs = ptjpl_et_sparse.notna().sum()\n",
    "\n",
    "    # CAPTURE DATES ONLY comparison (PT-JPL sparse)\n",
    "    capture_df = pd.DataFrame(\n",
    "        {\"swim_et\": output[\"et_act\"], \"ptjpl_et\": ptjpl_et_sparse, \"flux_et\": flux_et}\n",
    "    ).dropna()\n",
    "\n",
    "    # FULL TIME SERIES comparison (PT-JPL interpolated)\n",
    "    full_df = pd.DataFrame(\n",
    "        {\"swim_et\": output[\"et_act\"], \"ptjpl_et\": ptjpl_et_interp, \"flux_et\": flux_et}\n",
    "    ).dropna()\n",
    "\n",
    "    def calc_metrics(df, col1, col2):\n",
    "        r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "        r2 = r2_score(df[col1], df[col2])\n",
    "        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "        bias = (df[col2] - df[col1]).mean()\n",
    "        return r2, r, rmse, bias\n",
    "\n",
    "    # Capture dates metrics\n",
    "    r2_swim_cap, r_swim_cap, rmse_swim_cap, bias_swim_cap = calc_metrics(\n",
    "        capture_df, \"flux_et\", \"swim_et\"\n",
    "    )\n",
    "    r2_ptjpl_cap, r_ptjpl_cap, rmse_ptjpl_cap, bias_ptjpl_cap = calc_metrics(\n",
    "        capture_df, \"flux_et\", \"ptjpl_et\"\n",
    "    )\n",
    "\n",
    "    # Full time series metrics\n",
    "    r2_swim_full, r_swim_full, rmse_swim_full, bias_swim_full = calc_metrics(\n",
    "        full_df, \"flux_et\", \"swim_et\"\n",
    "    )\n",
    "    r2_ptjpl_full, r_ptjpl_full, rmse_ptjpl_full, bias_ptjpl_full = calc_metrics(\n",
    "        full_df, \"flux_et\", \"ptjpl_et\"\n",
    "    )\n",
    "\n",
    "    if label:\n",
    "        print(f\"\\n{label}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    print(f\"CAPTURE DATES ONLY ({len(capture_df)} Landsat overpass dates)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'PT-JPL ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_cap:>12.3f} {r2_ptjpl_cap:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_cap:>12.3f} {r_ptjpl_cap:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_cap:>12.3f} {bias_ptjpl_cap:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_cap:>12.3f} {rmse_ptjpl_cap:>12.3f}\")\n",
    "\n",
    "    print()\n",
    "    print(f\"FULL TIME SERIES ({len(full_df)} days, PT-JPL interpolated from {n_ptjpl_obs} obs)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'PT-JPL ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_full:>12.3f} {r2_ptjpl_full:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_full:>12.3f} {r_ptjpl_full:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_full:>12.3f} {bias_ptjpl_full:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_full:>12.3f} {rmse_ptjpl_full:>12.3f}\")\n",
    "\n",
    "    # Return metrics for summary comparison\n",
    "    return (\n",
    "        full_df,\n",
    "        capture_df,\n",
    "        {\n",
    "            \"capture\": {\n",
    "                \"r2\": r2_swim_cap,\n",
    "                \"r\": r_swim_cap,\n",
    "                \"rmse\": rmse_swim_cap,\n",
    "                \"bias\": bias_swim_cap,\n",
    "            },\n",
    "            \"full\": {\n",
    "                \"r2\": r2_swim_full,\n",
    "                \"r\": r_swim_full,\n",
    "                \"rmse\": rmse_swim_full,\n",
    "                \"bias\": bias_swim_full,\n",
    "            },\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:09.464695780Z",
     "start_time": "2026-01-27T19:16:09.351333463Z"
    }
   },
   "outputs": [],
   "source": [
    "flux_data = os.path.join(data, \"US-FPe_daily_data.csv\")\n",
    "full_df, capture_df, metrics_cal = compare_et_estimates(out_csv, flux_data, label=\"CALIBRATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-uncal-header",
   "metadata": {},
   "source": [
    "### Compare with Uncalibrated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-both",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:09.593915255Z",
     "start_time": "2026-01-27T19:16:09.466080319Z"
    }
   },
   "outputs": [],
   "source": [
    "uncal_csv = os.path.join(project_ws, f\"combined_output_{selected_feature}_uncalibrated.csv\")\n",
    "\n",
    "if os.path.exists(uncal_csv):\n",
    "    _, _, metrics_uncal = compare_et_estimates(uncal_csv, flux_data, label=\"UNCALIBRATED\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CALIBRATION IMPROVEMENT SUMMARY (SWIM vs Flux)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nCapture Dates Only:\")\n",
    "    print(\n",
    "        f\"  R²:           {metrics_uncal['capture']['r2']:.3f} -> {metrics_cal['capture']['r2']:.3f}  ({metrics_cal['capture']['r2'] - metrics_uncal['capture']['r2']:+.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Pearson r:    {metrics_uncal['capture']['r']:.3f} -> {metrics_cal['capture']['r']:.3f}  ({metrics_cal['capture']['r'] - metrics_uncal['capture']['r']:+.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Bias (mm):    {metrics_uncal['capture']['bias']:+.3f} -> {metrics_cal['capture']['bias']:+.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  RMSE (mm):    {metrics_uncal['capture']['rmse']:.3f} -> {metrics_cal['capture']['rmse']:.3f}  ({metrics_cal['capture']['rmse'] - metrics_uncal['capture']['rmse']:+.3f})\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nFull Time Series:\")\n",
    "    print(\n",
    "        f\"  R²:           {metrics_uncal['full']['r2']:.3f} -> {metrics_cal['full']['r2']:.3f}  ({metrics_cal['full']['r2'] - metrics_uncal['full']['r2']:+.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Pearson r:    {metrics_uncal['full']['r']:.3f} -> {metrics_cal['full']['r']:.3f}  ({metrics_cal['full']['r'] - metrics_uncal['full']['r']:+.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Bias (mm):    {metrics_uncal['full']['bias']:+.3f} -> {metrics_cal['full']['bias']:+.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  RMSE (mm):    {metrics_uncal['full']['rmse']:.3f} -> {metrics_cal['full']['rmse']:.3f}  ({metrics_cal['full']['rmse'] - metrics_uncal['full']['rmse']:+.3f})\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Uncalibrated output not found. Run notebook 01 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-scatter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:10.634587511Z",
     "start_time": "2026-01-27T19:16:09.595112919Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatter plots for both comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "\n",
    "# Helper function to calculate metrics\n",
    "def calc_metrics(df, col1, col2):\n",
    "    r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "    r2 = r2_score(df[col1], df[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "    return r2, r, rmse\n",
    "\n",
    "\n",
    "# Determine axis limits\n",
    "max_et = max(full_df[\"flux_et\"].max(), full_df[\"swim_et\"].max(), full_df[\"ptjpl_et\"].max()) * 1.1\n",
    "\n",
    "# TOP ROW: Capture dates only\n",
    "r2_swim_cap, r_swim_cap, rmse_swim_cap = calc_metrics(capture_df, \"flux_et\", \"swim_et\")\n",
    "r2_ptjpl_cap, r_ptjpl_cap, rmse_ptjpl_cap = calc_metrics(capture_df, \"flux_et\", \"ptjpl_et\")\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(capture_df[\"flux_et\"], capture_df[\"swim_et\"], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], \"r--\", label=\"1:1 line\")\n",
    "ax.set_xlabel(\"Flux ET (mm/day)\")\n",
    "ax.set_ylabel(\"SWIM ET (mm/day)\")\n",
    "ax.set_title(\n",
    "    f\"SWIM vs Flux - Capture Dates (n={len(capture_df)})\\n\"\n",
    "    f\"R² = {r2_swim_cap:.3f}, r = {r_swim_cap:.3f}, RMSE = {rmse_swim_cap:.2f} mm\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(capture_df[\"flux_et\"], capture_df[\"ptjpl_et\"], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], \"r--\", label=\"1:1 line\")\n",
    "ax.set_xlabel(\"Flux ET (mm/day)\")\n",
    "ax.set_ylabel(\"PT-JPL ET (mm/day)\")\n",
    "ax.set_title(\n",
    "    f\"PT-JPL vs Flux - Capture Dates (n={len(capture_df)})\\n\"\n",
    "    f\"R² = {r2_ptjpl_cap:.3f}, r = {r_ptjpl_cap:.3f}, RMSE = {rmse_ptjpl_cap:.2f} mm\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "# BOTTOM ROW: Full time series comparison\n",
    "r2_swim, r_swim, rmse_swim = calc_metrics(full_df, \"flux_et\", \"swim_et\")\n",
    "r2_ptjpl, r_ptjpl, rmse_ptjpl = calc_metrics(full_df, \"flux_et\", \"ptjpl_et\")\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(full_df[\"flux_et\"], full_df[\"swim_et\"], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], \"r--\", label=\"1:1 line\")\n",
    "ax.set_xlabel(\"Flux ET (mm/day)\")\n",
    "ax.set_ylabel(\"SWIM ET (mm/day)\")\n",
    "ax.set_title(\n",
    "    f\"SWIM vs Flux - Full Series (n={len(full_df)})\\n\"\n",
    "    f\"R² = {r2_swim:.3f}, r = {r_swim:.3f}, RMSE = {rmse_swim:.2f} mm\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(full_df[\"flux_et\"], full_df[\"ptjpl_et\"], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], \"r--\", label=\"1:1 line\")\n",
    "ax.set_xlabel(\"Flux ET (mm/day)\")\n",
    "ax.set_ylabel(\"PT-JPL ET (mm/day)\")\n",
    "ax.set_title(\n",
    "    f\"PT-JPL vs Flux - Full Series, interpolated (n={len(full_df)})\\n\"\n",
    "    f\"R² = {r2_ptjpl:.3f}, r = {r_ptjpl:.3f}, RMSE = {rmse_ptjpl:.2f} mm\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_scatter_calibrated.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uytgogfp5qi",
   "metadata": {},
   "source": [
    "## 5. Mass Balance Check\n",
    "\n",
    "Verify water conservation across the full simulation period. The water balance equation:\n",
    "\n",
    "**Starting Water + Inputs - Outputs = Ending Water**\n",
    "\n",
    "Where:\n",
    "- **Starting Water** = Initial Soil Water + Initial SWE\n",
    "- **Inputs** = Precipitation + Irrigation  \n",
    "- **Outputs** = ET + Deep Percolation + Runoff\n",
    "- **Ending Water** = Final Soil Water + Final SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sp2rve1wnfp",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:10.767624373Z",
     "start_time": "2026-01-27T19:16:10.705698019Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mass_balance(df):\n",
    "    \"\"\"Compute full-run mass balance and display as a table.\n",
    "\n",
    "    Water Balance: Starting Water + Inputs - Outputs = Ending Water\n",
    "\n",
    "    Components:\n",
    "    - Starting Water: Initial soil water + Initial SWE\n",
    "    - Inputs: Total precipitation + Total irrigation\n",
    "    - Outputs: Total ET + Total deep percolation + Total runoff\n",
    "    - Ending Water: Final soil water + Final SWE\n",
    "    \"\"\"\n",
    "    # Starting conditions (first day values)\n",
    "    start_soil_water = df[\"soil_water\"].iloc[0]\n",
    "    start_swe = df[\"swe\"].iloc[0]\n",
    "    start_water = start_soil_water + start_swe\n",
    "\n",
    "    # Ending conditions (last day values)\n",
    "    end_soil_water = df[\"soil_water\"].iloc[-1]\n",
    "    end_swe = df[\"swe\"].iloc[-1]\n",
    "    end_water = end_soil_water + end_swe\n",
    "\n",
    "    # Inputs (sums over full period)\n",
    "    total_precip = df[\"ppt\"].sum()\n",
    "    total_irrigation = df[\"irrigation\"].sum()\n",
    "    total_inputs = total_precip + total_irrigation\n",
    "\n",
    "    # Outputs (sums over full period)\n",
    "    total_et = df[\"et_act\"].sum()\n",
    "    total_dperc = df[\"dperc\"].sum()\n",
    "    total_runoff = df[\"runoff\"].sum()\n",
    "    total_outputs = total_et + total_dperc + total_runoff\n",
    "\n",
    "    # Mass balance check\n",
    "    # Expected: Start + Inputs - Outputs = End\n",
    "    # Residual: (Start + Inputs - Outputs) - End = 0 if balanced\n",
    "    expected_end = start_water + total_inputs - total_outputs\n",
    "    residual = expected_end - end_water\n",
    "    pct_error = (residual / total_inputs) * 100 if total_inputs > 0 else 0\n",
    "\n",
    "    # Create summary table\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MASS BALANCE SUMMARY\")\n",
    "    print(\n",
    "        f\"Period: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')} ({len(df)} days)\"\n",
    "    )\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n--- STARTING WATER ---\")\n",
    "    print(f\"  Initial Soil Water:     {start_soil_water:>10.2f} mm\")\n",
    "    print(f\"  Initial SWE:            {start_swe:>10.2f} mm\")\n",
    "    print(f\"  TOTAL STARTING:         {start_water:>10.2f} mm\")\n",
    "\n",
    "    print(\"\\n--- INPUTS (+) ---\")\n",
    "    print(f\"  Precipitation:          {total_precip:>10.2f} mm\")\n",
    "    print(f\"  Irrigation:             {total_irrigation:>10.2f} mm\")\n",
    "    print(f\"  TOTAL INPUTS:           {total_inputs:>10.2f} mm\")\n",
    "\n",
    "    print(\"\\n--- OUTPUTS (-) ---\")\n",
    "    print(f\"  Evapotranspiration:     {total_et:>10.2f} mm\")\n",
    "    print(f\"  Deep Percolation:       {total_dperc:>10.2f} mm\")\n",
    "    print(f\"  Runoff:                 {total_runoff:>10.2f} mm\")\n",
    "    print(f\"  TOTAL OUTPUTS:          {total_outputs:>10.2f} mm\")\n",
    "\n",
    "    print(\"\\n--- ENDING WATER ---\")\n",
    "    print(f\"  Final Soil Water:       {end_soil_water:>10.2f} mm\")\n",
    "    print(f\"  Final SWE:              {end_swe:>10.2f} mm\")\n",
    "    print(f\"  TOTAL ENDING:           {end_water:>10.2f} mm\")\n",
    "\n",
    "    print(\"\\n--- BALANCE CHECK ---\")\n",
    "    print(f\"  Expected End (Start + In - Out):  {expected_end:>10.2f} mm\")\n",
    "    print(f\"  Actual End:                       {end_water:>10.2f} mm\")\n",
    "    print(f\"  Residual (Error):                 {residual:>10.2f} mm\")\n",
    "    print(f\"  Error as % of Inputs:             {pct_error:>10.4f} %\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if abs(pct_error) < 0.01:\n",
    "        print(\"MASS BALANCE: CLOSED (error < 0.01%)\")\n",
    "    elif abs(pct_error) < 1.0:\n",
    "        print(\"MASS BALANCE: ACCEPTABLE (error < 1%)\")\n",
    "    else:\n",
    "        print(f\"MASS BALANCE: WARNING - {abs(pct_error):.2f}% error\")\n",
    "\n",
    "    return {\n",
    "        \"start_soil_water\": start_soil_water,\n",
    "        \"start_swe\": start_swe,\n",
    "        \"end_soil_water\": end_soil_water,\n",
    "        \"end_swe\": end_swe,\n",
    "        \"precip\": total_precip,\n",
    "        \"irrigation\": total_irrigation,\n",
    "        \"et\": total_et,\n",
    "        \"dperc\": total_dperc,\n",
    "        \"runoff\": total_runoff,\n",
    "        \"residual\": residual,\n",
    "        \"pct_error\": pct_error,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run mass balance check\n",
    "balance = compute_mass_balance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "Calibration improved model performance:\n",
    "\n",
    "- **Correlation improved** from uncalibrated to calibrated, while the metric improvement is modest, the tightening of the values around the 1:1 line is clear in comparison\n",
    "- **SWIM outperforms PT-JPL** in both correlation and bias when comparing actual ET (mm/day)\n",
    "- **PT-JPL performance is worse** during days without a satellite overpass\n",
    "- **SWIM suffers no performance penalty** on between-capture dates\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "**The key insight is that we can mine the deep remote sensing-based ET record, but rather than driving the model with remote sensing ET directly, we drive the calibration with it.**\n",
    "\n",
    "The model has access to:\n",
    "1. Daily meteorological data (not just satellite overpass days)\n",
    "2. Physically-based soil water balance constraints (if the soil is dry, ET slows)\n",
    "3. Flexibility to tune parameters using the remote sensing record\n",
    "\n",
    "This combination gives SWIM a more grounded perspective on daily fluxes than remote sensing alone, resulting in better ET estimates. The fact that SWIM is tracking the available soil moisture on a daily basis allows it to respond to conditions through time, rather than simply interpolating the underlying ET driver (ETf) between capture dates. If we simply drove a naive soil water balance with remote sensing data, we'd fall victim to the same degredation in performance between satellite overpasses. When we use the capture date information to refine an already-good hydrological model, we enable it to find a parameterization that can match or exceed capture date remote sensing accuracy and also make good estimates on days with no remote sensing data.\n",
    "\n",
    "Next, we will use a more sophisticaed approach in an irrigated setting, a combination of the open-source OpenET ensemble members.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2602f7-705d-4267-999e-fd92cc3dcf0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:16:10.816611587Z",
     "start_time": "2026-01-27T19:16:10.769017333Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
