{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Calibration Tutorial - Crane, OR - Irrigated Flux Plot\n",
    "\n",
    "## Step 1: Uncalibrated Model Run\n",
    "\n",
    "This tutorial focuses on calibrating SWIM-RS for a single irrigated alfalfa plot at the S2 flux station in Crane, Oregon. Unlike the unirrigated Fort Peck example, this site is actively irrigated.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading pre-built model input data\n",
    "2. Running the uncalibrated SWIM model\n",
    "3. Comparing model output with OpenET ensemble (PT-JPL, SIMS, SSEBop, geeSEBAL)\n",
    "4. Validation against flux tower observations using multiple metrics (R², r, RMSE, bias)\n",
    "\n",
    "**Input Data:** The `data/prepped_input.json` file contains pre-computed input data.\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "The full data workflow uses two scripts and can be re-run if needed:\n",
    "\n",
    "1. **`extract_data.py`** - Extracts raw data from Earth Engine and GridMET to CSV/parquet files\n",
    "2. **`build_inputs.py`** - Processes extracted data through SwimContainer and exports to `prepped_input.json`\n",
    "\n",
    "To reproduce the input data from scratch:\n",
    "\n",
    "```bash\n",
    "cd data\n",
    "python extract_data.py    # Extract from EE/GridMET (requires authentication)\n",
    "python build_inputs.py    # Build container and export JSON\n",
    "```\n",
    "\n",
    "See `data/extract_data.py` for extraction options and `data/build_inputs.py` for container workflow details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.061529210Z",
     "start_time": "2026-01-15T03:04:25.922073628Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.swim.sampleplots import SamplePlots\n",
    "from swimrs.model.obs_field_cycle import field_day_loop\n",
    "\n",
    "from swimrs.viz.swim_timeseries import plot_swim_timeseries\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "\n",
    "Define paths and unzip pre-built data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-paths",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.119644800Z",
     "start_time": "2026-01-15T03:04:26.063862452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project workspace: /home/dgketchum/code/swim-rs/examples/3_Crane\n",
      "Config file: /home/dgketchum/code/swim-rs/examples/3_Crane/3_Crane.toml\n",
      "Input data: /home/dgketchum/code/swim-rs/examples/3_Crane/data/prepped_input.json\n"
     ]
    }
   ],
   "source": [
    "project_ws = os.path.abspath('.')\n",
    "data = os.path.join(project_ws, 'data')\n",
    "\n",
    "config_file = os.path.join(project_ws, '3_Crane.toml')\n",
    "prepped_input = os.path.join(data, 'prepped_input.json')\n",
    "\n",
    "# Unzip data files if they haven't been extracted\n",
    "prepped_zip = os.path.join(data, 'prepped_input.zip')\n",
    "\n",
    "if os.path.exists(prepped_zip) and not os.path.exists(prepped_input):\n",
    "    print(\"Extracting prepped_input.zip...\")\n",
    "    with zipfile.ZipFile(prepped_zip, 'r') as z:\n",
    "        z.extractall(data)\n",
    "\n",
    "# Unzip flux tower data if needed\n",
    "flux_zip = os.path.join(data, 'S2_daily_data.zip')\n",
    "flux_csv = os.path.join(data, 'S2_daily_data.csv')\n",
    "\n",
    "if os.path.exists(flux_zip) and not os.path.exists(flux_csv):\n",
    "    print(\"Extracting S2_daily_data.zip...\")\n",
    "    with zipfile.ZipFile(flux_zip, 'r') as z:\n",
    "        z.extractall(data)\n",
    "\n",
    "print(f\"Project workspace: {project_ws}\")\n",
    "print(f\"Config file: {config_file}\")\n",
    "print(f\"Input data: {prepped_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-load-config",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.172785923Z",
     "start_time": "2026-01-15T03:04:26.121223144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the project configuration\n",
    "config = ProjectConfig()\n",
    "config.read_config(config_file, project_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1mbfjk45fs",
   "metadata": {},
   "source": [
    "### Initial Parameter Values\n",
    "\n",
    "The model will run with the following default parameter values and bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bprovbbl88t",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.729392041Z",
     "start_time": "2026-01-15T03:04:26.174421345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default Python script at: /home/dgketchum/code/swim-rs/src/swimrs/calibrate/custom_forward_run.py\n",
      "================================================================================\n",
      "INITIAL PARAMETER VALUES AND BOUNDS\n",
      "================================================================================\n",
      "Parameter         Initial      Lower      Upper      Std  Description\n",
      "--------------------------------------------------------------------------------\n",
      "aw                   auto     100.00     400.00    50.00  Available water capacity (mm)\n",
      "ks_alpha             0.50       0.01       1.00     0.15  Soil evap stress damping\n",
      "kr_alpha             0.50       0.01       1.00     0.15  Root zone stress damping\n",
      "ndvi_k               7.00       4.00      10.00     0.75  NDVI-Kcb slope\n",
      "ndvi_0               0.40       0.10       0.70     0.25  NDVI-Kcb intercept\n",
      "mad                  auto       0.01       0.90     0.15  Management allowable depletion\n",
      "swe_alpha            0.30      -0.50       1.00     0.20  Snow melt temp coefficient\n",
      "swe_beta             1.50       0.50       2.50     0.30  Snow melt rate coefficient\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgketchum/miniconda/envs/swim/lib/python3.11/site-packages/pyemu/__init__.py:37: UserWarning: Failed to import legacy module. May impact ability to access older methods.ModuleNotFoundError No module named 'flopy'\n"
     ]
    }
   ],
   "source": [
    "from swimrs.calibrate.pest_builder import PestBuilder\n",
    "\n",
    "def show_parameter_table(config):\n",
    "    \"\"\"Display parameter bounds and initial values from PestBuilder.\"\"\"\n",
    "    builder = PestBuilder(config)\n",
    "    params = builder.initial_parameter_dict()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"INITIAL PARAMETER VALUES AND BOUNDS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Parameter':<12} {'Initial':>12} {'Lower':>10} {'Upper':>10} {'Std':>8}  Description\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    descriptions = {\n",
    "        'aw': 'Available water capacity (mm)',\n",
    "        'ks_alpha': 'Soil evap stress damping',\n",
    "        'kr_alpha': 'Root zone stress damping', \n",
    "        'ndvi_k': 'NDVI-Kcb slope',\n",
    "        'ndvi_0': 'NDVI-Kcb intercept',\n",
    "        'mad': 'Management allowable depletion',\n",
    "        'swe_alpha': 'Snow melt temp coefficient',\n",
    "        'swe_beta': 'Snow melt rate coefficient',\n",
    "    }\n",
    "    \n",
    "    for name, p in params.items():\n",
    "        init = p['initial_value']\n",
    "        if init is None:\n",
    "            init_str = 'auto'\n",
    "        elif isinstance(init, str):\n",
    "            init_str = init[:12]\n",
    "        else:\n",
    "            init_str = f\"{init:.2f}\"\n",
    "        print(f\"{name:<12} {init_str:>12} {p['lower_bound']:>10.2f} {p['upper_bound']:>10.2f} {p['std']:>8.2f}  {descriptions.get(name, '')}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "show_parameter_table(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-site-header",
   "metadata": {},
   "source": [
    "## 2. About the Study Site\n",
    "\n",
    "The S2 site is an irrigated alfalfa field in Crane, Oregon. According to IrrMapper data, this location has been irrigated since about 1996, making it a good test case for the irrigation scheduling component of SWIM-RS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-site-info",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.869248182Z",
     "start_time": "2026-01-15T03:04:26.801025285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: S2\n",
      "Location: Crane, Oregon\n",
      "Crop: Irrigated alfalfa\n",
      "Date range: 1987-01-01 00:00:00 to 2022-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "selected_feature = 'S2'\n",
    "\n",
    "print(f\"Site: {selected_feature}\")\n",
    "print(f\"Location: Crane, Oregon\")\n",
    "print(f\"Crop: Irrigated alfalfa\")\n",
    "print(f\"Date range: {config.start_dt} to {config.end_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-header",
   "metadata": {},
   "source": [
    "## 3. Run the Uncalibrated Model\n",
    "\n",
    "We define a helper function to run the SWIM model and capture its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-run-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:26.927642439Z",
     "start_time": "2026-01-15T03:04:26.871064773Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_fields(ini_path, project_ws, selected_feature, output_csv, forecast=False):\n",
    "    \"\"\"Run SWIM model and save combined input/output to CSV.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = ProjectConfig()\n",
    "    config.read_config(ini_path, project_ws, forecast=forecast)\n",
    "\n",
    "    fields = SamplePlots()\n",
    "    fields.initialize_plot_data(config)\n",
    "    fields.output = field_day_loop(config, fields, debug_flag=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'\\nExecution time: {end_time - start_time:.2f} seconds\\n')\n",
    "\n",
    "    out_df = fields.output[selected_feature].copy()\n",
    "    in_df = fields.input_to_dataframe(selected_feature)\n",
    "    \n",
    "    # Drop columns from input that already exist in output to avoid duplicates\n",
    "    overlap_cols = out_df.columns.intersection(in_df.columns)\n",
    "    if len(overlap_cols) > 0:\n",
    "        in_df = in_df.drop(columns=overlap_cols)\n",
    "    \n",
    "    df = pd.concat([out_df, in_df], axis=1, ignore_index=False)\n",
    "    \n",
    "    # Cut out nan output from before the start of the model run\n",
    "    df = df.loc[config.start_dt:config.end_dt]\n",
    "    \n",
    "    df.to_csv(output_csv)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-run-model",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:28.440830019Z",
     "start_time": "2026-01-15T03:04:26.928655500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING PARAMETER DEFAULTS\n",
      "\n",
      "Execution time: 1.79 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_feature = 'S2'\n",
    "out_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_uncalibrated.csv')\n",
    "\n",
    "df = run_fields(config_file, project_ws, selected_feature=selected_feature, output_csv=out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-inspect-output",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:28.549598157Z",
     "start_time": "2026-01-15T03:04:28.492768156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1826, 73)\n",
      "Date range: 2003-01-01 00:00:00 to 2007-12-31 00:00:00\n",
      "\n",
      "Key output columns:\n",
      "  et_act: mean=1.653, max=6.797\n",
      "  etref: mean=2.884, max=8.528\n",
      "  kc_act: mean=0.589, max=0.963\n",
      "  kc_bas: mean=0.426, max=0.967\n",
      "  ks: mean=0.989, max=1.000\n",
      "  ke: mean=0.337, max=0.698\n",
      "  melt: mean=0.140, max=9.366\n",
      "  rain: mean=0.586, max=25.900\n",
      "  depl_root: mean=37.361, max=157.927\n",
      "  swe: mean=1.541, max=59.716\n",
      "  ppt: mean=0.736, max=25.900\n",
      "  irrigation: mean=1.017, max=25.400\n",
      "  soil_water: mean=252.309, max=336.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(f\"\\nKey output columns:\")\n",
    "key_cols = ['et_act', 'etref', 'kc_act', 'kc_bas', 'ks', 'ke', 'melt', 'rain', \n",
    "            'depl_root', 'swe', 'ppt', 'irrigation', 'soil_water']\n",
    "for col in key_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"  {col}: mean={df[col].mean():.3f}, max={df[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 4. Visualize Model Output\n",
    "\n",
    "Let's examine a single year (2004) to see the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-viz-year",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:31.164366836Z",
     "start_time": "2026-01-15T03:04:28.551189475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total irrigation: 321.2 mm\n",
      "Total ET: 553.4 mm\n",
      "Total precipitation: 267.7 mm\n",
      "et_uncalibrated.png\n"
     ]
    }
   ],
   "source": [
    "ydf = df.loc['2004-01-01': '2004-12-31']\n",
    "print(f'Total irrigation: {ydf.irrigation.sum():.1f} mm')\n",
    "print(f'Total ET: {ydf.et_act.sum():.1f} mm')\n",
    "print(f'Total precipitation: {ydf.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_swim_timeseries(ydf, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], \n",
    "                     start='2004-01-01', end='2004-12-31', png_dir='et_uncalibrated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-header",
   "metadata": {},
   "source": [
    "## 5. Compare with Flux Tower Observations\n",
    "\n",
    "We compare three estimates of actual ET (mm/day):\n",
    "\n",
    "1. **SWIM ET**: Model-estimated actual evapotranspiration (daily)\n",
    "2. **OpenET Ensemble ET**: Remote sensing retrievals from OpenET (PT-JPL, SIMS, SSEBop, geeSEBAL) averaged together (ETf × ETo)\n",
    "3. **Flux ET**: Independent observations from the S2 eddy covariance tower (Volk et al.)\n",
    "\n",
    "We show two comparisons:\n",
    "- **Capture dates only**: Both methods compared only on Landsat overpass dates\n",
    "- **Full time series**: SWIM (daily) vs OpenET (interpolated between Landsat dates) on all flux tower days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-compare-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:31.285943380Z",
     "start_time": "2026-01-15T03:04:31.227172993Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_et_estimates(combined_output_path, flux_data_path, irr=True):\n",
    "    \"\"\"Compare model ET and OpenET ensemble ET against flux tower observations.\n",
    "    \n",
    "    Returns two comparison DataFrames:\n",
    "    1. Capture dates only: Both methods on Landsat overpass dates only\n",
    "    2. Full time series: SWIM daily, OpenET interpolated, on all flux tower days\n",
    "    \n",
    "    Reports R², Pearson r, bias, and RMSE for each comparison.\n",
    "    \"\"\"\n",
    "    flux_data = pd.read_csv(flux_data_path, index_col='date', parse_dates=True)\n",
    "    flux_et = flux_data['ET']  # Actual ET from flux tower (mm/day)\n",
    "\n",
    "    output = pd.read_csv(combined_output_path, index_col=0)\n",
    "    output.index = pd.to_datetime(output.index)\n",
    "\n",
    "    # Determine suffix based on irrigation mask\n",
    "    mask_suffix = 'irr' if irr else 'inv_irr'\n",
    "    \n",
    "    # OpenET ensemble models - compute mean ETf across available models\n",
    "    ensemble_models = ['ptjpl', 'sims', 'ssebop', 'geesebal']\n",
    "    etf_cols = []\n",
    "    for model in ensemble_models:\n",
    "        col_name = f'etf_{model}_{mask_suffix}'\n",
    "        if col_name in output.columns:\n",
    "            etf_cols.append(col_name)\n",
    "    \n",
    "    # Fallback to single SSEBop if ensemble columns not available\n",
    "    if not etf_cols:\n",
    "        etf_col = f'etf_{mask_suffix}'\n",
    "        if etf_col in output.columns:\n",
    "            etf_cols = [etf_col]\n",
    "            print(f\"Using single ETf column: {etf_col}\")\n",
    "        else:\n",
    "            print(f\"Warning: No ETf columns found for mask '{mask_suffix}'\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using ETf columns: {etf_cols}\")\n",
    "    \n",
    "    # Compute ensemble mean ETf (ignoring NaN)\n",
    "    ensemble_etf = output[etf_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Calculate actual ET from OpenET ensemble: ETf × ETo (sparse, only on Landsat dates)\n",
    "    openet_et_sparse = ensemble_etf * output['etref']\n",
    "    \n",
    "    # Linear interpolation of OpenET to get daily values\n",
    "    openet_et_interp = openet_et_sparse.interpolate(method='linear')\n",
    "    \n",
    "    # Count original OpenET observations\n",
    "    n_openet_obs = openet_et_sparse.notna().sum()\n",
    "\n",
    "    # CAPTURE DATES ONLY comparison (OpenET sparse)\n",
    "    capture_df = pd.DataFrame({\n",
    "        'swim_et': output['et_act'],\n",
    "        'openet_et': openet_et_sparse,\n",
    "        'flux_et': flux_et\n",
    "    }).dropna()\n",
    "\n",
    "    # FULL TIME SERIES comparison (OpenET interpolated)\n",
    "    full_df = pd.DataFrame({\n",
    "        'swim_et': output['et_act'],\n",
    "        'openet_et': openet_et_interp,\n",
    "        'flux_et': flux_et\n",
    "    }).dropna()\n",
    "\n",
    "    def calc_metrics(df, col1, col2):\n",
    "        r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "        r2 = r2_score(df[col1], df[col2])\n",
    "        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "        bias = (df[col2] - df[col1]).mean()\n",
    "        return r2, r, rmse, bias\n",
    "\n",
    "    # Capture dates metrics\n",
    "    r2_swim_cap, r_swim_cap, rmse_swim_cap, bias_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\n",
    "    r2_openet_cap, r_openet_cap, rmse_openet_cap, bias_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n",
    "\n",
    "    # Full time series metrics\n",
    "    r2_swim_full, r_swim_full, rmse_swim_full, bias_swim_full = calc_metrics(full_df, 'flux_et', 'swim_et')\n",
    "    r2_openet_full, r_openet_full, rmse_openet_full, bias_openet_full = calc_metrics(full_df, 'flux_et', 'openet_et')\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(f\"CAPTURE DATES ONLY ({len(capture_df)} Landsat overpass dates)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_cap:>12.3f} {r2_openet_cap:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_cap:>12.3f} {r_openet_cap:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_cap:>12.3f} {bias_openet_cap:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_cap:>12.3f} {rmse_openet_cap:>12.3f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "    print(f\"FULL TIME SERIES ({len(full_df)} days, OpenET interpolated from {n_openet_obs} obs)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_full:>12.3f} {r2_openet_full:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_full:>12.3f} {r_openet_full:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_full:>12.3f} {bias_openet_full:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_full:>12.3f} {rmse_openet_full:>12.3f}\")\n",
    "\n",
    "    return full_df, capture_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-compare-run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:31.356631166Z",
     "start_time": "2026-01-15T03:04:31.287456731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using single ETf column: etf_irr\n",
      "Using ETf columns: ['etf_irr']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` and `y` must have length at least 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_2467320/977194606.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use irrigated mask since this is an irrigated site\u001b[39;00m\n\u001b[32m      2\u001b[39m flux_data = os.path.join(data, \u001b[33m'S2_daily_data.csv'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m full_df, capture_df = compare_et_estimates(out_csv, flux_data, irr=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32m/tmp/ipykernel_2467320/1619256658.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(combined_output_path, flux_data_path, irr)\u001b[39m\n\u001b[32m     69\u001b[39m         bias = (df[col2] - df[col1]).mean()\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m r2, r, rmse, bias\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Capture dates metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     r2_swim_cap, r_swim_cap, rmse_swim_cap, bias_swim_cap = calc_metrics(capture_df, \u001b[33m'flux_et'\u001b[39m, \u001b[33m'swim_et'\u001b[39m)\n\u001b[32m     74\u001b[39m     r2_openet_cap, r_openet_cap, rmse_openet_cap, bias_openet_cap = calc_metrics(capture_df, \u001b[33m'flux_et'\u001b[39m, \u001b[33m'openet_et'\u001b[39m)\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# Full time series metrics\u001b[39;00m\n",
      "\u001b[32m/tmp/ipykernel_2467320/1619256658.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, col1, col2)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m calc_metrics(df, col1, col2):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         r, _ = stats.pearsonr(df[col1], df[col2])\n\u001b[32m     67\u001b[39m         r2 = r2_score(df[col1], df[col2])\n\u001b[32m     68\u001b[39m         rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n\u001b[32m     69\u001b[39m         bias = (df[col2] - df[col1]).mean()\n",
      "\u001b[32m~/miniconda/envs/swim/lib/python3.11/site-packages/scipy/stats/_stats_py.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, alternative, method, axis)\u001b[39m\n\u001b[32m   4685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n != y.shape[axis]:\n\u001b[32m   4686\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'`x` and `y` must have the same length along `axis`.'\u001b[39m)\n\u001b[32m   4687\u001b[39m \n\u001b[32m   4688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4689\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'`x` and `y` must have length at least 2.'\u001b[39m)\n\u001b[32m   4690\u001b[39m \n\u001b[32m   4691\u001b[39m     x = xp.moveaxis(x, axis, -\u001b[32m1\u001b[39m)\n\u001b[32m   4692\u001b[39m     y = xp.moveaxis(y, axis, -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: `x` and `y` must have length at least 2."
     ]
    }
   ],
   "source": [
    "# Use irrigated mask since this is an irrigated site\n",
    "flux_data = os.path.join(data, 'S2_daily_data.csv')\n",
    "full_df, capture_df = compare_et_estimates(out_csv, flux_data, irr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-scatter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:31.905154172Z",
     "start_time": "2026-01-15T03:04:31.358846346Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create 2x2 scatter plots for both comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Helper function to calculate metrics\n",
    "def calc_metrics(df, col1, col2):\n",
    "    r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "    r2 = r2_score(df[col1], df[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "    return r2, r, rmse\n",
    "\n",
    "# Determine axis limits\n",
    "max_et = max(full_df['flux_et'].max(), full_df['swim_et'].max(), \n",
    "             full_df['openet_et'].max()) * 1.1\n",
    "\n",
    "# TOP ROW: Capture dates only\n",
    "r2_swim_cap, r_swim_cap, rmse_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\n",
    "r2_openet_cap, r_openet_cap, rmse_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(capture_df['flux_et'], capture_df['swim_et'], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('SWIM ET (mm/day)')\n",
    "ax.set_title(f'SWIM vs Flux - Capture Dates (n={len(capture_df)})\\n'\n",
    "             f'R² = {r2_swim_cap:.3f}, r = {r_swim_cap:.3f}, RMSE = {rmse_swim_cap:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(capture_df['flux_et'], capture_df['openet_et'], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('OpenET Ensemble ET (mm/day)')\n",
    "ax.set_title(f'OpenET vs Flux - Capture Dates (n={len(capture_df)})\\n'\n",
    "             f'R² = {r2_openet_cap:.3f}, r = {r_openet_cap:.3f}, RMSE = {rmse_openet_cap:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "# BOTTOM ROW: Full time series comparison\n",
    "r2_swim, r_swim, rmse_swim = calc_metrics(full_df, 'flux_et', 'swim_et')\n",
    "r2_openet, r_openet, rmse_openet = calc_metrics(full_df, 'flux_et', 'openet_et')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(full_df['flux_et'], full_df['swim_et'], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('SWIM ET (mm/day)')\n",
    "ax.set_title(f'SWIM vs Flux - Full Series (n={len(full_df)})\\n'\n",
    "             f'R² = {r2_swim:.3f}, r = {r_swim:.3f}, RMSE = {rmse_swim:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(full_df['flux_et'], full_df['openet_et'], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('OpenET Ensemble ET (mm/day)')\n",
    "ax.set_title(f'OpenET vs Flux - Full Series, interpolated (n={len(full_df)})\\n'\n",
    "             f'R² = {r2_openet:.3f}, r = {r_openet:.3f}, RMSE = {rmse_openet:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_scatter_uncalibrated.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The uncalibrated model using default parameters shows the baseline performance before calibration. We compared both SWIM and the OpenET ensemble (PT-JPL, SIMS, SSEBop, geeSEBAL average) against independent flux tower observations.\n",
    "\n",
    "**Two comparison modes:**\n",
    "- **Capture dates**: Only Landsat overpass dates where we have satellite observations\n",
    "- **Full time series**: All flux tower days, with OpenET values interpolated between satellite dates\n",
    "\n",
    "**Key observations:**\n",
    "- The model isn't applying enough irrigation\n",
    "- The NDVI-to-Kcb relationship needs tuning for alfalfa\n",
    "- Soil parameters may not match the actual site conditions\n",
    "- OpenET ensemble provides robust remote sensing benchmark\n",
    "\n",
    "**Next step:** In notebook `02_calibration.ipynb`, we'll use PEST++ to calibrate the model parameters using SSEBop ETf and SNODAS SWE observations.\n",
    "\n",
    "**Key insight:** We're not using the flux data for calibration - it's only for validation. For calibration, we rely solely on widely-available remote sensing data (ETf and SNODAS SWE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m50ezgsmf4",
   "metadata": {},
   "source": [
    "## Optional: Query Data from SwimContainer\n",
    "\n",
    "If you've built the container using `build_inputs.py`, you can query ingested data directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1lxgaaliloa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:32.966548252Z",
     "start_time": "2026-01-15T03:04:31.975514193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query container data (optional - requires build_inputs.py to have been run)\n",
    "from swimrs.container import SwimContainer\n",
    "\n",
    "container_path = os.path.join(data, '3_Crane.swim')\n",
    "\n",
    "if os.path.exists(container_path):\n",
    "    container = SwimContainer.open(container_path, mode='r')\n",
    "    \n",
    "    # List available fields\n",
    "    print(f\"Fields in container: {container.field_uids}\")\n",
    "    \n",
    "    # Get all time series for a single field using field_timeseries\n",
    "    ts_df = container.query.field_timeseries('S2')\n",
    "    print(f\"\\nTime series shape: {ts_df.shape}\")\n",
    "    print(f\"Variables: {list(ts_df.columns)[:10]}...\")\n",
    "    \n",
    "    # Query specific data using dataframe with zarr paths\n",
    "    # Path structure: remote_sensing/{type}/{instrument}/{model}/{mask}\n",
    "    ndvi_df = container.query.dataframe(\"remote_sensing/ndvi/landsat/irr\", fields=['S2'])\n",
    "    print(f\"\\nNDVI observations: {ndvi_df.notna().sum().values[0]}\")\n",
    "    \n",
    "    etf_df = container.query.dataframe(\"remote_sensing/etf/landsat/ssebop/irr\", fields=['S2'])\n",
    "    print(f\"ETf observations: {etf_df.notna().sum().values[0]}\")\n",
    "    \n",
    "    # Show container status\n",
    "    print(\"\\n\" + container.query.status())\n",
    "    \n",
    "    container.close()\n",
    "else:\n",
    "    print(f\"Container not found at {container_path}\")\n",
    "    print(\"Run: cd data && python build_inputs.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
