{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": "# Calibration Tutorial - Crane, OR - Irrigated Flux Plot\n\n## Step 3: Running the Calibrated Model\n\nNow we evaluate whether calibration improved model performance by running in **forecast mode** with calibrated parameters.\n\nThis notebook:\n1. Visualizes how parameters evolved during calibration\n2. Runs the model with calibrated parameters\n3. Compares calibrated vs uncalibrated performance against flux tower observations\n4. Reports multiple metrics (R², Pearson r, RMSE, bias) for both capture dates and full time series"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nroot = os.path.abspath('../..')\nsys.path.append(root)\n\nfrom swimrs.swim.config import ProjectConfig\nfrom swimrs.swim.sampleplots import SamplePlots\nfrom swimrs.model.obs_field_cycle import field_day_loop\n\nfrom swimrs.viz.param_evolution import plot_parameter_histograms\nfrom swimrs.viz.swim_timeseries import plot_swim_timeseries\n\n%matplotlib inline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_ws = os.path.abspath('.')\n",
    "data = os.path.join(project_ws, 'data')\n",
    "pest_dir = os.path.join(project_ws, 'pest')\n",
    "\n",
    "config_file = os.path.join(project_ws, '3_Crane.toml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-param-header",
   "metadata": {},
   "source": [
    "## 1. Visualize Parameter Evolution\n",
    "\n",
    "Let's see how the parameters changed across optimization iterations. The histograms show the distribution of parameter values across ensemble realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-param-hist",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = os.path.join(project_ws, 'params.csv')\n",
    "\n",
    "# Get all parameter files from optimization steps\n",
    "steps = []\n",
    "for i in range(10):  # Check up to 10 iterations\n",
    "    step_file = os.path.join(pest_dir, f'3_Crane.{i}.par.csv')\n",
    "    if os.path.exists(step_file):\n",
    "        steps.append(step_file)\n",
    "\n",
    "if steps:\n",
    "    print(f\"Found {len(steps)} optimization steps\")\n",
    "    \n",
    "    fig_dir = os.path.join(project_ws, 'figures', 'parameter_hist')\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot histograms (set fig_out_dir=fig_dir to save PNGs)\n",
    "    plot_parameter_histograms(initial_params, steps, fig_out_dir=None)\n",
    "else:\n",
    "    print(\"No parameter files found. Run notebook 02_calibration first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-header",
   "metadata": {},
   "source": [
    "## 2. Run the Calibrated Model\n",
    "\n",
    "To run with calibrated parameters:\n",
    "1. Set `forecast=True` when reading config\n",
    "2. Ensure `[forecast]` section in config points to the final `.par.csv` file\n",
    "\n",
    "The config file should have:\n",
    "```toml\n",
    "[forecast]\n",
    "forecast_parameters = \"{pest_run_dir}/pest/3_Crane.3.par.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fields(ini_path, project_ws, selected_feature, output_csv, forecast=False):\n",
    "    \"\"\"Run SWIM model and save combined input/output to CSV.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = ProjectConfig()\n",
    "    config.read_config(ini_path, project_ws, forecast=forecast)\n",
    "\n",
    "    fields = SamplePlots()\n",
    "    fields.initialize_plot_data(config)\n",
    "    fields.output = field_day_loop(config, fields, debug_flag=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'\\nExecution time: {end_time - start_time:.2f} seconds\\n')\n",
    "\n",
    "    out_df = fields.output[selected_feature].copy()\n",
    "    in_df = fields.input_to_dataframe(selected_feature)\n",
    "    \n",
    "    # Drop columns from input that already exist in output to avoid duplicates\n",
    "    overlap_cols = out_df.columns.intersection(in_df.columns)\n",
    "    if len(overlap_cols) > 0:\n",
    "        in_df = in_df.drop(columns=overlap_cols)\n",
    "    \n",
    "    df = pd.concat([out_df, in_df], axis=1, ignore_index=False)\n",
    "    df.to_csv(output_csv)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = 'S2'\n",
    "out_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_calibrated.csv')\n",
    "\n",
    "# Run with forecast=True to use calibrated parameters\n",
    "df = run_fields(config_file, project_ws, selected_feature=selected_feature, \n",
    "                output_csv=out_csv, forecast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 3. Visualize Calibrated Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-year",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf = df.loc['2004-01-01': '2004-12-31']\n",
    "print(f'Total irrigation: {ydf.irrigation.sum():.1f} mm')\n",
    "print(f'Total ET: {ydf.et_act.sum():.1f} mm')\n",
    "print(f'Total precipitation: {ydf.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_swim_timeseries(ydf, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], \n",
    "                     start='2004-01-01', end='2004-12-31', png_dir='et_calibrated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-header",
   "metadata": {},
   "source": "## 4. Compare with Flux Tower Observations\n\nWe compare three estimates of actual ET (mm/day):\n\n1. **SWIM ET**: Model-estimated actual evapotranspiration (daily)\n2. **OpenET Ensemble ET**: Remote sensing retrievals from OpenET (PT-JPL, SIMS, SSEBop, geeSEBAL) averaged together (ETf x ETo)\n3. **Flux ET**: Independent observations from the S2 eddy covariance tower\n\nWe show two comparisons:\n- **Capture dates only**: Both methods compared only on Landsat overpass dates\n- **Full time series**: SWIM (daily) vs OpenET (interpolated between Landsat dates) on all flux tower days"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-function",
   "metadata": {},
   "outputs": [],
   "source": "def compare_et_estimates(combined_output_path, flux_data_path, irr=True):\n    \"\"\"Compare model ET and OpenET ensemble ET against flux tower observations.\n    \n    Returns two comparison DataFrames:\n    1. Capture dates only: Both methods on Landsat overpass dates only\n    2. Full time series: SWIM daily, OpenET interpolated, on all flux tower days\n    \n    Reports R², Pearson r, bias, and RMSE for each comparison.\n    \"\"\"\n    flux_data = pd.read_csv(flux_data_path, index_col='date', parse_dates=True)\n    flux_et = flux_data['ET']  # Actual ET from flux tower (mm/day)\n\n    output = pd.read_csv(combined_output_path, index_col=0)\n    output.index = pd.to_datetime(output.index)\n\n    # Determine suffix based on irrigation mask\n    mask_suffix = 'irr' if irr else 'inv_irr'\n    \n    # OpenET ensemble models - compute mean ETf across available models\n    ensemble_models = ['ptjpl', 'sims', 'ssebop', 'geesebal']\n    etf_cols = []\n    for model in ensemble_models:\n        col_name = f'etf_{model}_{mask_suffix}'\n        if col_name in output.columns:\n            etf_cols.append(col_name)\n    \n    # Fallback to single SSEBop if ensemble columns not available\n    if not etf_cols:\n        etf_col = f'etf_{mask_suffix}'\n        if etf_col in output.columns:\n            etf_cols = [etf_col]\n            print(f\"Using single ETf column: {etf_col}\")\n        else:\n            print(f\"Warning: No ETf columns found for mask '{mask_suffix}'\")\n            return pd.DataFrame(), pd.DataFrame()\n    \n    print(f\"Using ETf columns: {etf_cols}\")\n    \n    # Compute ensemble mean ETf (ignoring NaN)\n    ensemble_etf = output[etf_cols].mean(axis=1, skipna=True)\n    \n    # Calculate actual ET from OpenET ensemble: ETf x ETo (sparse, only on Landsat dates)\n    openet_et_sparse = ensemble_etf * output['etref']\n    \n    # Linear interpolation of OpenET to get daily values\n    openet_et_interp = openet_et_sparse.interpolate(method='linear')\n    \n    # Count original OpenET observations\n    n_openet_obs = openet_et_sparse.notna().sum()\n\n    # CAPTURE DATES ONLY comparison (OpenET sparse)\n    capture_df = pd.DataFrame({\n        'swim_et': output['et_act'],\n        'openet_et': openet_et_sparse,\n        'flux_et': flux_et\n    }).dropna()\n\n    # FULL TIME SERIES comparison (OpenET interpolated)\n    full_df = pd.DataFrame({\n        'swim_et': output['et_act'],\n        'openet_et': openet_et_interp,\n        'flux_et': flux_et\n    }).dropna()\n\n    def calc_metrics(df, col1, col2):\n        r, _ = stats.pearsonr(df[col1], df[col2])\n        r2 = r2_score(df[col1], df[col2])\n        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n        bias = (df[col2] - df[col1]).mean()\n        return r2, r, rmse, bias\n\n    # Capture dates metrics\n    r2_swim_cap, r_swim_cap, rmse_swim_cap, bias_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\n    r2_openet_cap, r_openet_cap, rmse_openet_cap, bias_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n\n    # Full time series metrics\n    r2_swim_full, r_swim_full, rmse_swim_full, bias_swim_full = calc_metrics(full_df, 'flux_et', 'swim_et')\n    r2_openet_full, r_openet_full, rmse_openet_full, bias_openet_full = calc_metrics(full_df, 'flux_et', 'openet_et')\n\n    print(\"=\"*70)\n    print(f\"CAPTURE DATES ONLY ({len(capture_df)} Landsat overpass dates)\")\n    print(\"=\"*70)\n    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n    print(\"-\" * 38)\n    print(f\"{'R²':<12} {r2_swim_cap:>12.3f} {r2_openet_cap:>12.3f}\")\n    print(f\"{'Pearson r':<12} {r_swim_cap:>12.3f} {r_openet_cap:>12.3f}\")\n    print(f\"{'Bias (mm)':<12} {bias_swim_cap:>12.3f} {bias_openet_cap:>12.3f}\")\n    print(f\"{'RMSE (mm)':<12} {rmse_swim_cap:>12.3f} {rmse_openet_cap:>12.3f}\")\n    \n    print()\n    print(\"=\"*70)\n    print(f\"FULL TIME SERIES ({len(full_df)} days, OpenET interpolated from {n_openet_obs} obs)\")\n    print(\"=\"*70)\n    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n    print(\"-\" * 38)\n    print(f\"{'R²':<12} {r2_swim_full:>12.3f} {r2_openet_full:>12.3f}\")\n    print(f\"{'Pearson r':<12} {r_swim_full:>12.3f} {r_openet_full:>12.3f}\")\n    print(f\"{'Bias (mm)':<12} {bias_swim_full:>12.3f} {bias_openet_full:>12.3f}\")\n    print(f\"{'RMSE (mm)':<12} {rmse_swim_full:>12.3f} {rmse_openet_full:>12.3f}\")\n\n    return full_df, capture_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-run",
   "metadata": {},
   "outputs": [],
   "source": "# Use irrigated mask since this is an irrigated site\nflux_data = os.path.join(data, 'S2_daily_data.csv')\nfull_df, capture_df = compare_et_estimates(out_csv, flux_data, irr=True)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-uncal-header",
   "metadata": {},
   "source": "### Compare Calibrated vs Uncalibrated Results\n\nLet's compare the calibrated results with the uncalibrated baseline to see how much calibration improved performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-both",
   "metadata": {},
   "outputs": [],
   "source": "uncal_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_uncalibrated.csv')\n\nif os.path.exists(uncal_csv):\n    print(\"UNCALIBRATED RESULTS:\")\n    print(\"-\" * 70)\n    full_df_uncal, capture_df_uncal = compare_et_estimates(uncal_csv, flux_data, irr=True)\n    \n    # Calculate improvement metrics (using full time series)\n    def calc_metrics(df, col1, col2):\n        r, _ = stats.pearsonr(df[col1], df[col2])\n        r2 = r2_score(df[col1], df[col2])\n        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n        bias = (df[col2] - df[col1]).mean()\n        return r2, r, rmse, bias\n\n    r2_cal, r_cal, rmse_cal, bias_cal = calc_metrics(full_df, 'flux_et', 'swim_et')\n    r2_uncal, r_uncal, rmse_uncal, bias_uncal = calc_metrics(full_df_uncal, 'flux_et', 'swim_et')\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(\"CALIBRATION IMPROVEMENT SUMMARY (Full Time Series)\")\n    print(\"=\"*70)\n    print(f\"{'Metric':<12} {'Uncalibrated':>15} {'Calibrated':>15} {'Change':>12}\")\n    print(\"-\" * 56)\n    print(f\"{'R²':<12} {r2_uncal:>15.3f} {r2_cal:>15.3f} {r2_cal - r2_uncal:>+12.3f}\")\n    print(f\"{'Pearson r':<12} {r_uncal:>15.3f} {r_cal:>15.3f} {r_cal - r_uncal:>+12.3f}\")\n    print(f\"{'Bias (mm)':<12} {bias_uncal:>15.3f} {bias_cal:>15.3f} {bias_cal - bias_uncal:>+12.3f}\")\n    print(f\"{'RMSE (mm)':<12} {rmse_uncal:>15.3f} {rmse_cal:>15.3f} {rmse_cal - rmse_uncal:>+12.3f}\")\nelse:\n    print(\"Uncalibrated output not found. Run notebook 01 first.\")\n    full_df_uncal = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-scatter",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Create 2x2 scatter plots for both comparisons\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Helper function to calculate metrics\ndef calc_metrics(df, col1, col2):\n    r, _ = stats.pearsonr(df[col1], df[col2])\n    r2 = r2_score(df[col1], df[col2])\n    rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n    return r2, r, rmse\n\n# Determine axis limits\nmax_et = max(full_df['flux_et'].max(), full_df['swim_et'].max(), \n             full_df['openet_et'].max()) * 1.1\n\n# TOP ROW: Capture dates only\nr2_swim_cap, r_swim_cap, rmse_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\nr2_openet_cap, r_openet_cap, rmse_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n\nax = axes[0, 0]\nax.scatter(capture_df['flux_et'], capture_df['swim_et'], alpha=0.5, s=15)\nax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\nax.set_xlabel('Flux ET (mm/day)')\nax.set_ylabel('SWIM ET (mm/day)')\nax.set_title(f'SWIM vs Flux - Capture Dates (n={len(capture_df)})\\n'\n             f'R² = {r2_swim_cap:.3f}, r = {r_swim_cap:.3f}, RMSE = {rmse_swim_cap:.2f} mm')\nax.legend()\nax.set_xlim(0, max_et)\nax.set_ylim(0, max_et)\n\nax = axes[0, 1]\nax.scatter(capture_df['flux_et'], capture_df['openet_et'], alpha=0.5, s=15)\nax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\nax.set_xlabel('Flux ET (mm/day)')\nax.set_ylabel('OpenET Ensemble ET (mm/day)')\nax.set_title(f'OpenET vs Flux - Capture Dates (n={len(capture_df)})\\n'\n             f'R² = {r2_openet_cap:.3f}, r = {r_openet_cap:.3f}, RMSE = {rmse_openet_cap:.2f} mm')\nax.legend()\nax.set_xlim(0, max_et)\nax.set_ylim(0, max_et)\n\n# BOTTOM ROW: Full time series comparison\nr2_swim, r_swim, rmse_swim = calc_metrics(full_df, 'flux_et', 'swim_et')\nr2_openet, r_openet, rmse_openet = calc_metrics(full_df, 'flux_et', 'openet_et')\n\nax = axes[1, 0]\nax.scatter(full_df['flux_et'], full_df['swim_et'], alpha=0.3, s=10)\nax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\nax.set_xlabel('Flux ET (mm/day)')\nax.set_ylabel('SWIM ET (mm/day)')\nax.set_title(f'SWIM vs Flux - Full Series (n={len(full_df)})\\n'\n             f'R² = {r2_swim:.3f}, r = {r_swim:.3f}, RMSE = {rmse_swim:.2f} mm')\nax.legend()\nax.set_xlim(0, max_et)\nax.set_ylim(0, max_et)\n\nax = axes[1, 1]\nax.scatter(full_df['flux_et'], full_df['openet_et'], alpha=0.3, s=10)\nax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\nax.set_xlabel('Flux ET (mm/day)')\nax.set_ylabel('OpenET Ensemble ET (mm/day)')\nax.set_title(f'OpenET vs Flux - Full Series, interpolated (n={len(full_df)})\\n'\n             f'R² = {r2_openet:.3f}, r = {r_openet:.3f}, RMSE = {rmse_openet:.2f} mm')\nax.legend()\nax.set_xlim(0, max_et)\nax.set_ylim(0, max_et)\n\nplt.tight_layout()\nplt.savefig('comparison_scatter_calibrated.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": "## Key Insights\n\nCalibration improves model performance for this irrigated site. The improvement summary table shows how each metric changed from uncalibrated to calibrated.\n\n### Two comparison modes\n\n- **Capture dates**: Only Landsat overpass dates where we have satellite observations\n- **Full time series**: All flux tower days, with OpenET values interpolated between satellite dates\n\n### Why does this work?\n\n**The key insight is that we can mine the deep remote sensing-based ET record, but rather than driving the model with remote sensing ET directly, we drive the calibration with it.**\n\nThe model has access to:\n1. Daily meteorological data (not just satellite overpass days)\n2. Physically-based soil water balance constraints\n3. Flexibility to tune parameters using the remote sensing record\n\nThis combination gives SWIM a more grounded perspective on daily fluxes than remote sensing alone, resulting in better ET estimates.\n\n### For irrigated sites\n\nThe calibration is particularly important for irrigated sites because:\n- Default irrigation scheduling parameters may not match actual practices\n- The NDVI-to-Kcb relationship varies by crop type (alfalfa vs. other crops)\n- Soil parameters affect how quickly the model triggers irrigation\n\nBy calibrating against the OpenET ensemble ETf, the model learns the irrigation patterns and crop responses specific to this site."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}