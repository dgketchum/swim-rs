{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Calibration Tutorial - Crane, OR - Irrigated Flux Plot\n",
    "\n",
    "## Step 3: Running the Calibrated Model\n",
    "\n",
    "Now we evaluate whether calibration improved model performance by running in **forecast mode** with calibrated parameters.\n",
    "\n",
    "This notebook:\n",
    "1. Visualizes how parameters evolved during calibration\n",
    "2. Runs the model with calibrated parameters\n",
    "3. Compares calibrated vs uncalibrated performance against flux tower observations\n",
    "4. Reports multiple metrics (R², Pearson r, RMSE, bias) for both capture dates and full time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:09.359900773Z",
     "start_time": "2026-01-28T06:38:08.906320740Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:22.836927Z",
     "iopub.status.busy": "2026-01-28T18:03:22.836401Z",
     "iopub.status.idle": "2026-01-28T18:03:24.801656Z",
     "shell.execute_reply": "2026-01-28T18:03:24.800834Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "root = os.path.abspath('../..')\n",
    "sys.path.append(root)\n",
    "\n",
    "from swimrs.swim.config import ProjectConfig\n",
    "from swimrs.container import SwimContainer\n",
    "from swimrs.process.input import build_swim_input\n",
    "from swimrs.process.loop_fast import run_daily_loop_fast\n",
    "\n",
    "from swimrs.viz.param_evolution import plot_parameter_histograms\n",
    "from swimrs.viz.swim_timeseries import plot_swim_timeseries\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:09.410070041Z",
     "start_time": "2026-01-28T06:38:09.361313327Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:24.806649Z",
     "iopub.status.busy": "2026-01-28T18:03:24.806268Z",
     "iopub.status.idle": "2026-01-28T18:03:24.810840Z",
     "shell.execute_reply": "2026-01-28T18:03:24.809962Z"
    }
   },
   "outputs": [],
   "source": [
    "project_ws = os.path.abspath('.')\n",
    "data = os.path.join(project_ws, 'data')\n",
    "pestrun = os.path.join(data, 'pestrun')\n",
    "pest_dir = os.path.join(pestrun, 'pest')\n",
    "\n",
    "config_file = os.path.join(project_ws, '3_Crane.toml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-param-header",
   "metadata": {},
   "source": [
    "## 1. Visualize Parameter Evolution\n",
    "\n",
    "Let's see how the parameters changed across optimization iterations. The histograms show the distribution of parameter values across ensemble realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-param-hist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:09.467936777Z",
     "start_time": "2026-01-28T06:38:09.411536830Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:24.814518Z",
     "iopub.status.busy": "2026-01-28T18:03:24.814319Z",
     "iopub.status.idle": "2026-01-28T18:03:24.822737Z",
     "shell.execute_reply": "2026-01-28T18:03:24.821793Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_params = os.path.join(pestrun, 'params.csv')\n",
    "\n",
    "# Get all parameter files from optimization steps\n",
    "steps = []\n",
    "for i in range(10):  # Check up to 10 iterations\n",
    "    step_file = os.path.join(pest_dir, f'3_Crane.{i}.par.csv')\n",
    "    if os.path.exists(step_file):\n",
    "        steps.append(step_file)\n",
    "\n",
    "if steps:\n",
    "    print(f\"Found {len(steps)} optimization steps\")\n",
    "    \n",
    "    fig_dir = os.path.join(project_ws, 'figures', 'parameter_hist')\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot histograms (set fig_out_dir=fig_dir to save PNGs)\n",
    "    plot_parameter_histograms(initial_params, steps, fig_out_dir=None)\n",
    "else:\n",
    "    print(\"No parameter files found. Run notebook 02_calibration first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-header",
   "metadata": {},
   "source": [
    "## 2. Run the Calibrated Model\n",
    "\n",
    "To run with calibrated parameters:\n",
    "1. Set `forecast=True` when reading config\n",
    "2. Ensure `[forecast]` section in config points to the final `.par.csv` file\n",
    "\n",
    "The config file should have:\n",
    "```toml\n",
    "[forecast]\n",
    "forecast_parameters = \"{pest_run_dir}/pest/3_Crane.3.par.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:09.533840391Z",
     "start_time": "2026-01-28T06:38:09.469417218Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:24.826783Z",
     "iopub.status.busy": "2026-01-28T18:03:24.826538Z",
     "iopub.status.idle": "2026-01-28T18:03:24.848791Z",
     "shell.execute_reply": "2026-01-28T18:03:24.848190Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_fields(config, container, selected_feature, output_csv, forecast=False):\n",
    "    \"\"\"Run SWIM model using the process package and save output to CSV.\n",
    "    \n",
    "    Uses the modern process package workflow:\n",
    "    1. Build SwimInput from container (with calibrated params if forecast=True)\n",
    "    2. Run simulation with run_daily_loop_fast()\n",
    "    3. Convert DailyOutput to DataFrame with time series\n",
    "    4. Add ETf observations from container for comparison\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create temporary HDF5 for SwimInput\n",
    "    with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as tmp:\n",
    "        temp_h5_path = tmp.name\n",
    "    \n",
    "    # Handle calibrated parameters for forecast mode\n",
    "    calibrated_params_path = None\n",
    "    if forecast:\n",
    "        # Read forecast parameters and convert to JSON\n",
    "        config.read_forecast_parameters()\n",
    "        if hasattr(config, 'forecast_parameters') and config.forecast_parameters is not None:\n",
    "            calibrated_params_path = _convert_forecast_params_to_json(\n",
    "                config.forecast_parameters, \n",
    "                os.path.dirname(output_csv)\n",
    "            )\n",
    "    \n",
    "    try:\n",
    "        # Build SwimInput from container\n",
    "        swim_input = build_swim_input(\n",
    "            container,\n",
    "            output_h5=temp_h5_path,\n",
    "            calibrated_params_path=calibrated_params_path,\n",
    "            runoff_process=getattr(config, 'runoff_process', 'cn'),\n",
    "            etf_model=getattr(config, 'etf_target_model', 'ssebop'),\n",
    "            met_source=getattr(config, 'met_source', 'gridmet'),\n",
    "            fields=[selected_feature],\n",
    "        )\n",
    "        \n",
    "        # Run simulation (uses fast JIT-compiled loop)\n",
    "        output, final_state = run_daily_loop_fast(swim_input)\n",
    "        \n",
    "        # Get time series data\n",
    "        n_days = swim_input.n_days\n",
    "        dates = pd.date_range(swim_input.start_date, periods=n_days, freq='D')\n",
    "        \n",
    "        # Get input time series for DataFrame\n",
    "        etr = swim_input.get_time_series('etr')\n",
    "        prcp = swim_input.get_time_series('prcp')\n",
    "        tmin = swim_input.get_time_series('tmin')\n",
    "        tmax = swim_input.get_time_series('tmax')\n",
    "        ndvi = swim_input.get_time_series('ndvi')\n",
    "        \n",
    "        # Build DataFrame (field index 0 since we're doing single field)\n",
    "        i = 0\n",
    "        df_data = {\n",
    "            # Model outputs\n",
    "            'et_act': output.eta[:, i],\n",
    "            'etref': etr[:, i],\n",
    "            'kc_act': output.etf[:, i],\n",
    "            'kc_bas': output.kcb[:, i],\n",
    "            'ks': output.ks[:, i],\n",
    "            'ke': output.ke[:, i],\n",
    "            'melt': output.melt[:, i],\n",
    "            'rain': output.rain[:, i],\n",
    "            'depl_root': output.depl_root[:, i],\n",
    "            'dperc': output.dperc[:, i],\n",
    "            'runoff': output.runoff[:, i],\n",
    "            'swe': output.swe[:, i],\n",
    "            'irrigation': output.irr_sim[:, i],\n",
    "            'gw_sim': output.gw_sim[:, i],\n",
    "            # Input time series\n",
    "            'ppt': prcp[:, i],\n",
    "            'tmin': tmin[:, i],\n",
    "            'tmax': tmax[:, i],\n",
    "            'tavg': (tmin[:, i] + tmax[:, i]) / 2.0,\n",
    "            'ndvi': ndvi[:, i],\n",
    "        }\n",
    "        \n",
    "        # Calculate derived columns\n",
    "        df_data['soil_water'] = swim_input.properties.awc[i] - output.depl_root[:, i]\n",
    "        \n",
    "        df = pd.DataFrame(df_data, index=dates)\n",
    "        \n",
    "        swim_input.close()\n",
    "        \n",
    "        # Load ETf observations from container for comparison\n",
    "        etf_model = getattr(config, 'etf_target_model', 'ssebop')\n",
    "        \n",
    "        for mask in ['inv_irr', 'irr']:\n",
    "            if etf_model == 'ensemble':\n",
    "                # Compute ensemble mean from all available models\n",
    "                known_models = ['ssebop', 'ptjpl', 'sims', 'geesebal', 'eemetric', 'disalexi']\n",
    "                mask_data = []\n",
    "                for model in known_models:\n",
    "                    etf_path = f\"remote_sensing/etf/landsat/{model}/{mask}\"\n",
    "                    try:\n",
    "                        etf_df = container.query.dataframe(etf_path, fields=[selected_feature])\n",
    "                        if selected_feature in etf_df.columns:\n",
    "                            mask_data.append(etf_df[selected_feature])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                \n",
    "                if mask_data:\n",
    "                    combined = pd.concat(mask_data, axis=1)\n",
    "                    ensemble_mean = combined.mean(axis=1)\n",
    "                    etf_series = ensemble_mean.reindex(dates)\n",
    "                    df[f'etf_{mask}'] = etf_series.values\n",
    "                    print(f\"Loaded ensemble ETf for {mask} from {len(mask_data)} models\")\n",
    "                else:\n",
    "                    print(f\"Warning: No ETf models found for {mask}\")\n",
    "                    df[f'etf_{mask}'] = np.nan\n",
    "            else:\n",
    "                etf_path = f\"remote_sensing/etf/landsat/{etf_model}/{mask}\"\n",
    "                try:\n",
    "                    etf_df = container.query.dataframe(etf_path, fields=[selected_feature])\n",
    "                    etf_series = etf_df[selected_feature].reindex(dates)\n",
    "                    df[f'etf_{mask}'] = etf_series.values\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load ETf for {mask}: {e}\")\n",
    "                    df[f'etf_{mask}'] = np.nan\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temp files\n",
    "        if os.path.exists(temp_h5_path):\n",
    "            os.remove(temp_h5_path)\n",
    "        if calibrated_params_path and os.path.exists(calibrated_params_path):\n",
    "            os.remove(calibrated_params_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'\\nExecution time: {end_time - start_time:.2f} seconds\\n')\n",
    "\n",
    "    df.to_csv(output_csv)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _convert_forecast_params_to_json(forecast_params, out_dir: str) -> str:\n",
    "    \"\"\"Convert forecast_parameters Series to JSON format for build_swim_input.\n",
    "\n",
    "    The forecast_parameters Series has index like 'kc_max_FID1', 'ndvi_k_FID1', etc.\n",
    "    We convert to: {FID1: {kc_max: val, ndvi_k: val, ...}, ...}\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import tempfile\n",
    "\n",
    "    params_by_fid = {}\n",
    "    for param_name in forecast_params.index:\n",
    "        # Parse param name: expect format like 'kc_max_FID1' or 'ndvi_k_FID1'\n",
    "        parts = param_name.rsplit('_', 1)\n",
    "        if len(parts) == 2:\n",
    "            base_param, fid = parts\n",
    "            if fid not in params_by_fid:\n",
    "                params_by_fid[fid] = {}\n",
    "            params_by_fid[fid][base_param] = float(forecast_params[param_name])\n",
    "\n",
    "    # Write to temp JSON file\n",
    "    fd, json_path = tempfile.mkstemp(suffix='.json', prefix='calib_params_', dir=out_dir)\n",
    "    os.close(fd)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(params_by_fid, f)\n",
    "\n",
    "    return json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-model",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:11.412938231Z",
     "start_time": "2026-01-28T06:38:09.535046925Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:24.852286Z",
     "iopub.status.busy": "2026-01-28T18:03:24.852128Z",
     "iopub.status.idle": "2026-01-28T18:03:26.858811Z",
     "shell.execute_reply": "2026-01-28T18:03:26.858045Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_feature = 'S2'\n",
    "out_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_calibrated.csv')\n",
    "\n",
    "# Open container for model run\n",
    "container_path = os.path.join(data, '3_Crane.swim')\n",
    "container = SwimContainer.open(container_path, mode='r')\n",
    "\n",
    "# Load config with forecast=True to get calibrated parameter path\n",
    "config = ProjectConfig()\n",
    "config.read_config(config_file, project_ws, forecast=True)\n",
    "\n",
    "try:\n",
    "    # Run with forecast=True to use calibrated parameters\n",
    "    df = run_fields(config, container, selected_feature=selected_feature, \n",
    "                    output_csv=out_csv, forecast=True)\n",
    "finally:\n",
    "    container.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 3. Visualize Calibrated Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-year",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:13.937354392Z",
     "start_time": "2026-01-28T06:38:11.473505007Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:26.863191Z",
     "iopub.status.busy": "2026-01-28T18:03:26.862941Z",
     "iopub.status.idle": "2026-01-28T18:03:29.606769Z",
     "shell.execute_reply": "2026-01-28T18:03:29.605444Z"
    }
   },
   "outputs": [],
   "source": [
    "ydf = df.loc['2004-01-01': '2004-12-31']\n",
    "print(f'Total irrigation: {ydf.irrigation.sum():.1f} mm')\n",
    "print(f'Total ET: {ydf.et_act.sum():.1f} mm')\n",
    "print(f'Total precipitation: {ydf.ppt.sum():.1f} mm')\n",
    "\n",
    "plot_swim_timeseries(ydf, ['et_act', 'etref', 'rain', 'melt', 'irrigation'], \n",
    "                     start='2004-01-01', end='2004-12-31', png_dir='et_calibrated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-header",
   "metadata": {},
   "source": [
    "## 4. Compare with Flux Tower Observations\n",
    "\n",
    "We compare three estimates of actual ET (mm/day):\n",
    "\n",
    "1. **SWIM ET**: Model-estimated actual evapotranspiration (daily)\n",
    "2. **OpenET Ensemble ET**: Remote sensing retrievals from OpenET (PT-JPL, SIMS, SSEBop, geeSEBAL) averaged together (ETf x ETo)\n",
    "3. **Flux ET**: Independent observations from the S2 eddy covariance tower\n",
    "\n",
    "We show two comparisons:\n",
    "- **Capture dates only**: Both methods compared only on Landsat overpass dates\n",
    "- **Full time series**: SWIM (daily) vs OpenET (interpolated between Landsat dates) on all flux tower days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:14.054975988Z",
     "start_time": "2026-01-28T06:38:14.000703423Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:29.611625Z",
     "iopub.status.busy": "2026-01-28T18:03:29.611114Z",
     "iopub.status.idle": "2026-01-28T18:03:29.638058Z",
     "shell.execute_reply": "2026-01-28T18:03:29.637389Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_et_estimates(combined_output_path, flux_data_path, irr=True):\n",
    "    \"\"\"Compare model ET and OpenET ensemble ET against flux tower observations.\n",
    "    \n",
    "    Returns two comparison DataFrames:\n",
    "    1. Capture dates only: Both methods on Landsat overpass dates only\n",
    "    2. Full time series: SWIM daily, OpenET interpolated, on all flux tower days\n",
    "    \n",
    "    Reports R², Pearson r, bias, and RMSE for each comparison.\n",
    "    \"\"\"\n",
    "    flux_data = pd.read_csv(flux_data_path, index_col='date', parse_dates=True)\n",
    "    flux_et = flux_data['ET_corr']  # Actual ET from flux tower (mm/day)\n",
    "\n",
    "    output = pd.read_csv(combined_output_path, index_col=0)\n",
    "    output.index = pd.to_datetime(output.index)\n",
    "\n",
    "    # Determine suffix based on irrigation mask\n",
    "    mask_suffix = 'irr' if irr else 'inv_irr'\n",
    "    \n",
    "    # OpenET ensemble models - compute mean ETf across available models\n",
    "    ensemble_models = ['ptjpl', 'sims', 'ssebop', 'geesebal']\n",
    "    etf_cols = []\n",
    "    for model in ensemble_models:\n",
    "        col_name = f'etf_{model}_{mask_suffix}'\n",
    "        if col_name in output.columns:\n",
    "            etf_cols.append(col_name)\n",
    "    \n",
    "    # Fallback to single SSEBop if ensemble columns not available\n",
    "    if not etf_cols:\n",
    "        etf_col = f'etf_{mask_suffix}'\n",
    "        if etf_col in output.columns:\n",
    "            etf_cols = [etf_col]\n",
    "            print(f\"Using single ETf column: {etf_col}\")\n",
    "        else:\n",
    "            print(f\"Warning: No ETf columns found for mask '{mask_suffix}'\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using ETf columns: {etf_cols}\")\n",
    "    \n",
    "    # Compute ensemble mean ETf (ignoring NaN)\n",
    "    ensemble_etf = output[etf_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Calculate actual ET from OpenET ensemble: ETf x ETo (sparse, only on Landsat dates)\n",
    "    openet_et_sparse = ensemble_etf * output['etref']\n",
    "    \n",
    "    # Linear interpolation of OpenET to get daily values\n",
    "    openet_et_interp = openet_et_sparse.interpolate(method='linear')\n",
    "    \n",
    "    # Count original OpenET observations\n",
    "    n_openet_obs = openet_et_sparse.notna().sum()\n",
    "\n",
    "    # CAPTURE DATES ONLY comparison (OpenET sparse)\n",
    "    capture_df = pd.DataFrame({\n",
    "        'swim_et': output['et_act'],\n",
    "        'openet_et': openet_et_sparse,\n",
    "        'flux_et': flux_et\n",
    "    }).dropna()\n",
    "\n",
    "    # FULL TIME SERIES comparison (OpenET interpolated)\n",
    "    full_df = pd.DataFrame({\n",
    "        'swim_et': output['et_act'],\n",
    "        'openet_et': openet_et_interp,\n",
    "        'flux_et': flux_et\n",
    "    }).dropna()\n",
    "\n",
    "    def calc_metrics(df, col1, col2):\n",
    "        r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "        r2 = r2_score(df[col1], df[col2])\n",
    "        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "        bias = (df[col2] - df[col1]).mean()\n",
    "        return r2, r, rmse, bias\n",
    "\n",
    "    # Capture dates metrics\n",
    "    r2_swim_cap, r_swim_cap, rmse_swim_cap, bias_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\n",
    "    r2_openet_cap, r_openet_cap, rmse_openet_cap, bias_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n",
    "\n",
    "    # Full time series metrics\n",
    "    r2_swim_full, r_swim_full, rmse_swim_full, bias_swim_full = calc_metrics(full_df, 'flux_et', 'swim_et')\n",
    "    r2_openet_full, r_openet_full, rmse_openet_full, bias_openet_full = calc_metrics(full_df, 'flux_et', 'openet_et')\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(f\"CAPTURE DATES ONLY ({len(capture_df)} Landsat overpass dates)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_cap:>12.3f} {r2_openet_cap:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_cap:>12.3f} {r_openet_cap:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_cap:>12.3f} {bias_openet_cap:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_cap:>12.3f} {rmse_openet_cap:>12.3f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "    print(f\"FULL TIME SERIES ({len(full_df)} days, OpenET interpolated from {n_openet_obs} obs)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<12} {'SWIM ET':>12} {'OpenET ET':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"{'R²':<12} {r2_swim_full:>12.3f} {r2_openet_full:>12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_swim_full:>12.3f} {r_openet_full:>12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_swim_full:>12.3f} {bias_openet_full:>12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_swim_full:>12.3f} {rmse_openet_full:>12.3f}\")\n",
    "\n",
    "    return full_df, capture_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:14.150496018Z",
     "start_time": "2026-01-28T06:38:14.056094191Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:29.641112Z",
     "iopub.status.busy": "2026-01-28T18:03:29.640880Z",
     "iopub.status.idle": "2026-01-28T18:03:29.735520Z",
     "shell.execute_reply": "2026-01-28T18:03:29.734908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use irrigated mask since this is an irrigated site\n",
    "flux_data = os.path.join(data, 'S2_daily_data.csv')\n",
    "full_df, capture_df = compare_et_estimates(out_csv, flux_data, irr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-uncal-header",
   "metadata": {},
   "source": [
    "### Compare Calibrated vs Uncalibrated Results\n",
    "\n",
    "Let's compare the calibrated results with the uncalibrated baseline to see how much calibration improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-both",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:14.228038759Z",
     "start_time": "2026-01-28T06:38:14.152051058Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:29.739125Z",
     "iopub.status.busy": "2026-01-28T18:03:29.738953Z",
     "iopub.status.idle": "2026-01-28T18:03:29.814579Z",
     "shell.execute_reply": "2026-01-28T18:03:29.813972Z"
    }
   },
   "outputs": [],
   "source": [
    "uncal_csv = os.path.join(project_ws, f'combined_output_{selected_feature}_uncalibrated.csv')\n",
    "\n",
    "if os.path.exists(uncal_csv):\n",
    "    print(\"UNCALIBRATED RESULTS:\")\n",
    "    print(\"-\" * 70)\n",
    "    full_df_uncal, capture_df_uncal = compare_et_estimates(uncal_csv, flux_data, irr=True)\n",
    "    \n",
    "    # Calculate improvement metrics (using full time series)\n",
    "    def calc_metrics(df, col1, col2):\n",
    "        r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "        r2 = r2_score(df[col1], df[col2])\n",
    "        rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "        bias = (df[col2] - df[col1]).mean()\n",
    "        return r2, r, rmse, bias\n",
    "\n",
    "    r2_cal, r_cal, rmse_cal, bias_cal = calc_metrics(full_df, 'flux_et', 'swim_et')\n",
    "    r2_uncal, r_uncal, rmse_uncal, bias_uncal = calc_metrics(full_df_uncal, 'flux_et', 'swim_et')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"CALIBRATION IMPROVEMENT SUMMARY (Full Time Series)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<12} {'Uncalibrated':>15} {'Calibrated':>15} {'Change':>12}\")\n",
    "    print(\"-\" * 56)\n",
    "    print(f\"{'R²':<12} {r2_uncal:>15.3f} {r2_cal:>15.3f} {r2_cal - r2_uncal:>+12.3f}\")\n",
    "    print(f\"{'Pearson r':<12} {r_uncal:>15.3f} {r_cal:>15.3f} {r_cal - r_uncal:>+12.3f}\")\n",
    "    print(f\"{'Bias (mm)':<12} {bias_uncal:>15.3f} {bias_cal:>15.3f} {bias_cal - bias_uncal:>+12.3f}\")\n",
    "    print(f\"{'RMSE (mm)':<12} {rmse_uncal:>15.3f} {rmse_cal:>15.3f} {rmse_cal - rmse_uncal:>+12.3f}\")\n",
    "else:\n",
    "    print(\"Uncalibrated output not found. Run notebook 01 first.\")\n",
    "    full_df_uncal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-scatter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:15.289858006Z",
     "start_time": "2026-01-28T06:38:14.229223606Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:29.817922Z",
     "iopub.status.busy": "2026-01-28T18:03:29.817749Z",
     "iopub.status.idle": "2026-01-28T18:03:30.814536Z",
     "shell.execute_reply": "2026-01-28T18:03:30.813880Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create 2x2 scatter plots for both comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Helper function to calculate metrics\n",
    "def calc_metrics(df, col1, col2):\n",
    "    r, _ = stats.pearsonr(df[col1], df[col2])\n",
    "    r2 = r2_score(df[col1], df[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df[col1], df[col2]))\n",
    "    return r2, r, rmse\n",
    "\n",
    "# Determine axis limits\n",
    "max_et = max(full_df['flux_et'].max(), full_df['swim_et'].max(), \n",
    "             full_df['openet_et'].max()) * 1.1\n",
    "\n",
    "# TOP ROW: Capture dates only\n",
    "r2_swim_cap, r_swim_cap, rmse_swim_cap = calc_metrics(capture_df, 'flux_et', 'swim_et')\n",
    "r2_openet_cap, r_openet_cap, rmse_openet_cap = calc_metrics(capture_df, 'flux_et', 'openet_et')\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(capture_df['flux_et'], capture_df['swim_et'], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('SWIM ET (mm/day)')\n",
    "ax.set_title(f'SWIM vs Flux - Capture Dates (n={len(capture_df)})\\n'\n",
    "             f'R² = {r2_swim_cap:.3f}, r = {r_swim_cap:.3f}, RMSE = {rmse_swim_cap:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(capture_df['flux_et'], capture_df['openet_et'], alpha=0.5, s=15)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('OpenET Ensemble ET (mm/day)')\n",
    "ax.set_title(f'OpenET vs Flux - Capture Dates (n={len(capture_df)})\\n'\n",
    "             f'R² = {r2_openet_cap:.3f}, r = {r_openet_cap:.3f}, RMSE = {rmse_openet_cap:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "# BOTTOM ROW: Full time series comparison\n",
    "r2_swim, r_swim, rmse_swim = calc_metrics(full_df, 'flux_et', 'swim_et')\n",
    "r2_openet, r_openet, rmse_openet = calc_metrics(full_df, 'flux_et', 'openet_et')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(full_df['flux_et'], full_df['swim_et'], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('SWIM ET (mm/day)')\n",
    "ax.set_title(f'SWIM vs Flux - Full Series (n={len(full_df)})\\n'\n",
    "             f'R² = {r2_swim:.3f}, r = {r_swim:.3f}, RMSE = {rmse_swim:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(full_df['flux_et'], full_df['openet_et'], alpha=0.3, s=10)\n",
    "ax.plot([0, max_et], [0, max_et], 'r--', label='1:1 line')\n",
    "ax.set_xlabel('Flux ET (mm/day)')\n",
    "ax.set_ylabel('OpenET Ensemble ET (mm/day)')\n",
    "ax.set_title(f'OpenET vs Flux - Full Series, interpolated (n={len(full_df)})\\n'\n",
    "             f'R² = {r2_openet:.3f}, r = {r_openet:.3f}, RMSE = {rmse_openet:.2f} mm')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, max_et)\n",
    "ax.set_ylim(0, max_et)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_scatter_calibrated.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bzn4buxg0e8",
   "metadata": {},
   "source": [
    "## 5. Mass Balance Check\n",
    "\n",
    "Verify water conservation across the full simulation period. The water balance equation:\n",
    "\n",
    "**Starting Water + Inputs - Outputs = Ending Water**\n",
    "\n",
    "Where:\n",
    "- **Starting Water** = Initial Soil Water + Initial SWE\n",
    "- **Inputs** = Precipitation + Irrigation  \n",
    "- **Outputs** = ET + Deep Percolation + Runoff\n",
    "- **Ending Water** = Final Soil Water + Final SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hl8wfbs77dk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:38:15.426490224Z",
     "start_time": "2026-01-28T06:38:15.362887845Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T18:03:30.822082Z",
     "iopub.status.busy": "2026-01-28T18:03:30.821894Z",
     "iopub.status.idle": "2026-01-28T18:03:30.834821Z",
     "shell.execute_reply": "2026-01-28T18:03:30.834262Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mass_balance(df):\n",
    "    \"\"\"Compute full-run mass balance and display as a table.\n",
    "    \n",
    "    Water Balance: Starting Water + Inputs - Outputs = Ending Water\n",
    "    \n",
    "    Components:\n",
    "    - Starting Water: Initial soil water + Initial SWE\n",
    "    - Inputs: Total precipitation + Total irrigation\n",
    "    - Outputs: Total ET + Total deep percolation + Total runoff\n",
    "    - Ending Water: Final soil water + Final SWE\n",
    "    \"\"\"\n",
    "    # Starting conditions (first day values)\n",
    "    start_soil_water = df['soil_water'].iloc[0]\n",
    "    start_swe = df['swe'].iloc[0]\n",
    "    start_water = start_soil_water + start_swe\n",
    "    \n",
    "    # Ending conditions (last day values)\n",
    "    end_soil_water = df['soil_water'].iloc[-1]\n",
    "    end_swe = df['swe'].iloc[-1]\n",
    "    end_water = end_soil_water + end_swe\n",
    "    \n",
    "    # Inputs (sums over full period)\n",
    "    total_precip = df['ppt'].sum()\n",
    "    total_irrigation = df['irrigation'].sum()\n",
    "    total_inputs = total_precip + total_irrigation\n",
    "    \n",
    "    # Outputs (sums over full period)\n",
    "    total_et = df['et_act'].sum()\n",
    "    total_dperc = df['dperc'].sum()\n",
    "    total_runoff = df['runoff'].sum()\n",
    "    total_outputs = total_et + total_dperc + total_runoff\n",
    "    \n",
    "    # Mass balance check\n",
    "    # Expected: Start + Inputs - Outputs = End\n",
    "    # Residual: (Start + Inputs - Outputs) - End = 0 if balanced\n",
    "    expected_end = start_water + total_inputs - total_outputs\n",
    "    residual = expected_end - end_water\n",
    "    pct_error = (residual / total_inputs) * 100 if total_inputs > 0 else 0\n",
    "    \n",
    "    # Create summary table\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MASS BALANCE SUMMARY\")\n",
    "    print(f\"Period: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')} ({len(df)} days)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n--- STARTING WATER ---\")\n",
    "    print(f\"  Initial Soil Water:     {start_soil_water:>10.2f} mm\")\n",
    "    print(f\"  Initial SWE:            {start_swe:>10.2f} mm\")\n",
    "    print(f\"  TOTAL STARTING:         {start_water:>10.2f} mm\")\n",
    "    \n",
    "    print(\"\\n--- INPUTS (+) ---\")\n",
    "    print(f\"  Precipitation:          {total_precip:>10.2f} mm\")\n",
    "    print(f\"  Irrigation:             {total_irrigation:>10.2f} mm\")\n",
    "    print(f\"  TOTAL INPUTS:           {total_inputs:>10.2f} mm\")\n",
    "    \n",
    "    print(\"\\n--- OUTPUTS (-) ---\")\n",
    "    print(f\"  Evapotranspiration:     {total_et:>10.2f} mm\")\n",
    "    print(f\"  Deep Percolation:       {total_dperc:>10.2f} mm\")\n",
    "    print(f\"  Runoff:                 {total_runoff:>10.2f} mm\")\n",
    "    print(f\"  TOTAL OUTPUTS:          {total_outputs:>10.2f} mm\")\n",
    "    \n",
    "    print(\"\\n--- ENDING WATER ---\")\n",
    "    print(f\"  Final Soil Water:       {end_soil_water:>10.2f} mm\")\n",
    "    print(f\"  Final SWE:              {end_swe:>10.2f} mm\")\n",
    "    print(f\"  TOTAL ENDING:           {end_water:>10.2f} mm\")\n",
    "    \n",
    "    print(\"\\n--- BALANCE CHECK ---\")\n",
    "    print(f\"  Expected End (Start + In - Out):  {expected_end:>10.2f} mm\")\n",
    "    print(f\"  Actual End:                       {end_water:>10.2f} mm\")\n",
    "    print(f\"  Residual (Error):                 {residual:>10.2f} mm\")\n",
    "    print(f\"  Error as % of Inputs:             {pct_error:>10.4f} %\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if abs(pct_error) < 0.01:\n",
    "        print(\"MASS BALANCE: CLOSED (error < 0.01%)\")\n",
    "    elif abs(pct_error) < 1.0:\n",
    "        print(\"MASS BALANCE: ACCEPTABLE (error < 1%)\")\n",
    "    else:\n",
    "        print(f\"MASS BALANCE: WARNING - {abs(pct_error):.2f}% error\")\n",
    "    \n",
    "    return {\n",
    "        'start_soil_water': start_soil_water,\n",
    "        'start_swe': start_swe,\n",
    "        'end_soil_water': end_soil_water,\n",
    "        'end_swe': end_swe,\n",
    "        'precip': total_precip,\n",
    "        'irrigation': total_irrigation,\n",
    "        'et': total_et,\n",
    "        'dperc': total_dperc,\n",
    "        'runoff': total_runoff,\n",
    "        'residual': residual,\n",
    "        'pct_error': pct_error\n",
    "    }\n",
    "\n",
    "# Run mass balance check\n",
    "balance = compute_mass_balance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "Calibration improves model performance for this irrigated site. The improvement summary table shows how each metric changed from uncalibrated to calibrated.\n",
    "\n",
    "### Two comparison modes\n",
    "\n",
    "- **Capture dates**: Only Landsat overpass dates where we have satellite observations\n",
    "- **Full time series**: All flux tower days, with OpenET values interpolated between satellite dates\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "**The key insight is that we can mine the deep remote sensing-based ET record, but rather than driving the model with remote sensing ET directly, we drive the calibration with it.**\n",
    "\n",
    "The model has access to:\n",
    "1. Daily meteorological data (not just satellite overpass days)\n",
    "2. Physically-based soil water balance constraints\n",
    "3. Flexibility to tune parameters using the remote sensing record\n",
    "\n",
    "This combination gives SWIM a more grounded perspective on daily fluxes than remote sensing alone, resulting in better ET estimates.\n",
    "\n",
    "### For irrigated sites\n",
    "\n",
    "The calibration is particularly important for irrigated sites because:\n",
    "- Default irrigation scheduling parameters may not match actual practices\n",
    "- The NDVI-to-Kcb relationship varies by crop type (alfalfa vs. other crops)\n",
    "- Soil parameters affect how quickly the model triggers irrigation\n",
    "\n",
    "By calibrating against the OpenET ensemble ETf, the model learns the irrigation patterns and crop responses specific to this site."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
