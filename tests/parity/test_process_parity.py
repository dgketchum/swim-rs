"""Parity tests comparing the new process package to golden outputs from old model.

These tests verify that the new process package produces equivalent results
to golden files generated by the old model (swimrs.model.obs_field_cycle).

The golden outputs are in tests/fixtures/multi_station/golden/*.csv and were
generated by running the legacy code on the prepped_input.json fixture.
"""

import json
import os
import shutil
import sys
import tempfile
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import pytest
from numpy.testing import assert_array_almost_equal, assert_allclose

# Mark entire module as parity and slow
pytestmark = [pytest.mark.parity, pytest.mark.slow]

# Ensure project root is on path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from swimrs.container import SwimContainer
from swimrs.process.input import build_swim_input, SwimInput
from swimrs.process.loop import run_daily_loop

# Import the converter
sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
from convert_legacy_input import convert_to_container, load_prepped_input


# Path to multi-station fixture
FIXTURE_DIR = PROJECT_ROOT / "tests" / "fixtures" / "multi_station"
GOLDEN_DIR = FIXTURE_DIR / "golden"
GOLDEN_INPUT = GOLDEN_DIR / "prepped_input.json"
GOLDEN_SPINUP = GOLDEN_DIR / "spinup.json"
GOLDEN_METADATA = GOLDEN_DIR / "metadata.json"
CALIBRATED_PARAMS = GOLDEN_DIR / "calibrated_params.json"
SHAPEFILE = FIXTURE_DIR / "data" / "gis" / "multi_station.shp"

# Field IDs in fixture (order matters - must match golden CSV order)
FIELD_IDS = ["ALARC2_Smith6", "US-FPe", "MR"]


def fixture_available():
    """Check if multi-station fixture is available."""
    return (
        GOLDEN_INPUT.exists()
        and SHAPEFILE.exists()
        and all((GOLDEN_DIR / f"{fid}.csv").exists() for fid in FIELD_IDS)
    )


@pytest.fixture(scope="module")
def converted_container(tmp_path_factory):
    """Convert legacy JSON to SwimContainer once per module."""
    if not fixture_available():
        pytest.skip("Multi-station fixture not available")

    # Create container in a temp directory that persists for the module
    tmpdir = tmp_path_factory.mktemp("parity")
    container_path = tmpdir / "converted.swim"

    convert_to_container(
        json_path=GOLDEN_INPUT,
        shapefile_path=SHAPEFILE,
        output_path=container_path,
        uid_column="site_id",
        met_source="gridmet",
        overwrite=True,
    )

    container = SwimContainer.open(str(container_path), mode="r")
    yield container
    container.close()


@pytest.fixture
def golden_outputs() -> dict[str, pd.DataFrame]:
    """Load golden outputs from CSV files."""
    if not fixture_available():
        pytest.skip("Multi-station fixture not available")

    outputs = {}
    for fid in FIELD_IDS:
        csv_path = GOLDEN_DIR / f"{fid}.csv"
        df = pd.read_csv(csv_path, index_col=0, parse_dates=True)
        outputs[fid] = df
    return outputs


@pytest.fixture
def process_outputs(converted_container):
    """Run process package and return outputs."""
    if not fixture_available():
        pytest.skip("Multi-station fixture not available")

    with tempfile.TemporaryDirectory() as tmpdir:
        h5_path = Path(tmpdir) / "swim_input.h5"

        # Load calibrated parameters if available
        calibrated_params = None
        if CALIBRATED_PARAMS.exists():
            with open(CALIBRATED_PARAMS) as f:
                calibrated_params = json.load(f)

        # Build HDF5 from converted container
        swim_input = build_swim_input(
            converted_container,
            h5_path,
            spinup_json_path=GOLDEN_SPINUP if GOLDEN_SPINUP.exists() else None,
            calibrated_params_path=CALIBRATED_PARAMS if CALIBRATED_PARAMS.exists() else None,
            start_date="2010-01-01",
            end_date="2011-12-31",
            etf_model="ssebop",
        )

        try:
            output, final_state = run_daily_loop(swim_input)
        finally:
            swim_input.close()

        # Convert to per-field DataFrames
        # Need to map container field order to golden field order
        dates = pd.date_range("2010-01-01", "2011-12-31", freq="D")
        outputs = {}

        # Get field order from swim_input
        container_fids = swim_input.fids

        for fid in FIELD_IDS:
            if fid in container_fids:
                i = container_fids.index(fid)
                outputs[fid] = pd.DataFrame({
                    'et_act': output.eta[:, i],
                    'kc_act': output.etf[:, i],
                    'kc_bas': output.kcb[:, i],
                    'ks': output.ks[:, i],
                    'ke': output.ke[:, i],
                    'kr': output.kr[:, i],
                    'depl_root': output.depl_root[:, i],
                    'swe': output.swe[:, i],
                    'rain': output.rain[:, i],
                    'melt': output.melt[:, i],
                    'runoff': output.runoff[:, i],
                    'dperc': output.dperc[:, i],
                    'irrigation': output.irr_sim[:, i],
                    'gw_sim': output.gw_sim[:, i],
                }, index=dates)

        return outputs


class TestKernelParity:
    """Test individual kernel calculations match old model."""

    def test_kcb_sigmoid_parity(self):
        """Test sigmoid Kcb calculation matches old model."""
        from swimrs.process.kernels.crop_coefficient import kcb_sigmoid

        # Test with known inputs from old model
        ndvi = np.array([0.2, 0.4, 0.6, 0.8])
        kc_max = np.array([1.15, 1.15, 1.15, 1.15])
        ndvi_k = np.array([7.0, 7.0, 7.0, 7.0])
        ndvi_0 = np.array([0.4, 0.4, 0.4, 0.4])

        kcb = kcb_sigmoid(ndvi, kc_max, ndvi_k, ndvi_0)

        # Old model formula: kc_max / (1 + exp(-k * (ndvi - ndvi_0)))
        expected = kc_max / (1.0 + np.exp(-ndvi_k * (ndvi - ndvi_0)))

        assert_array_almost_equal(kcb, expected, decimal=5)

    def test_snow_partition_parity(self):
        """Test snow partitioning matches old model."""
        from swimrs.process.kernels.snow import partition_precip

        # Cold - all snow
        precip = np.array([10.0])
        temp_cold = np.array([-5.0])
        rain, snow = partition_precip(precip, temp_cold)
        assert snow[0] == 10.0
        assert rain[0] == 0.0

        # Warm - all rain
        temp_warm = np.array([10.0])
        rain, snow = partition_precip(precip, temp_warm)
        assert rain[0] == 10.0
        assert snow[0] == 0.0

    def test_scs_runoff_parity(self):
        """Test SCS runoff calculation matches old model."""
        from swimrs.process.kernels.runoff import scs_runoff

        precip = np.array([50.0])
        cn = np.array([75.0])

        sro, s = scs_runoff(precip, cn)

        # Old model formula
        s_expected = 250.0 * (100.0 / cn[0] - 1.0)
        ia = 0.2 * s_expected
        if precip[0] > ia:
            sro_expected = (precip[0] - ia) ** 2 / (precip[0] + 0.8 * s_expected)
        else:
            sro_expected = 0.0

        assert abs(sro[0] - sro_expected) < 0.001
        assert abs(s[0] - s_expected) < 0.001


class TestGoldenParity:
    """Parity tests comparing process package to golden outputs.

    Note: The golden outputs are from a calibrated model run with parameters
    that differ from the process package defaults:
    - Calibrated `aw` (available water) parameter differs from soil `awc`
    - Year-specific groundwater subsidy (`f_sub`) for MR field
    - Calibrated kc_max, ke_max values

    Therefore, tests focus on:
    - Kcb correlation (should be high since same NDVI->Kcb formula)
    - Physical bounds (depletion, SWE non-negative, etc.)
    - General behavior patterns (not exact numerical match)
    """

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_output_shape_matches(self, golden_outputs, process_outputs):
        """Both models produce same number of days for each field."""
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                pytest.skip(f"Field {fid} not in process outputs")
            golden = golden_outputs[fid]
            process = process_outputs[fid]
            assert len(golden) == len(process), f"{fid}: {len(golden)} != {len(process)}"

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_kcb_correlation(self, golden_outputs, process_outputs):
        """Kcb values should be highly correlated.

        This is the core test for physics kernel correctness - both models
        use the same NDVI->Kcb sigmoid formula, so correlation should be high.
        """
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                continue
            golden_kcb = golden_outputs[fid]['kc_bas'].values
            process_kcb = process_outputs[fid]['kc_bas'].values

            # Filter out zeros/NaNs for correlation
            mask = (golden_kcb > 0) & (process_kcb > 0) & ~np.isnan(golden_kcb) & ~np.isnan(process_kcb)
            if mask.sum() < 10:
                continue

            correlation = np.corrcoef(golden_kcb[mask], process_kcb[mask])[0, 1]
            assert correlation > 0.95, f"{fid} Kcb correlation too low: {correlation:.3f}"

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_et_correlation(self, golden_outputs, process_outputs):
        """ET values should be correlated between legacy and new code."""
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                continue
            golden_et = golden_outputs[fid]['et_act'].values
            process_et = process_outputs[fid]['et_act'].values

            # Filter for valid values
            mask = (golden_et > 0) & (process_et > 0) & ~np.isnan(golden_et) & ~np.isnan(process_et)
            if mask.sum() < 10:
                continue

            correlation = np.corrcoef(golden_et[mask], process_et[mask])[0, 1]
            # ET may differ more due to parameter differences, so use lower threshold
            assert correlation > 0.80, f"{fid} ET correlation too low: {correlation:.3f}"

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_et_responds_to_climate(self, golden_outputs, process_outputs):
        """ET should respond similarly to climate forcing (seasonal pattern).

        We don't expect exact match due to parameter differences, but both
        should show similar seasonal patterns.
        """
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                continue
            golden_et = golden_outputs[fid]['et_act']
            process_et = process_outputs[fid]['et_act']

            # Compare monthly means - should have same seasonal pattern
            golden_monthly = golden_et.groupby(golden_et.index.month).mean()
            process_monthly = process_et.groupby(process_et.index.month).mean()

            # Check that peak month is within 1 month
            golden_peak = golden_monthly.idxmax()
            process_peak = process_monthly.idxmax()
            assert abs(golden_peak - process_peak) <= 2, (
                f"{fid}: Peak ET month differs by more than 2: "
                f"golden peak={golden_peak}, process peak={process_peak}"
            )

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_swe_accumulates_in_winter(self, golden_outputs, process_outputs):
        """SWE should accumulate during cold months at snowy stations."""
        # US-FPe is the snowy station
        fid = "US-FPe"
        if fid not in process_outputs:
            pytest.skip(f"Field {fid} not in process outputs")

        process_swe = process_outputs[fid]['swe']

        # Check winter has some snow accumulation
        winter = process_swe.loc["2010-01":"2010-03"]
        assert winter.max() > 0, "No snow accumulation in winter"

        # Check summer has less snow
        summer = process_swe.loc["2010-06":"2010-08"]
        assert summer.mean() < winter.max(), "Summer SWE should be less than winter peak"

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_depletion_reasonable_range(self, golden_outputs, process_outputs):
        """Depletion values should be in reasonable range."""
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                continue
            golden_depl = golden_outputs[fid]['depl_root'].values
            process_depl = process_outputs[fid]['depl_root'].values

            # Check both are within reasonable bounds
            assert golden_depl.min() >= -10, f"{fid} golden depl too negative"
            assert process_depl.min() >= -10, f"{fid} process depl too negative"

            # Check max depletion is reasonable (< 1000mm)
            assert golden_depl.max() < 1000, f"{fid} golden depl unreasonably high"
            assert process_depl.max() < 1000, f"{fid} process depl unreasonably high"

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_total_et_within_range(self, golden_outputs, process_outputs):
        """Total ET should be within reasonable range of golden values."""
        for fid in FIELD_IDS:
            if fid not in process_outputs:
                continue
            golden_total = golden_outputs[fid]['et_act'].sum()
            process_total = process_outputs[fid]['et_act'].sum()

            # Allow 30% difference due to parameter variations
            ratio = process_total / golden_total if golden_total > 0 else 1.0
            assert 0.7 < ratio < 1.3, (
                f"{fid}: Total ET ratio out of range: "
                f"golden={golden_total:.1f}, process={process_total:.1f}, ratio={ratio:.2f}"
            )


class TestProcessPackageStandalone:
    """Tests that run the new process package standalone (no golden comparison)."""

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_run_on_multi_station_data(self, converted_container):
        """Process package can run on multi-station data."""
        with tempfile.TemporaryDirectory() as tmpdir:
            h5_path = Path(tmpdir) / "swim_input.h5"

            # Build HDF5 from converted container for a shorter period
            swim_input = build_swim_input(
                converted_container,
                h5_path,
                start_date="2010-06-01",
                end_date="2010-06-30",
                etf_model="ssebop",
            )

            try:
                output, final_state = run_daily_loop(swim_input)
            finally:
                swim_input.close()

            # Basic sanity checks
            assert output.n_days == 30
            assert output.n_fields == 3

            # ET should be positive in summer
            assert output.eta.sum() > 0
            assert np.all(output.eta >= 0)

            # Kcb should be in reasonable range
            assert np.all(output.kcb >= 0)
            assert np.all(output.kcb <= 1.5)

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_physical_constraints_maintained(self, converted_container):
        """Physical constraints are maintained throughout simulation."""
        with tempfile.TemporaryDirectory() as tmpdir:
            h5_path = Path(tmpdir) / "swim_input.h5"

            swim_input = build_swim_input(
                converted_container,
                h5_path,
                start_date="2010-01-01",
                end_date="2010-12-31",
                etf_model="ssebop",
            )

            try:
                output, final_state = run_daily_loop(swim_input)
            finally:
                swim_input.close()

            # ET must be non-negative
            assert np.all(output.eta >= 0)

            # Kcb must be bounded
            assert np.all(output.kcb >= 0)
            assert np.all(output.kcb <= 1.5)

            # Ks must be in [0, 1]
            assert np.all(output.ks >= 0)
            assert np.all(output.ks <= 1.0 + 1e-10)

            # Runoff must be non-negative
            assert np.all(output.runoff >= 0)

            # SWE must be non-negative
            assert np.all(output.swe >= 0)

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_multiple_fields_independent(self, converted_container):
        """Each field should be processed independently."""
        with tempfile.TemporaryDirectory() as tmpdir:
            h5_path = Path(tmpdir) / "swim_input.h5"

            swim_input = build_swim_input(
                converted_container,
                h5_path,
                start_date="2010-07-01",
                end_date="2010-07-31",
                etf_model="ssebop",
            )

            try:
                output, final_state = run_daily_loop(swim_input)
            finally:
                swim_input.close()

            # Different fields should have different ET totals
            et_totals = [output.eta[:, i].sum() for i in range(output.n_fields)]
            assert len(set(et_totals)) == len(et_totals), "All fields have identical ET"


class TestDiagnostics:
    """Diagnostic tests that print comparison statistics."""

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_print_parity_summary(self, golden_outputs, process_outputs):
        """Print detailed parity statistics (always passes, for debugging)."""
        print("\n" + "="*60)
        print("PARITY SUMMARY")
        print("="*60)

        for fid in FIELD_IDS:
            if fid not in process_outputs:
                print(f"\n{fid}: NOT IN PROCESS OUTPUTS")
                continue

            print(f"\n{fid}")
            print("-" * 40)

            golden = golden_outputs[fid]
            process = process_outputs[fid]

            for col in ['et_act', 'kc_bas', 'ks', 'depl_root', 'swe', 'runoff']:
                if col not in golden.columns:
                    continue

                g = golden[col].values
                p = process[col].values

                # Statistics
                g_mean, g_std = g.mean(), g.std()
                p_mean, p_std = p.mean(), p.std()

                # Correlation (filter zeros)
                mask = (np.abs(g) > 0.001) & (np.abs(p) > 0.001)
                if mask.sum() > 10:
                    corr = np.corrcoef(g[mask], p[mask])[0, 1]
                else:
                    corr = np.nan

                # RMSE
                rmse = np.sqrt(np.mean((g - p)**2))

                # Bias
                bias = p_mean - g_mean

                print(f"  {col:12s}: "
                      f"golden={g_mean:8.2f}±{g_std:6.2f}, "
                      f"process={p_mean:8.2f}±{p_std:6.2f}, "
                      f"r={corr:.3f}, RMSE={rmse:.3f}, bias={bias:+.2f}")

        print("\n" + "="*60)


class TestWaterBalanceConservation:
    """Test that water balance is conserved over the simulation."""

    @pytest.mark.skipif(not fixture_available(), reason="Fixture not available")
    def test_cumulative_water_balance(self, converted_container):
        """Verify water balance closes over multi-year simulation."""
        with tempfile.TemporaryDirectory() as tmpdir:
            h5_path = Path(tmpdir) / "swim_input.h5"

            swim_input = build_swim_input(
                converted_container,
                h5_path,
                start_date="2010-01-01",
                end_date="2011-12-31",
                etf_model="ssebop",
            )

            try:
                output, final_state = run_daily_loop(swim_input)

                # Get precipitation from input
                prcp = swim_input.get_time_series("prcp")

                for i in range(swim_input.n_fields):
                    # Total inputs
                    total_precip = prcp[:, i].sum()
                    total_irr = output.irr_sim[:, i].sum()
                    total_gw = output.gw_sim[:, i].sum()
                    total_input = total_precip + total_irr + total_gw

                    # Total outputs
                    total_et = output.eta[:, i].sum()
                    total_runoff = output.runoff[:, i].sum()
                    total_dperc = output.dperc[:, i].sum()
                    total_output = total_et + total_runoff + total_dperc

                    # Irrigation bypass (10% goes directly to L3)
                    irr_bypass = 0.1 * total_irr

                    # For a rough check, inputs should roughly equal outputs
                    # (ignoring storage change which is harder to compute without spinup)
                    balance = total_input + irr_bypass - total_output

                    # This is a soft check - exact balance requires accounting for storage
                    # Just verify no massive imbalance (> 50% of inputs)
                    imbalance_ratio = abs(balance) / (total_input + 1)
                    assert imbalance_ratio < 0.5, (
                        f"Field {i}: Large water imbalance: "
                        f"inputs={total_input:.1f}, outputs={total_output:.1f}, "
                        f"imbalance={balance:.1f} ({imbalance_ratio:.1%})"
                    )

            finally:
                swim_input.close()
