"""Golden-loop regression tests using native container path.

These tests verify that build_swim_input() + run_daily_loop_fast() produce
numerically identical results to previously snapshotted golden arrays. Each
case uses a minimal 2-year fixture container built from production data.

Fixtures are generated by scripts/generate_golden_loop.py.
"""

import sys
import tempfile
from pathlib import Path

import numpy as np
import pytest
from numpy.testing import assert_allclose

pytestmark = [pytest.mark.regression, pytest.mark.slow]

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from swimrs.container import SwimContainer
from swimrs.process.input import build_swim_input
from swimrs.process.loop_fast import run_daily_loop_fast

FIXTURE_DIR = PROJECT_ROOT / "tests" / "fixtures" / "golden_loop"

OUTPUT_FIELDS = [
    "eta",
    "etf",
    "kcb",
    "ke",
    "ks",
    "kr",
    "runoff",
    "rain",
    "melt",
    "swe",
    "depl_root",
    "dperc",
    "irr_sim",
    "gw_sim",
]

CASES = {
    "fort_peck": {
        "container": "fort_peck.swim",
        "golden": "fort_peck_golden.npz",
        "etf_model": "ptjpl",
        "start_date": "2007-01-01",
        "end_date": "2008-12-31",
        "irrigated": False,
    },
    "crane": {
        "container": "crane.swim",
        "golden": "crane_golden.npz",
        "etf_model": "ssebop",
        "start_date": "2020-01-01",
        "end_date": "2021-12-31",
        "irrigated": True,
    },
}


def fixture_available(case_name):
    """Check if fixture files exist for a case."""
    case = CASES[case_name]
    return (FIXTURE_DIR / case["container"]).exists() and (FIXTURE_DIR / case["golden"]).exists()


def _run_case(case_name):
    """Run simulation and load golden arrays for a case."""
    case = CASES[case_name]
    container_path = FIXTURE_DIR / case["container"]
    golden_path = FIXTURE_DIR / case["golden"]

    container = SwimContainer.open(str(container_path), mode="r")
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            h5_path = Path(tmpdir) / "swim_input.h5"
            swim_input = build_swim_input(
                container,
                h5_path,
                start_date=case["start_date"],
                end_date=case["end_date"],
                etf_model=case["etf_model"],
            )
            try:
                output, _ = run_daily_loop_fast(swim_input)
            finally:
                swim_input.close()

        golden = dict(np.load(golden_path))
        actual = {field: getattr(output, field).copy() for field in OUTPUT_FIELDS}
    finally:
        container.close()

    return actual, golden


# Module-scoped fixtures: run each case once and cache results


@pytest.fixture(scope="module")
def fort_peck_results():
    if not fixture_available("fort_peck"):
        pytest.skip("Fort Peck golden-loop fixture not available")
    return _run_case("fort_peck")


@pytest.fixture(scope="module")
def crane_results():
    if not fixture_available("crane"):
        pytest.skip("Crane golden-loop fixture not available")
    return _run_case("crane")


def _get_results(request, case_name):
    """Helper to get the right fixture results by case name."""
    if case_name == "fort_peck":
        return request.getfixturevalue("fort_peck_results")
    elif case_name == "crane":
        return request.getfixturevalue("crane_results")


class TestOutputFieldMatchesGolden:
    """Each output field must match the golden snapshot exactly."""

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    @pytest.mark.parametrize("field", OUTPUT_FIELDS)
    def test_field(self, request, case_name, field):
        actual, golden = _get_results(request, case_name)
        assert_allclose(
            actual[field],
            golden[field],
            rtol=1e-10,
            atol=1e-12,
            err_msg=f"{case_name}/{field} differs from golden",
        )


class TestNoNans:
    """No NaN values in any output field."""

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_no_nans(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        for field in OUTPUT_FIELDS:
            assert not np.any(np.isnan(actual[field])), f"{case_name}/{field} contains NaN"


class TestPhysicalBounds:
    """Output values respect physical constraints."""

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_et_non_negative(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        assert np.all(actual["eta"] >= 0)

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_ks_bounded(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        assert np.all(actual["ks"] >= 0)
        assert np.all(actual["ks"] <= 1.0)

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_kr_bounded(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        assert np.all(actual["kr"] >= 0)
        assert np.all(actual["kr"] <= 1.0)

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_swe_non_negative(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        assert np.all(actual["swe"] >= 0)

    @pytest.mark.parametrize("case_name", ["fort_peck", "crane"])
    def test_runoff_non_negative(self, request, case_name):
        actual, _ = _get_results(request, case_name)
        assert np.all(actual["runoff"] >= 0)


class TestIrrigationBehavior:
    """Irrigated and unirrigated sites behave correctly."""

    def test_irrigated_site_has_irrigation(self, crane_results):
        actual, _ = crane_results
        assert np.any(actual["irr_sim"] > 0), "Crane (irrigated) should have nonzero irrigation"

    def test_unirrigated_site_no_irrigation(self, fort_peck_results):
        actual, _ = fort_peck_results
        assert np.allclose(actual["irr_sim"], 0), (
            "Fort Peck (unirrigated) should have zero irrigation"
        )
